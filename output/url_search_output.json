[
    {
        "title": "Automatic Topic Label Generation using Conversational Models",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.8018043",
                "type": "zenodo",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1145/3587259.3627574",
        "arxiv": null,
        "abstract": "AbstractProbabilistic topic modelling is an unsupervised machine learning technique that, given a set ofdocuments, is capable of scanning, detecting patterns of words and phrases, and automaticallygrouping words that best characterize a topic. Many times, however, we are interested in know-ing what relates these documents beyond the most characteristic patterns or sets of words inthe set. Consequently, the generation of topic labels appeared, which sought to generate a labelthat would characterize the set of documents in a more interpretable way than having a groupof words that we, a priori, do not know the relationship they have with each other. Currently,new ways of generating these topic labels that are easily understandable automatically are stillbeing investigated.At the same time, Neural Language Models based on Neural Networks with conversationalpurpose have recently emerged, which are trained to understand and generate dialogues be-tween humans and machines. These models possess capabilities beyond the ability to engage inconversation, such as ChatGPT, which has demonstrated the ability to autonomously composeemails or write about a specific topic, for example.Conversational models present an apparent potential to not only have recreational applica-tions, but can also be useful for other tasks, as stated in Sallam’s publication \"ChatGPT Utilityin Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspec-tives and Valid Concerns\" [1], where the author analysed 60 publications that talked about thebenefits of using ChatGPT in different tasks, such as efficient analysis of datasets or code gen-eration for health care research. Given this fact, the purpose of this Final Master’s Project is tostudy the capacity that conversational models may have to automatically and unsupervisedlygenerate tags for probabilistic topics given a set of keywords representative of the topic, follow-ing a methodology which we will refer as Conversational Probabilistic Topic Labelling (CPTL).We also compare the performance of these conversational models with the performance of atask-specific language model trained to generate topic labels.iiiContents1 Introduction 12 Related work 52.1 Probabilistic topic modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52.2 Topic label generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1 Supervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82.2.1.1 Term lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92.2.1.2 Term hierarchies . . . . . . . . . . . . . . . . . . . . . . . . . . . 102.2.1.3 External knowledge sources . . . . . . . . . . . . . . . . . . . . . 102.2.2 Unsupervised methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112.3 Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123 Approach 153.1 Topic Labelling system based on Conversational Models . . . . . . . . . . . . . . 154 Evaluation 194.1 Modules selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224.1.2 Question templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234.1.3 Topic words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.1.4 QA models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254.2 Modules evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274.2.1 Top words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2 Language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284.2.2.1 Conversational models . . . . . . . . . . . . . . . . . . . . . . . 28",
        "file_name": "10!1145%3587259!3627574.pdf",
        "file_path": ".\\PDFs\\10!1145%3587259!3627574.pdf"
    },
    {
        "title": "Traffic Optimization Through Waiting Prediction and Evolutive Algorithms",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.12.001",
        "arxiv": null,
        "abstract": "AbstractTraffic optimization systems require optimization procedures to optimize traffic light timing settings in order to improve pedestrian and vehicle mobility. Traffic simulators allow obtaining accurate estimates of traffic behavior by applying different timing configurations, but require considerable computational time to perform validation tests. For this reason, this project proposes the development of traffic optimizations based on the estimation of vehicle waiting times through the use of different prediction techniques and the use of this estimation to subsequently apply evolutionary algorithms that allow the optimizations to be carried out. The combination of these two techniques leads to a considerable reduction in calculation time, which makes it possible to apply this system at runtime. The tests have been carried out on a real traffic junction on which different traffic volumes have been applied to analyze the performance of the system.DOI:  10.9781/ijimai.2023.12.001Traffic Optimization Through Waiting Prediction and Evolutive AlgorithmsFrancisco García1, Helena Hernández2, María N. Moreno-García2, Juan F. De Paz1*, Vivian F. López2, Javier Bajo31 Expert Systems and Applications Lab. University of Salamanca. Plaza de los Caídos s/n. Salamanca (Spain)2 Data Mining Research Group. University of Salamanca Plaza de los Caídos s/n. Salamanca (Spain)3 Department of Artificial Intelligence, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)Received 10 May 2022 | Accepted 22 September 2023 | Early Access 5 December 2023 I. IntroductionACCORDING to United Nations data, in 2018 55% of the population was living in urban spaces, the distribution of the urban population varies considerably by region: Northern America 82%, Latin America and the Caribbean 81%, Europe 74%, Oceania 68%, Asia 60% and Africa 43%. The urban population is continuously increasing; it is estimated that 66% of the population will live in urban areas by 2050, an increase of 16% compared to 2008 [1]. These data are very similar to those provided by the United Nations organization since in 2018 it estimated that 68% of the population will live in urban areas in 2050. This increase implies greater traffic congestion in cities due to both the increase in traffic and the unsuitable infrastructures [1].  For this reason, programs such as Horizonte Europa have analyzed global challenges such as climate, energy and mobility, and in particular, intelligent mobility through the optimization of infrastructures. Due to this increase in population and the need to improve infrastructure management, there is a demand to create systems capable of improving traffic efficiency, which will be applied in this project.Traditional operational research incorporates the use of queuing theory to make predictions about different parameters such as waiting times [2]. The queuing theory approach in which there are usually M/G/s models [3] where M refers to the arrival of vehicles which is represented by a poisson, G the service rate which in certain cases can be modeled by an exponential and finally, s represents the number of servers.  From these definitions, it is possible to determine parameters such as waiting times, which will be the object of study of this project. However, classical queuing theory would not take into account parameters that need to be considered, such as the time lost from the moment a traffic light turns green until the cars start moving. For these reasons, simulators such as SUMO [4] are currently being used ",
        "file_name": "ijimai.2023.12.001",
        "file_path": "PDFs\\ijimai.2023.12.001.pdf"
    },
    {
        "title": "Is Automated Consent in Solid GDPR-Compliant? An Approach for Obtaining Valid Consent with the Solid Protocol",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/duo-odrl-dpv",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1700836284.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://www.dataprotectioncontrol.org/spec/(accessed on 25 October 2023)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info14120631",
        "arxiv": null,
        "abstract": "Abstract: Personal Information Management Systems (PIMS) are acquiring a prominent role in thedata economy by promoting services that help individuals to have more control over the processing oftheir personal data, in line with the European data protection laws. One of the highlighted solutionsin this area is Solid, a new protocol that is decentralizing the storage of data, through the usage ofinteroperable web standards and semantic vocabularies, to empower its users to have more controlover the processing of data by agents and applications. However, to fulfill this vision and gatherwidespread adoption, Solid needs to be aligned with the law governing the processing of personaldata in Europe, the main piece of legislation being the General Data Protection Regulation (GDPR).To assist with this process, we analyze the current efforts to introduce a policy layer in the Solidecosystem, in particular, related to the challenge of obtaining consent for processing personal data,focusing on the GDPR. Furthermore, we investigate if, in the context of using personal data forbiomedical research, consent can be expressed in advance, and discuss the conditions for validconsent and how it can be obtained in this decentralized setting, namely through the matching ofprivacy preferences, set by the user, with requests for data and whether this can signify informedconsent. Finally, we discuss the technical challenges of an implementation that caters to the previouslyidentified legal requirements.Keywords: personal information management systems; Solid; semantic web; data protection; consent1. IntroductionThe General Data Protection Regulation (GDPR) [1] has become the gold standard inthe European Union (EU) and its effects are being globally felt in Asia, Latin America andAfrica [2].The purpose of the GDPR is twofold: on the one hand, it protects individuals in whatconcerns their human rights and, on the other hand, it enables the free flow of personaldata (Article 1 GDPR). The EU expressed a vision that encompasses the creation of a singleEuropean market for data, where access to personal and non-personal data from acrossthe world is secure and can be used by an ecosystem of companies, governments, andindividuals to provide high-quality data-driven products and services for its citizens whileensuring that “EU law can be enforced effectively” and data subjects are still in control ofwhat happens to their personal data [3].In addition to the GDPR, novel data-related legislation with new data governanceschemes, such as the Data Governance Act (DGA) [4], is being brought forward by the EUto build an infrastructure for data-sharing and to improve citizens’ trust. In particular, trusthas been proven as an important factor that positively influences the perceived usefulnessInformation 2023, 14, 631. https://doi.org/10.3390/info14120631 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14120631https://doi.org/10.3390/info14120631https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-0319-8935https://orcid.org/0000-0003-0259-7560https://doi.org/10.3390/info14120631https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14120631?type=check_update&version=1Information 2023, 14, 631 2 of 33and ease of use of digital personal datastores [5] in data-handling services and allow themto share their sensitive data for the ‘public good’.However, this has not come without challenges in its interpretation and enforcement.",
        "file_name": "pdf?version=1700836284",
        "file_path": "PDFs\\pdfversion1700836284.pdf"
    },
    {
        "title": "A Review of Bias and Fairness in Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.11.001",
        "arxiv": null,
        "abstract": "AbstractAutomating decision systems has led to hidden biases in the use of artificial intelligence (AI). Consequently, explaining these decisions and identifying responsibilities has become a challenge. As a result, a new field of research on algorithmic fairness has emerged. In this area, detecting biases and mitigating them is essential to ensure fair and discrimination-free decisions. This paper contributes with: (1) a categorization of biases and how these are associated with different phases of an AI model’s development (including the data-generation phase); (2) a revision of fairness metrics to audit the data and AI models trained with them (considering agnostic models when focusing on fairness); and, (3) a novel taxonomy of the procedures to mitigate biases in the different phases of an AI model’s development (pre-processing, training, and post-processing) with the addition of transversal actions that help to produce fairer models.DOI:  10.9781/ijimai.2023.11.001A Review of Bias and Fairness in Artificial IntelligenceRubén González-Sendino1, Emilio Serrano1, Javier Bajo1, Paulo Novais2 *1 Ontology Engineering Group, Departamento de Inteligencia Artificial, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)2 ALGORITMI Research Centre/LASI, University of Minho, Braga (Portugal)Received 16 September 2022 | Accepted 29 September 2023 | Early Access 10 November 2023 I. IntroductionThe evolution of artificial intelligence (AI) has allowed humans to be heavily supported in the decision-making process of some application domains [1]. The high degree of independence that AI is capable of exhibiting can be problematic [2], [3], especially when humans are not in the loop [4]–[6]. Automatization of decisions can come at the cost of amplifying bias and creating feedback loops [7], [8]. One of the main reasons AI can produce unfair results is due to the data with which it has been trained [9].Although the concept of bias is broad, this paper adheres to the following definition: “the systematic tendency in a model to favor one demographic group/individual over another, which can be mitigated but may well lead to unfairness” [9], [10]. Therefore, the next definition needed to understand the problem this paper studies is Fairness, which is defined as: “ the absence of prejudice or favoritism towards an individual or a group based on its inherent or acquired characteristics” [9].In the AI scope, incorrect predictions do not necessarily indicate that the model is unfair if its development was correct [11]. An unfair model is one whose decisions are biased toward a particular group of people. Moreover, biases cannot always be avoided. Thus, techniques must be used to mitigate their consequences, which aim to increase equality in the results. Data and models can be audited with fairness metrics, which are used to measure fairness between two groups or similar individuals. Furthermore, the categorization of methods for bias and unfairness mitigation depends on the phase of the AI model’s development in which they are used. These phases are typically pre-training, training, and post-training.This paper contributes with a systematic review of bias and fairness in artificial intelligence. The purpose of a systematic review is to provide a comprehensive summary of the literature available which is relevant to several research questions. The three questions addressed in this paper are: (1) What bias affects fairness?; (2) What are the metrics to measure fairness?; and, (3) How are biases mitigated? ",
        "file_name": "ijimai.2023.11.001",
        "file_path": "PDFs\\ijimai.2023.11.001.pdf"
    },
    {
        "title": "A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine",
        "implementation_urls": [],
        "doi": "10.3390/app131910778",
        "arxiv": null,
        "abstract": "Abstract: Due to the success of artificial intelligence (AI) applications in the medical field overthe past decade, concerns about the explainability of these systems have increased. The reliabilityrequirements of black-box algorithms for making decisions affecting patients pose a challenge evenbeyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integratingexplainability into these systems. While most traditional AI methods and expert systems are in-herently interpretable, the recent literature has focused primarily on explainability techniques formore complex models such as deep learning. This scoping review critically analyzes the existingliterature regarding the explainability and interpretability of AI methods within the clinical do-main. It offers a comprehensive overview of past and current research trends with the objective ofidentifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) inthe field of medicine. Such constraints encompass the diverse requirements of key stakeholders,including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition,the absence of standardised evaluation criteria, the potential for mistaking explanations for causalrelationships, and the apparent trade-off between model accuracy and interpretability. Furthermore,this review discusses possible research directions aimed at surmounting these challenges. Theseinclude alternative approaches to leveraging medical expertise to enhance interpretability withinclinical settings, such as data fusion techniques and interdisciplinary assessments throughout thedevelopment process, emphasizing the relevance of taking into account the needs of final users todesign trustable explainability methods.Keywords: artificial intelligence; medicine; explainable AI; interpretable AI1. Introduction1.1. AI in Medicine: Opportunities and ChallengesToday’s Artificial Intelligence (AI), with its capability to automate and ease almost anykind of task, frequently appearing to surpass human performance, has become a popularand widespread technology for many applications, especially over the last decade, thanksto advances in deep learning (DL), with clinical healthcare being no exception.Medicine has been one of the most challenging, but also most attention-getting appli-cation fields for AI for the past five decades, with diagnostic decision support, the interpre-tation of medical images and clinical lab tests, drug development, patient management,and others all demonstrating the broad and diverse scope of AI techniques applied tomedical issues.AI methods have promised a range of potential advantages for medical informaticssystems. Automating burdensome tasks can be of great help, alleviating clinicians fromunnecessary efforts and allowing them to focus on more important issues surroundingAppl. Sci. 2023, 13, 10778. https://doi.org/10.3390/app131910778 https://www.mdpi.com/journal/applscihttps://doi.org/10.3390/app131910778https://doi.org/10.3390/app131910778https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0002-1215-3333https://doi.org/10.3390/app131910778https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app131910778?type=check_update&version=1Appl. Sci. 2023, 13, 10778 2 of 23patient care. AI systems can perform these tasks with high precision and efficiency, and also,they can assist the extraction of relevant information from the large quantities of databeing produced by modern medicine [1]. AI systems might be particularly beneficial in",
        "file_name": "pdf?version=1695880594",
        "file_path": "PDFs\\pdfversion1695880594.pdf"
    },
    {
        "title": "Experiential Observations: An Ontology Pattern-Based Study on Capturing the Potential Content within Evidences of Experiences",
        "implementation_urls": [
            {
                "identifier": "https://github.com/modellingDH/odp_experience",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3586078.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "25OWL 2 profiles: https://www.w3.org/TR/owl2-profiles."
                    }
                ]
            }
        ],
        "doi": "10.1145/3586078",
        "arxiv": null,
        "abstract": "can we perform an abstraction over the cultural domain to render these data interoperable through areusable ontological framework?RQ2: If such an abstraction is possible, what will its key concepts be, so that the phenomenological coordi-nates of the experience can be extracted from the informational content, while still preserving theirdiversity?A hypothetical upper ontology of observation would abstract from the phenomenon by providing a middle-ware between phenomenological ontologies and ontologies of sources (Figure 1). On the one hand, ontologiesof sources like LRMoo/FRBRoo [4] and SPAR [5] describe objective features of sources but not their informationcontent. On the other hand, experiential ontologies describe the phenomenon in object but not the relationsbetween the content structures and features of the sources. In other words, the need for the E&O is grounded onthe need for a language to express general features of sources with a direct relation to their phenomenologicalcontent. For example, a listener may recount that hearing a particularly “groovy” riff in a rock song, or a peculiar1Submission to the ODP portal: http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation.ACM Journal on Computing and Cultural Heritage, Vol. 16, No. 3, Article 58. Publication date: August 2023.http://ontologydesignpatterns.org/wiki/Submissions:Experience_%26_Observation58:4 • A. Antonini et al.Fig. 1. Observations are the result of a combination of activity, experience and the opportunity to express the observation(evidence).rendition of it in a live performance, inspired them to conduct research on the musicians who wrote, played orwere influenced by it. This latter example demonstrates that an abstraction over this reality into a knowledgepattern should not make assumptions on what is labelled as sources—that is, those whose consumption promptsthe observation. They could belong anywhere in the information layering of FRBR, such as works, expressionsor manifestations (which musical performances generate),2 and which ones should be treated as sources dependson the user’s ability to discern among those layers when recording their evidence.Indeed, each different experiential study adopts different models for the description of data, grounded on thenature of the phenomenon in object and relying on a set of assumptions concerning the homogeneous nature ofsources. In this scenario, the E&O pattern supports data interoperability between research use cases, clarifyingthe relation between activity, reflection and the evidences used in experiential studies.1.2 Application ScenariosThe E&O pattern is intended to support extending schemas and ontologies to encode research data from expe-riential studies to express key facts concerning sources, which would otherwise be lost with the backgroundknowledge of the experts on those sources.Interoperable research data do introduce new issues. Indeed, data reuse enables a detachment between theanalysis of sources and the use of the generated data: the in-depth understanding of sources (e.g., provenanceand limitations) is therefore no longer a requirement for using the data. For instance, a scholar in ClassicalStudies can mix and match data from heterogeneous studies, such as online book reviews or marginalia froman author’s library, without being an expert in either type of source or method. In other words, the technicalfeasibility granted by having a common schema of the research subject does remove the need for informationnecessary to the correct framing of a study. E&O provides a way to reinstate the context of datasets, which isnecessary to interpret the data correctly. For instance, is the reading experience spontaneous or guided by aquestionnaire? Is it a mature recollection years after reading or an immediate response? Is reading a free choiceor part of a school or work assignment?The rest of the article is structured as follows. Section 2 provides a brief background on experiential studiesand the issues concerning observing experience. Section 3 presents a summary of the state of the art, includingrelevant patterns and experiential ontologies. Section 4 describes the E&O pattern in detail, whereas Section 5is devoted to its evaluation. In Section 6, we discuss the application of the pattern and of the retrospective casestudies. Section 7 provides a discussion of the case studies in terms of common patterns emerging from applyingE&O to the encoding of the different types of sources. Finally, Section 8 concludes the article and includes futurelines of work.2https://www.ifla.org/publications/functional-requirements-for-bibliographic-records.",
        "file_name": "10!1145%3586078.pdf",
        "file_path": ".\\PDFs\\10!1145%3586078.pdf"
    },
    {
        "title": "Automatic thematic classification of documents based on vocabularies and use frequencies. The case of scientific dissemination articles",
        "implementation_urls": [],
        "doi": "10.3989/redc.2023.3.1996",
        "arxiv": null,
        "abstract": "Abstract: It is often necessary to classify documents by assigning them a theme or topic from a series of predefined options. This work is usually done manually, by reading the document by a specialist. This manual process is tedious, requires time and resources, and is prone to bias and preferences of each specialist.As an alternative, this article presents an automatic thematic classification system, capable of classifying hundreds of documents in a few seconds, highly parameterized, and that does not require the specialists intervention. The system is based on predefined thematic vocabularies and frequencies of use of lexical forms, and assigns one or more priority top-ics to each document. The suggested approach has been developed and tested in the context of scientific dissemination articles in the Spanish language.Using this approach, it is possible to systematically classify large amounts of documents by topic, using fewer resources than doing it manually, and avoiding unknown biases. The approach has shown to be as effective as other proposals, but requires less computational resources.Keywords: Document classification; thematic classification; algorithm; vocabularies; lexical frequencies; science dis-semination.Copyright: © 2023 CSIC. Este es un artículo de acceso abierto distribuido bajo los términos de la licencia de uso y distribución Creative Commons Reconocimiento 4.0 Internacional (CC BY 4.0).https://doi.org/10.3989/redc.2023.3.1996mailto:cesar.gonzalez-perez@incipit.csic.eshttps://orcid.org/0000-0002-3976-7589mailto:nacho.vidal@cchs.csic.eshttps://orcid.org/0000-0001-6169-784Xmailto:ana.garcia.g@cchs.csic.eshttps://orcid.org/0000-0002-5952-4971mailto:pcalleja%40fi.upm.es?subject=https://orcid.org/0000-0001-8423-8240https://doi.org/10.3989/redc.2023.3.1996César González-Pérez, José Ignacio Vidal Liy, Ana García García, Pablo Calleja Ibáñez2 Rev. Esp. Doc. Cient., 46(3), julio-septiembre 2023, e362. ISSN-L: 0210-0614. https://doi.org/10.3989/redc.2023.3.19961. INTRODUCCIÓNCualquier institución que trate con un gran nú-mero de documentos, sobre todo si son de pro-cedencia externa, debe clasificarlos temáticamente para su adecuada gestión. Este es el caso de bi-bliotecas o archivos, por ejemplo. La clasificación temática consiste en asignar uno o más temas a cada documento, de un repertorio de temas que puede ser fijo o bien cambiante. En cualquier caso, esta clasificación suele realizarse de forma manual mediante un proceso de análisis de contenido, le-yendo el documento o un resumen de este, si exis-te, por parte de un especialista humano, y asig-nando después uno o más temas. Para decidir qué temas se asignan a un documento, el especialista hace uso de su conocimiento tácito y experiencia, y, a veces, también de criterios previamente espe-cificados. Sea como sea, este proceso es tedioso, requiere mucho tiempo y recursos humanos, y es propenso a los sesgos y preferencias de cada es-pecialista.Hoy en día, en un momento en el que muchos de los documentos que manejamos existen en for-",
        "file_name": "2292",
        "file_path": "PDFs\\2292.pdf"
    },
    {
        "title": "“Who Should I Trust with My Data?” Ethical and Legal Challenges for Innovation in New Decentralized Data Management Technologies",
        "implementation_urls": [],
        "doi": "10.3390/info14070351",
        "arxiv": null,
        "abstract": "Abstract: News about personal data breaches or data abusive practices, such as Cambridge Analytica,has questioned the trustworthiness of certain actors in the control of personal data. Innovations inthe field of personal information management systems to address this issue have regained traction inrecent years, also coinciding with the emergence of new decentralized technologies. However, onlywith ethically and legally responsible developments will the mistakes of the past be avoided. Thiscontribution explores how current data management schemes are insufficient to adequately safeguarddata subjects, and in particular, it focuses on making these data flows transparent to provide anadequate level of accountability. To showcase this, and with the goal of enhancing transparency tofoster trust, this paper investigates solutions for standardizing machine-readable policies to expresspersonal data processing activities and their application to decentralized personal data stores as anexample of ethical, legal, and technical responsible innovation in this field.Keywords: data governance; digital age; transparency; personal data management; identity management1. IntroductionData-driven innovations are expected to deliver further economic and societal develop-ment [1]. Through the analysis, sharing, and (re-)use of data, business models and govern-ments’ processes have been transformed to benefit from those practices [2]. The emergenceof a data-driven society is being fostered by policy actions from different governments on aworldwide scale. The European Union (EU) is no exception to this, as the European Com-mission has put on its agenda the development of “A Europe fit for the Digital Age”. The Eu-ropean Commission’s strategy and related policy documents can be located at the followinglink: https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_en(accessed on 26 May 2023). Regardless of whether it is a Big Tech company based in theUnited States (US), a large data broker in the EU, or a Chinese government-controlledentity, current data practices have been questioned by different societal sectors, from in-dividuals to nongovernmental organizations (NGOs) or from academics to governments.Trust in many digital services has been compromised [3], which has left individuals askingthemselves “who should I trust with my data”.In response to this trust crisis, technology has been looked upon to provide answers.Applied to the field of (personal) data, self-sovereign identity models [4] — as improve-ments over existing Personal Information Management Systems (PIMS) — have been putunder the spotlight due to their potential, but they are also taken with “a grain of salt”, asInformation 2023, 14, 351. https://doi.org/10.3390/info14070351 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14070351https://doi.org/10.3390/info14070351https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-2344-4061https://orcid.org/0000-0002-6820-999Xhttps://orcid.org/0000-0003-0259-7560https://orcid.org/0000-0002-3503-4644https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_enhttps://doi.org/10.3390/info14070351https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14070351?type=check_update&version=1Information 2023, 14, 351 2 of 17they are not free from shortcomings [5]. Through them, users would be in direct control oftheir information and decide when, how, and who can access such information. Certain",
        "file_name": "pdf?version=1687339409",
        "file_path": "PDFs\\pdfversion1687339409.pdf"
    },
    {
        "title": "MuHeQA: Zero-shot question answering over multiple and heterogeneous knowledge bases",
        "implementation_urls": [
            {
                "identifier": "https://github.com/librairy/MuHeQA",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-233379.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This evaluation set is available as part of our additional material.12 4.2."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-233379",
        "arxiv": null,
        "abstract": "Abstract. There are two main limitations in most of the existing Knowledge Graph Question Answering (KGQA) algorithms.First, the approaches depend heavily on the structure and cannot be easily adapted to other KGs. Second, the availability andamount of additional domain-specific data in structured or unstructured formats has also proven to be critical in many of thesesystems. Such dependencies limit the applicability of KGQA systems and make their adoption difficult. A novel algorithm isproposed, MuHeQA, that alleviates both limitations by retrieving the answer from textual content automatically generated fromKGs instead of queries over them. This new approach (1) works on one or several KGs simultaneously, (2) does not requiretraining data what makes it is domain-independent, (3) enables the combination of knowledge graphs with unstructured infor-mation sources to build the answer, and (4) reduces the dependency on the underlying schema since it does not navigate throughstructured content but only reads property values. MuHeQA extracts answers from textual summaries created by combininginformation related to the question from multiple knowledge bases, be them structured or not. Experiments over Wikidata andDBpedia show that our approach achieves comparable performance to other approaches in single-fact questions while beingdomain and KG independent. Results raise important questions for future work about how the textual content that can be createdfrom knowledge graphs enables answer extraction.Keywords: Question answering, natural language processing, knowledge graphs1. IntroductionKnowledge graphs are now being applied in multiple domains. Knowledge Graph Question Answering (KGQA)has emerged as a way to provide an intuitive mechanism for non-expert users to query knowledge graphs. KGQAsystems do not require specific technical knowledge (e.g., knowledge of SPARQL or Cypher), providing answers innatural language for questions that are also expressed in natural language.One of the main challenges in the design of KGQA systems is semantic parsing [14]. In this step, natural languagequeries (NLQs) are translated into a specific query language (e.g. SPARQL1 for RDF-based KGs, or Cypher2 for*Corresponding author. E-mail: carlos.badenes@upm.es.1https://www.w3.org/TR/rdf-sparql-query2https://opencypher.org1570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:carlos.badenes@upm.eshttps://orcid.org/0000-0002-2753-9917mailto:oscar.corcho@upm.eshttps://orcid.org/0000-0002-9260-0753mailto:carlos.badenes@upm.eshttps://www.w3.org/TR/rdf-sparql-queryhttps://opencypher.orghttps://creativecommons.org/licenses/by/4.0/CORRECTED  PROOF2 C. Badenes-Olmedo and O. Corcho / MuHeQAproperty graphs). KGQA systems typically use templates with placeholders for relations and entities that mustbe identified in the original NLQ. Once the template is filled in, the generated query (in SPARQL, Cypher, etc.)is evaluated in the system that stores the KG (e.g. Triple store, property graph DB) and the results will providethe answers to the question. Thus, KGQA systems can be reduced to entity and relationship search processes thatpopulate predefined query templates that are commonly identified using supervised machine learning techniques(e.g., supervised classifiers) [23]. This approach is heavily dependent on the data schema to explore the entityrelationships and on the availability of training data to create supervised classification models.Mitigating the dependency of KGQA systems on the underlying graph structure and eliminating the need fortraining sets is crucial to create cross-cutting solutions more efficiently and with less cost. The first research questionaddressed in this work is: “How to extract the answer from a knowledge graph without translating the naturallanguage question into a formal query language?”. Moreover, the dependence on the underlying data schema makesit also difficult for existing KGQA systems to combine knowledge from several graphs to extract a single answer.They typically translate the question into specific queries for each supported KG and obtain multiple answers, rather",
        "file_name": "10!3233%sw-233379.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-233379.pdf"
    },
    {
        "title": "Satellite Earth Observation for Essential Climate Variables Supporting Sustainable Development Goals: A Review on Applications",
        "implementation_urls": [],
        "doi": "10.3390/rs15112716",
        "arxiv": null,
        "abstract": "Abstract: Essential climate variables (ECVs) have been recognized as crucial information for achievingSustainable Development Goals (SDGs). There is an agreement on 54 ECVs to understand climateevolution, and multiple rely on satellite Earth observation (abbreviated as s-ECVs). Despite the effortsto encourage s-ECV use for SDGs, there is still a need to further integrate them into the indicatorcalculations. Therefore, we conducted a systematic literature review to identify s-ECVs used inSDG monitoring. Results showed the use of 14 s-ECVs, the most frequent being land cover, ozone,precursors for aerosols and ozone, precipitation, land surface temperature, soil moisture, soil carbon,lakes, and leaf area index. They were related to 16 SDGs (mainly SDGs 3, 6, 11, 14, and 15), 33 targets,and 23 indicators. However, only 10 indicators (belonging to SDGs 6, 11, and 15) were calculatedusing s-ECVs. This review raises research opportunities by identifying s-ECVs yet to be used in theindicator calculations. Therefore, indicators supporting SDGs must be updated to use this valuablesource of information which, in turn, allows a worldwide indicator comparison. Additionally, thisreview is relevant for scientists and policymakers for future actions and policies to better integrates-ECVs into the Agenda 2030.Keywords: SDG; sustainable development; satellite; Earth observation; review; essential variables;climate1. IntroductionThe Agenda 2030 for Sustainable Development and its 17 goals (SDGs) are connectedwith the environment, economy, and society dimensions of sustainable development [1].The 17 goals, their 169 associated targets, and 231 indicators are based on the first data-driven policy development framework, following the principle of “If you don’t measureit, you can’t manage it” [2] (p. 2). Despite the recognized importance of measuring theprogress towards the SDGs, two-thirds of the indicators remain unreported, especially inlow-income countries [3]. Moreover, less than 44% of the SDG indicators can be easilymeasured [4]. Therefore, it is a priority to boost the measuring and monitoring of theprogress towards the SDGs. In our work, we focus on two key approaches to pursue thisRemote Sens. 2023, 15, 2716. https://doi.org/10.3390/rs15112716 https://www.mdpi.com/journal/remotesensinghttps://doi.org/10.3390/rs15112716https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.comhttps://orcid.org/0000-0002-6926-4827https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0001-7380-0701https://orcid.org/0000-0001-5145-9223https://orcid.org/0000-0001-6559-2033https://doi.org/10.3390/rs15112716https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.com/article/10.3390/rs15112716?type=check_update&version=1Remote Sens. 2023, 15, 2716 2 of 25aim: (1) essential variables (EVs) that have been defined as an intermediate layer betweenobservations and indicators [5] and (2) satellite Earth observation (sEO) data that gainedparticular attention as worldwide feasible, cost-effective, and analysis-ready data acrossscales in remote, non-accessible, and poorly monitored regions [6].The EVs emerged in various social and environmental scientific communities relatedto specific domains such as climate, biodiversity, agriculture, and society [5,7–9]. Referto [10,11] for detailed EV compendiums. These kinds of variables are “a minimal setof variables that determine the system’s state and developments, [which] are crucial for",
        "file_name": "pdf?version=1684894645",
        "file_path": "PDFs\\pdfversion1684894645.pdf"
    },
    {
        "title": "Gender imbalance in doctoral education: an analysis of the Spanish university system (1977–2021)",
        "implementation_urls": [],
        "doi": "10.1007/s11192-023-04648-y",
        "arxiv": null,
        "abstract": "AbstractDoctoral education is a key feature of university systems, as well as a basic foundation of scientific practice. That period culminates in a dissertation and examination of the candi‑date that has been studied from several points of view. This paper reports the results of an analysis on the evolution and characteristics of gender imbalance of a complete doctoral system for a wide period of time. Data from the database Teseo was used in order to iden‑tify the individuals involved in the process, the scientific fields in which the dissertations where classified, and the institutions in which the examination took place. Results: the Spanish system shows a clear evolution towards gender balance, but also some concern‑ing trends that are worth tracking. Seemingly, STEM disciplines look to be evolving more slowly than other branches of science in several aspects. A leaky pipeline is characterized in this system around the roles of supervisors, candidates, members and chairs of the dis‑sertation committees. Gender assortativity is also studied and described, and its possible effects discussed around the academic relations that surround doctoral examination.Keywords Gender imbalance · STEM · Dissertations · Teseo · Leaky pipeline · Gender assortativity * Rodrigo Sánchez‑Jiménez  rodsanch@ucm.es1 Library and Information Science Department, SCImago Group, Universidad Complutense de Madrid, Madrid, Spain2 Library and Information Science Department (Internet Medialab Research Group), Universidad Complutense de Madrid, Madrid, Spain3 Sales Engineering EMEA, Neo4j, London, UK4 Ontology Engineering Group (Artificial Intelligence Department), Universidad Politécnica de Madrid, Madrid, Spain5 Library and Information Science Department, Universidad Complutense de Madrid, Madrid, Spainhttp://orcid.org/0000-0002-3685-7060http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-023-04648-y&domain=pdf2578 Scientometrics (2023) 128:2577–25991 3IntroductionGender imbalance and gender bias in science have been studied and described for a long time. Zuckerman and Cole (1975) already described this issue in quantitative terms and advanced the “principle of triple penalty” (cultural inappropriateness/perceived incompe‑tence/direct discrimination). Shiebinger (1987) went over the very abundant literature on the history of women in science and described how at least the number of female scientists was growing faster (low numbers having been associated with that cultural inappropriate‑ness) but the perception of a lesser competence by women (women were systematically employed in less prestigious jobs in the academia) and blatant discrimination (unjustified salary gaps were huge) was ever persistent. Etzkowitz, et al (1992) moved on to put the focus on the de‑genderization of science and society, and on the existence of “different gender styles of scientific work”, an idea that has been one way or another behind many studies comparing output, collaboration, and impact of men and women. Bordons et  al. (2003) acknowledge this factor as a warning to interpret their SCI‑based results but take it a step further. They also explained the cumulative advantage of achieving high ranks in academia over productivity, which in turn accounts for the gender differences in pro‑ductivity. Several years later, Lariviere et al., (2011) reached a somewhat different conclu‑sion, finding again that gender differences were present in terms of production and funding, although the nature of these differences was complex. The subject is therefore very much open to debate, and the focus on its study has varied significantly over time (Tomassini, ",
        "file_name": "s11192-023-04648-y.pdf",
        "file_path": "PDFs\\s11192-023-04648-y.pdf"
    },
    {
        "title": "LUBM4OBDA: Benchmarking OBDA Systems with Inference and Meta Knowledge",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/lubm4obda",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\19479.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are available online at GitHub6 and archived at Zenodo.7 In the following we introduce these elements."
                    }
                ]
            }
        ],
        "doi": "10.13052/jwe1540-9589.2284",
        "arxiv": null,
        "abstract": "AbstractOntology-based data access focuses on enabling query evaluation over het-erogeneous relational databases according to the model represented by anontology. The relationships between the ontology and the data sources arecommonly defined with declarative mappings, which are used by systemsto perform SPARQL-to-SQL query translation or to generate RDF dumpsfrom the relational databases. Besides the potential homogenization of databecause of using an ontology, some additional advantages of this paradigmare that it may allow applying reasoning thanks to the ontology, as wellas querying for meta knowledge, which describes statements with informa-tion such as provenance or certainty. In this paper, (i) we adapt a widelyused RDF graph store benchmark, namely LUBM, for ontology-based dataaccess, (ii) extend the benchmark for the evaluation of queries that exploitmeta knowledge, and (iii) apply it for performance evaluation of state-of-the-art declarative mapping systems. Our proposal, the LUBM4OBDABenchmark, considers inference capabilities that are not covered by previousJournal of Web Engineering, Vol. 22_8, 1163–1186.doi: 10.13052/jwe1540-9589.2284© 2024 River Publishers1164 J. Arenas-Guerrero et al.ontology-based data access benchmarks, and it is the first one for the eval-uation of meta knowledge and the RDF-star data model. The experimentalevaluation shows that current virtualization systems cannot handle someadvanced inference tasks, and that optimizations are needed to scale RDF-starmaterialization.Keywords: OBDA, semantic web, ontology, data integration.1 IntroductionRelational databases (RDBs) are widely used by organizations to managetheir data. Some application scenarios can benefit from exploiting this dataas knowledge graphs [22], to potentially homogenize data using ontologies,applying reasoning over ontologies, get new insights, or for semantic dataintegration use cases [39, 41]. Ontology-based data access [40] (OBDA)is a paradigm for making data sources such as RDBs (or local schemas)available through a standardized and common view (or global schema). Thisview is given in the form of an ontology, which also enriches the databasewith context by providing background knowledge, reasoning capabilities,and interlinking with other knowledge bases. The relationships between theglobal and local schemas are usually defined through mappings [27,39] whichpopulate the ontology with instances generated from the RDB. As a result, itis possible to perform semantic queries independently of the structure of theoriginal RDB.In the Semantic Web community, a set of standards has been pro-posed: OWL [21] and RDFS [8] are the languages used to encode ontolo-gies, RDF [13] allows representing data, and queries are expressed usingSPARQL [18]. The mappings are specified using R2RML [14], the W3Crecommendation to map RDBs to RDF that follows the global as view [27]approach, in which the mappings define, for each element in the globalschema, a query over the local schemas. In addition, RML [24] is a well-known superset of R2RML to map not only RDBs, but also other types ofdata sources such as CSV, JSON or XML.",
        "file_name": "19479",
        "file_path": "PDFs\\19479.pdf"
    },
    {
        "title": "Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature",
        "implementation_urls": [
            {
                "identifier": "https://github.com/drugs4covid/knowledge-acquisition",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!jbi!2023!104382.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of our pre-processing pipeline is available online.14 5."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.jbi.2023.104382",
        "arxiv": null,
        "abstract": "to facilitate the use of the KG by end user. This interface abstractsthe complexity of formal query languages to simplify the retrieval ofinformation from the generated KG.https://drugs4covid.oeg.fi.upm.es/rdfhttps://drugs4covid.oeg.fi.upm.es/sparqlhttps://drugs4covid.oeg.fi.upm.es/services/bio-qaJournal of Biomedical Informatics 142 (2023) 104382C. Badenes-Olmedo and O. CorchoFig. 1. Workflow proposed for the creation of KGs from scientific publications.The following sections detail each of the steps in the context ofthe Drugs4COVID initiative: how have they been addressed, whatchallenges have arisen and what are our recommendations to build aknowledge graph from biomedical literature on coronavirus.3. Harvesting research literature on coronavirus (step 1)In this first stage, the initial objective is to identify the most appro-priate sources of information and validate the availability of their data.Once they have been discovered, the data is downloaded and furtherdownloads are planned, depending on the periodicity of the changes.We performed a systematic literature review to identify the datasetsused when creating coronavirus knowledge graphs [13]. The digitalrepositories that were explored were Pubmed, BioRxiv, MedRxiv andScopus. Preprint databases were also considered to explore works pend-ing of publication. The searches were conducted by searching terms intitle or abstract. Specifically, the inclusion criteria can be summarizedas follows:• IC1: Text in English or Spanish.• IC2: The words ‘‘covid’’/’’coronavirus’’/’’SARS-Cov’’ (regardlessof upper or lower case) appears in either title or abstract.• IC3: Text published between 2019 and 2022.• IC4: The words ‘‘Knowledge Graph’’ (regardless of upper or lowercase, or abbreviations) appear in either title or abstract.• IC5: Text in the biomedical/computational biology/computerscience fields.Metadata of the documents returned in the search, such as title, doi,authors, venue, etc, was downloaded as csv files from the repositories.A total of 218 documents were retrieved. Then, they were scannedfor duplicates by looking at duplicates either in the title or in theDOI through an R script. 13 duplicated articles were removed so 205documents were left for further studying. Manual filtering was thencarried out by removing duplicates and by a manual reviewing of thetitles and abstracts in order to discard articles out of the scope of thisreview. The availability of the datasets mentioned in the articles wasalso checked. By following these criteria, from the 205 documents, only14 texts were selected and subject of a narrower reading. Althoughselected, the texts could still be out of the scope of this review dueto low information provided about the dataset used.The sources for input covid publication data were CORD-19 cor-pus (50%), Pubmed (14%), LitCovid (14%), Targeting2019-nCoV (byGHDDI) (7%), University of Luxembourg Covid corpus (7%) and Eu-ropePMC (7%). Moreover, additional unstructured data sources that donot contain covid-19 publications were also considered, these include",
        "file_name": "10!1016%j!jbi!2023!104382.pdf",
        "file_path": ".\\PDFs\\10!1016%j!jbi!2023!104382.pdf"
    },
    {
        "title": "Pody: A Solid-Based Approach to Embody Agents in Web-Based Multi-Agent-Systems",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-48539-8_15",
        "arxiv": null,
        "abstract": "Abstract. In this paper we discuss the problem of situatedness foragents perceiving and acting on the Web (namely, “Web agents”). As-suming Web agents are embodied on the World Wide Web, then we mustdefine what is a Web agent’s body. We first provide an abstract defini-tion of a Web agent’s body in terms of what it should comprise. Then wepropose a concrete definition of it relying on Solid, a recent Web technol-ogy for Social Linked Data: we implement a Web agent’s body as a datapod. Consequently, we coin the term pody to refer to the Web entity thatembodies an agent on the Web with Solid. This paper summarises thefindings of a working group from the Dagstuhl Seminar 23081: Agentson the Web (February 19-24, 2023).Keywords: MAS · Semantic Web · Solid· Embodiment · Situatedness1 IntroductionSituatedness and embodiment are key notions in research on intelligent agents.The dominant view is that intelligent, rational behaviour is closely related to theenvironment an agent occupies and is not disembodied [17]. This view emergedin the late ’80s in close relationship with research on intelligent robots [11], whichare naturally situated and embodied in a physical environment. The complexityof virtual environments, such as the Web, now rivals that of physical environ-ments. Furthermore, with the recent standardisation of the Web of Things atthe W3C and the IETF, the Web now extends to the physical world – and thusbecomes a uniform hypermedia fabric that interconnects virtual and physical en-vironments. This evolution unlocks new practical use cases for intelligent agents2 A. Zimmermann et al.on the Web, that need to be situated and embodied in their environment. Thisvision that can be traced back to the early days of the Web 6.In this paper, we discuss howWeb agents can be embodied into the Web, bothat an abstract level and concretely using Web standards and technologies. In anutshell, we envision a Web agent’s body as a collection of Web resources andWeb interfaces that are attached to the identity of the agent. The Web agent’sbody allows the agent to participate in collective work as part of a multi-agentsystem (MAS) on the Web: to perceive and actuate Web resources (includingWeb-enabled devices), to be discovered and perceived by other agents, to par-ticipate in organisations, to communicate with other agents, etc. We illustratethis vision through a concrete example of Web agents embodiement using Solidpods, the core concept and technology from Sir Tim Berners-Lee’s project forSocial Linked Data – an initiative to preserve the decentralised nature of theWeb and to radically decentralise personal data. In particular, this enables toseamlessly address MAS use cases where a strong emphasis on ownership of theagents’ personal data and resources is needed.The paper is organized as follows: We first present in Section 2 the context inwhich our proposal arose. Then we present in Section 3 our vision of how agentsshould be situated and embodied on the Web, independently of the technologiesused. Finally we show in Section 4 how this can be implemented using Solid.In the end, we discuss in Section 5 what other abstractions would be neededto articulate podies with other essential dimensions of Web-based MAS and weconclude in Section 6.2 BackgroundIn this section, we first discuss the notions of situatedness and embodiment inArtificial Intelligence – and, in particular, in MAS engineering (Section 2.1).",
        "file_name": "10!1007%978-3-031-48539-8_15.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-48539-8_15.pdf"
    },
    {
        "title": "Event Extraction and Semantic Representation from Spanish Workers’ Statute Using Large Language Models",
        "implementation_urls": [],
        "doi": "10.3233/faia230983",
        "arxiv": null,
        "abstract": "Abstract. This work uses Large Language Models to process an important piece ofSpanish legislation: the Workers’ Statute. The proposed method extracts the rele-vant events in its articles using a GPT-3.5 model and represents the entities involvedin the events and the relationships between them as RDF triples. The experimentscarried out to select a high-performance strategy include both zero- and few-shotlearning tests. Finally, this work proposes a strategy to uplift the extracted legalrelations into a legal knowledge graph.Keywords. Spanish Workers’ Statute, Large Language Models, Knowledge Graph,Legal Domain, Event Extraction1. IntroductionThe legal domain is a complex and dynamic field that involves interpreting and applyinglaws and regulations. Legal data (court cases, legislations, contracts, etc.) is becominga valuable source to push forward intelligent legal tools [1]. This work proposes an ap-proach using event extraction and semantic graph modeling to bring these systems closerto the public. The event extraction task is being tackled in the state-of-the-art using deeplearning models. However, it presents numerous challenges, especially for Spanish texts,including ambiguity, polysemy, and domain-specific terminology [2].The recent development of Large Language Models (LLMs) [3, 4] has proven tobe an excellent approach to mitigate these problems and an important tool to deal withlimited data [5, 6] through natural language instructions, called prompts.This research aims to improve the performance of the event extraction task withinthe legal domain and to link the information into a semantic graph representation. Thedata to be used will be the Spanish Workers’ Statute, given its importance for legislatorsand the general public, and the availability of an annotated corpus of 133 sentences fromthe Statute gathered by Revenko and Martı́n-Chozas [7]. The low amount of tagged datawill be tackled using the GPT-3.5 model, as it has been proven the high performanceon Natural Language Processing (NLP) tasks like event extraction [3, 5]. Finally, theextracted events will be represented in a knowledge graph.Legal Knowledge and Information SystemsG. Sileno et al. (Eds.)© 2023 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA2309833292. Related WorkThis work defines an event as a textual region likely to compact relevant legal informationencapsulated by the articles on the law. The most common event structure is formed by anevent mention, an event trigger, an event argument, and an argument role. The argumentrole is the relationship between an argument and the event it participates in. The basicargument roles are subject, object, and complement. To classify the legal relations, manyworks [7, 8] use the Hohfeldian classes Right, Duty, No-Right, and Privilege [9].State-of-the-art event extraction relies nowadays on deep learning models like graphneural networks (GAN) [10] and attention mechanisms [11]. However, these models relyon huge amounts of labeled data to improve the model’s performance and are mainly usedfor English corpora. This research uses only 133 annotated sentences from the SpanishWorkers’ Statute [7], which are insufficient to achieve high-performance models.To tackle this issue, common approaches use data augmentation techniques [5],transfer learning [7], and active learning [12]. In recent years, language models (LM)have also been used for this and other NLP tasks [6, 3]. In 2021, the work [13] presented",
        "file_name": "10!3233%faia230983.pdf",
        "file_path": ".\\PDFs\\10!3233%faia230983.pdf"
    },
    {
        "title": "Enhancing Drug Repurposing on Graphs by Integrating Drug Molecular Structure as Feature",
        "implementation_urls": [],
        "doi": "10.1109/cbms58004.2023.00215",
        "arxiv": null,
        "abstract": "Abstract— Drug repurposing has become increasingly important, particularly in light of the COVID-19 pandemic. This process involves identifying new therapeutic uses for existing drugs, which can significantly reduce the cost, risk, and time associated with developing new drugs, de novo development. A previous conducted study proved that Deep Learning can be used to streamline this process by identifying drug repurposing hypotheses. The study presented a model called REDIRECTION, which utilized the rich biomedical information available in graph form and combined it with Geometric Deep Learning to find new indications for existing drugs. The reported metrics for this model were 0.87 for AUROC and 0.83 for AUPRC. In this current study, the importance of node features in GNNs is explored. Specifically, the study used GNNs to embed two-dimensional drug molecular structures and obtain corresponding features. These features were incorporated into the drug repurposing graph, along with some other enhancements, resulting in an improved model called DMSR. Performance score for the reported metrics values raised by 0.0448 in AUROC and 0.0919 in AUPRC. Based on these findings, we believe that the method used for embedding drug molecular structures is interesting and captures valuable information about drugs. Its incorporation in the graph for drug repurposing can significantly benefit the process, leading to improved performance evaluation metrics. Keywords—Drug repurposing; Graph deep learning; Drug molecular structure; Graph Neural Networks; Graph Autoencoder; DISNET knowledge base  I. INTRODUCTION  In our prior work, Ayuso et al. (2022) [1], it was proved that incorporating diverse biomedical data into a multi-layered graph is a helpful method for gaining a deeper understanding of diseases and their associated components and characteristics. Using Graph Neural Networks (GNNs) and DISNET biomedical graph [2] a model called REDIRECTION was presented. DISNET is a biomedical integrated knowledge base containing information regarding diseases and their associations to symptoms and drugs, among others. REDIRECTION is based on graph deep learning [3], [4] and link  prediction. Its aim is to generate drug repurposing hypotheses. Drug repurposing or repositioning involves identifying new therapeutic uses for drugs that have already been approved. The model performed well, scoring 0.87 for the area under the receiving operating characteristics curve (AUROC) and 0.83 for the area under the precision-recall curve (AUPR) for a selected subset of already-tested repurposing hypotheses, RepoDB test. GNNs benefit enormously from the addition of node features, due to the foundational role these features play in the message passing framework upon which GNNs are built [5], [6]. This is an intuitive outcome, as the incorporation of relevant ",
        "file_name": "10!1109%cbms58004!2023!00215.pdf",
        "file_path": ".\\PDFs\\10!1109%cbms58004!2023!00215.pdf"
    },
    {
        "title": "Re-Construction Impact on Metadata Representation Models",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/kg-reconstruction-eval",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3587259!3627554.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "$15.00 https://doi.org/10.1145/3587259.3627554 1 INTRODUCTION Knowledge graphs (KG) have gained popularity in recent years for integrating and publishing knowledge on the web [22]."
                    }
                ]
            }
        ],
        "doi": "10.1145/3587259.3627554",
        "arxiv": null,
        "abstract": "ABSTRACTReification in knowledge graphs has been present since the incep-tion of RDF to allow capturing additional information in triples,usually metadata. The need of adopting or changing a metadatarepresentation in a pre-existing graph to enhance the knowledgecapture and access can lead to inducing complex structural changesin the graph, according the target representation’s schema. In thesesituations, it is necessary to decide whether to construct the knowl-edge graph again from its original sources, or to re-construct itusing the current version of the graph. In this paper we conduct anempirical study to analyze which re-construction approach is moresuitable for switching the representation approach from the createdgraph ensuring that the additional represented knowledge is pre-served. We study four well-known metadata representations, usingmapping languages to construct the graph, and SPARQL CONSTRUCTqueries to re-construct it. With this work we aim to provide insightsabout the impact of re-construction on metadata representationsinteroperability and the implications of different approaches.CCS CONCEPTS• Information systems→ResourceDescription Framework (RDF);Graph-based database models; Data exchange; Data model exten-sions;KEYWORDSKnowledge Graphs, Metadata, SPARQL, Declarative Mappings.ACM Reference Format:Ana Iglesias-Molina, Jhon Toledo, Oscar Corcho, and David Chaves-Fraga.2023. Re-Construction Impact onMetadata RepresentationModels. InKnowl-edge Capture Conference 2023 (K-CAP ’23), December 5–7, 2023, Pensacola, FL,USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3587259.3627554Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.K-CAP ’23, December 5–7, 2023, Pensacola, FL, USA© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0141-2/23/12. . . $15.00https://doi.org/10.1145/3587259.36275541 INTRODUCTIONKnowledge graphs (KG) have gained popularity in recent years forintegrating and publishing knowledge on the web [22]. KGs aremodelled according to schemes that are not immutable, as theyare subject to modifications triggered by changes in the domain ofknowledge or in the consumption needs of downstream tasks. Anexample is the need to incorporate additional knowledge about atriple, which is known as statement reification. Reification is usuallyused to include metadata and provenance to existing triples [15].",
        "file_name": "10!1145%3587259!3627554.pdf",
        "file_path": ".\\PDFs\\10!1145%3587259!3627554.pdf"
    },
    {
        "title": "Proof of Location through a Blockchain Agnostic Smart Contract Language",
        "implementation_urls": [],
        "doi": "10.1109/icdcsw60045.2023.00016",
        "arxiv": null,
        "abstract": "Abstract—Location-based services are at the heart of manyapplications that individuals use every day. However, there isoften no guarantee of the truthfulness of users’ location data,since this information can be easily spoofed without a proof mech-anism. In distributed system applications, preventing users fromsubmitting counterfeit locations becomes even more challengingbecause of the lack of a central authority that monitors dataprovenance. In this work, we propose a decentralized architecturebased on blockchains and decentralized technologies, offering atransparent solution for Proof of Location (PoL). We specificallyaddress two main challenges, i.e., the issuing process of the PoLand the proof verification. We describe a smart contract basedimplementation in Reach, a blockchain-agnostic smart contractlanguage, and the tests we conducted on different blockchains,i.e. Ethereum, Polygon, and Algorand, measuring latency andcosts due to the payment of fees. Results confirm the viability ofthe proposal.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Keyword Search, SmartContractsI. INTRODUCTIONNowadays, many users’ activities are supported by a differ-ent number of mobile applications, leveraging their positionto offer specific location-based services. For example, trust-worthy crowd-sourcing of urban or environmental obstaclesfor accessibility purposes [1], [2], customer-loyalty rewardsystems that offer discounts to users who frequently visit theshop, privacy-preserving contact tracing [3].These kinds of systems have at least three issues to copewith. First, some level of trust is needed in the user that crowd-sources some data related to a certain position. This led to theidea of a Proof-of-Location (PoL) because the location couldbe easily spoofed [4]. Second, there is the need to ensure,on the other hand, some privacy guarantees to the users thatgenerate data, so as to avoid everyone being entitled to knowa specific user location at a certain time. Third, the usualapproach is to resort to a centralized system, where a singleentity is responsible for collecting and storing data, users thatgenerated them, and their associated position representing,somehow, a PoL. While this solution can help in dealing withthe two issues above, it raises some concerns on personal dataThis work received funding from the EU H2020 R&D programme under theMSCA ITN EJD grant agreement No 814177 Law Science and TechnologyJoint Doctorate - RIoE.sovereignty, as location data is one of the most sensitive caseswith respect to users’ data exploitation [5], [6].With this in view, in this paper we propose a decentralizedProof of Location (PoL) system, based on blockchain anddistributed storage technologies. Our system is designed to bedecentralized, so as to avoid the presence of single point of",
        "file_name": "10!1109%icdcsw60045!2023!00016.pdf",
        "file_path": ".\\PDFs\\10!1109%icdcsw60045!2023!00016.pdf"
    },
    {
        "title": "Comparison of Knowledge Graph Representations for Consumer Scenarios",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk-browser",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\978-3-031-47240-4_15.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are accessible online for the participants.3,4 2 https://github.com/usc-isi-i2/kgtk-browser/."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47240-4_15",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have been widely adopted across organi-zations and research domains, fueling applications that span interactivebrowsing to large-scale analysis and data science. One design decisionin knowledge graph deployment is choosing a representation that opti-mally supports the application’s consumers. Currently, however, there isno consensus on which representations best support each consumer sce-nario. In this work, we analyze the fitness of popular knowledge graphrepresentations for three consumer scenarios: knowledge exploration, sys-tematic querying, and graph completion. We compare the accessibilityfor knowledge exploration through a user study with dedicated brows-ing interfaces and query endpoints. We assess systematic querying withSPARQL in terms of time and query complexity on both synthetic andreal-world datasets. We measure the impact of various representations onthe popular graph completion task by training graph embedding modelsper representation. We experiment with four representations: StandardReification, N-Ary Relationships, Wikidata qualifiers, and RDF-star. Wefind that Qualifiers and RDF-star are better suited to support use casesof knowledge exploration and systematic querying, while Standard Reifi-cation models perform most consistently for embedding model inferencetasks but may become cumbersome for users. With this study, we aimto provide novel insights into the relevance of the representation choiceand its impact on common knowledge graph consumption scenarios.Keywords: Knowledge Graphs · Knowledge Representation · UserStudy · Graph Completion1 IntroductionThe growth of the knowledge graph (KG) user base has triggered the emergenceof new representational requirements. While RDF is the traditional and standardmodel for KG representation, alternative models such as property graphs [25], theWikidata model [34], and RDF-star [12] have also become recently popular. Thec© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14265, pp. 271–289, 2023.https://doi.org/10.1007/978-3-031-47240-4 15http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47240-4_15&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-6606-9735http://orcid.org/0000-0002-1735-0686http://orcid.org/0000-0001-6921-1744http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-031-47240-4_15272 A. Iglesias-Molina et al.promise of these alternative and complementary representation models is thatthey can provide more flexibility to address certain use cases, such as statementannotation, for which RDF-based representations are not straightforward [17].While the plurality of knowledge representation (KR) models provides the meansto address a wider range of possibilities in consumer scenarios, there is currentlyno consensus nor sufficient empirical evidence on which representations are mostsuitable for different KG consumer tasks [16].Previous studies comparing knowledge representations have focused primar-ily on query performance [2,6,14,26,28] and graph interoperability [3,4]. Forthis scenario, the representations need to ensure efficiency to minimize perfor-",
        "file_name": "978-3-031-47240-4_15.pdf",
        "file_path": "PDFs\\978-3-031-47240-4_15.pdf"
    },
    {
        "title": "The RML Ontology: A Community-Driven Modular Redesign After a Decade of Experience in Mapping Heterogeneous Data to RDF",
        "implementation_urls": [
            {
                "identifier": "https://github.com/OP-TED/ePO",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-031-47243-5_9.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "We publish the complete ontol-ogy at http://w3id.org/rml/, and a summary of all modules with links to all their related resources (i.e."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_9",
        "arxiv": null,
        "abstract": "Abstract. The Relational to RDF Mapping Language (R2RML)became a W3C Recommendation a decade ago. Despite its wide adop-tion, its potential applicability beyond relational databases was swiftlyexplored. As a result, several extensions and new mapping languages wereproposed to tackle the limitations that surfaced as R2RML was appliedin real-world use cases. Over the years, one of these languages, the RDFMapping Language (RML), has gathered a large community of contribu-tors, users, and compliant tools. So far, there has been no well-defined setof features for the mapping language, nor was there a consensus-markingontology. Consequently, it has become challenging for non-experts tofully comprehend and utilize the full range of the language’s capabilities.After three years of work, the W3C Community Group on KnowledgeGraph Construction proposes a new specification for RML. This paperpresents the new modular RML ontology and the accompanying SHACLshapes that complement the specification. We discuss the motivationsand challenges that emerged when extending R2RML, the methodologywe followed to design the new ontology while ensuring its backward com-patibility with R2RML, and the novel features which increase its expres-siveness. The new ontology consolidates the potential of RML, empowerspractitioners to define mapping rules for constructing RDF graphs thatwere previously unattainable, and allows developers to implement sys-tems in adherence with [R2]RML.c© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14266, pp. 152–175, 2023.https://doi.org/10.1007/978-3-031-47243-5_9http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47243-5_9&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-7195-9935http://orcid.org/0000-0002-3029-6469http://orcid.org/0000-0003-0248-0987http://orcid.org/0000-0003-4734-3847http://orcid.org/0000-0003-1702-8707http://orcid.org/0000-0001-9064-0463http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0003-2138-7972https://doi.org/10.1007/978-3-031-47243-5_9The RML Ontology: A Community-Driven Modular Redesign 153Resource type: Ontology/License: CC BY 4.0 InternationalDOI: 10.5281/zenodo.7918478/URL: http://w3id.org/rml/portal/Keywords: Declarative Language · R2RML · RML · KnowledgeGraph1 IntroductionIn 2012, the Relational to RDF Mapping Language (R2RML) [37] was releasedas a W3C Recommendation. The R2RML ontology [8] provides a vocabularyto describe how an RDF graph should be generated from data in a relationaldatabase (RDB). Although R2RML gained wide adoption, its potential applica-bility beyond RDBs quickly appeared as a salient need [49,63,76,87].Targeting the generation of RDF from heterogeneous data sources other thanRDBs, several extensions [49,76,87] preserving R2RML’s core structure wereproposed. As R2RML and the growing number of extensions were applied in a",
        "file_name": "10!1007%978-3-031-47243-5_9.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-47243-5_9.pdf"
    },
    {
        "title": "TEC: Transparent Emissions Calculation Toolkit",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TEC-Toolkit/cfkg",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-031-47243-5_5.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The resultant KG and mappings are publicly available online17 (mappings are available under an Apache 2.0 license) [15]."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_5",
        "arxiv": null,
        "abstract": "Abstract. Greenhouse gas emissions have become a common means fordetermining the carbon footprint of any commercial activity, rangingfrom booking a trip or manufacturing a product to training a machinelearning model. However, calculating the amount of emissions associatedwith these activities can be a difficult task, involving estimations of en-ergy used and considerations of location and time period. In this paper,we introduce the Transparent Emissions Calculation (TEC) toolkit, anopen source effort aimed at addressing this challenge. Our contributionsinclude two ontologies (ECFO and PECO) that represent emissions con-version factors and the provenance traces of carbon emissions calcula-tions (respectively), a public knowledge graph with thousands of conver-sion factors (with their corresponding YARRRML and RML mappings)and a prototype carbon emissions calculator which uses our knowledgegraph to produce a transparent emissions report.Resource permanent URL: https://w3id.org/tec-toolkitKeywords: Ontology · GHG Emissions · Carbon Accounting · Trans-parency1 IntroductionThe Net Zero agenda has gained significant traction across the world, with over40 countries worldwide requiring organisations to periodically calculate and re-port their greenhouse gas (GHG) emissions [29]. Calculating them requires real-world data observations quantifying various aspects of business activities (e.g.,amount of fuel consumed by a fleet of vehicles) and additional resources such asmethodologies for transforming activity data into GHG estimates (also referredto as emissions scores). Reported emissions scores may differ depending on vari-ous factors including the calculation methodology and software used, geopoliticallocation, government requirements for reporting methods, applicable emissions2 M. Markovic et al.conversion factors (ECFs), and the type of reported GHG emissions. Emissionscalculations may also include unintentional errors, such as the use of ECFs whichmight be out of date, from unreliable publishers, or incorrectly applied to a spe-cific activity, thus causing erroneous results. In addition, organisations may havea vested interest in deliberately under-reporting on certain aspects of carbonfootprint if they deem it could have negative impact on the company image [20].While reporting requirements may differ from one country to another, organ-isations are expected to be transparent about their submitted results. Achievingsuch transparency may be challenging as it requires a clear history of whichECFs were used and how the emissions scores were calculated including detailsabout the origin and accuracy of the input data. These details are typicallycommunicated in the form of free text reports which are not suitable for auto-mated processing. However, such transparency is necessary to support assess-ments evaluating the trustworthiness and meaningful comparison of emissionsscores reported by organisations across different sectors over time. We arguethat provenance traces of such calculations described in the form of KnowledgeGraphs (KGs) potentially provide a machine-understandable solution to thischallenge by making the calculations more transparent and providing the meansfor automated processing and analysis. This is a core motivation for our Trans-parent Emissions Calculation (TEC) toolkit which aims to address this issueby providing ontologies and software tools for enhancing the transparency ofemissions calculations using KGs. Our contributions include:",
        "file_name": "10!1007%978-3-031-47243-5_5.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-47243-5_5.pdf"
    },
    {
        "title": "Comparison of Knowledge Graph Representations for Consumer Scenarios",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/kgtk-browser",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\978-3-031-47240-4_15.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All resources are accessible online for the participants.3,4 2 https://github.com/usc-isi-i2/kgtk-browser/."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.7443836",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-47240-4_15",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have been widely adopted across organi-zations and research domains, fueling applications that span interactivebrowsing to large-scale analysis and data science. One design decisionin knowledge graph deployment is choosing a representation that opti-mally supports the application’s consumers. Currently, however, there isno consensus on which representations best support each consumer sce-nario. In this work, we analyze the fitness of popular knowledge graphrepresentations for three consumer scenarios: knowledge exploration, sys-tematic querying, and graph completion. We compare the accessibilityfor knowledge exploration through a user study with dedicated brows-ing interfaces and query endpoints. We assess systematic querying withSPARQL in terms of time and query complexity on both synthetic andreal-world datasets. We measure the impact of various representations onthe popular graph completion task by training graph embedding modelsper representation. We experiment with four representations: StandardReification, N-Ary Relationships, Wikidata qualifiers, and RDF-star. Wefind that Qualifiers and RDF-star are better suited to support use casesof knowledge exploration and systematic querying, while Standard Reifi-cation models perform most consistently for embedding model inferencetasks but may become cumbersome for users. With this study, we aimto provide novel insights into the relevance of the representation choiceand its impact on common knowledge graph consumption scenarios.Keywords: Knowledge Graphs · Knowledge Representation · UserStudy · Graph Completion1 IntroductionThe growth of the knowledge graph (KG) user base has triggered the emergenceof new representational requirements. While RDF is the traditional and standardmodel for KG representation, alternative models such as property graphs [25], theWikidata model [34], and RDF-star [12] have also become recently popular. Thec© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14265, pp. 271–289, 2023.https://doi.org/10.1007/978-3-031-47240-4 15http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47240-4_15&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-6606-9735http://orcid.org/0000-0002-1735-0686http://orcid.org/0000-0001-6921-1744http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-031-47240-4_15272 A. Iglesias-Molina et al.promise of these alternative and complementary representation models is thatthey can provide more flexibility to address certain use cases, such as statementannotation, for which RDF-based representations are not straightforward [17].While the plurality of knowledge representation (KR) models provides the meansto address a wider range of possibilities in consumer scenarios, there is currentlyno consensus nor sufficient empirical evidence on which representations are mostsuitable for different KG consumer tasks [16].Previous studies comparing knowledge representations have focused primar-ily on query performance [2,6,14,26,28] and graph interoperability [3,4]. Forthis scenario, the representations need to ensure efficiency to minimize perfor-",
        "file_name": "978-3-031-47240-4_15.pdf",
        "file_path": "PDFs\\978-3-031-47240-4_15.pdf"
    },
    {
        "title": "Analysis of the Confidence in the Prediction of the Protein Folding by Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-38079-2_9",
        "arxiv": null,
        "abstract": "Abstract. The determination of protein structure has been facilitated using deep learning models, which can predict protein folding from protein sequences. In some cases, the predicted structure can be compared to the already-known distribution if there is information from classic methods such as nuclear magnetic resonance (NMR) spectroscopy, X-ray crystallography, or electron microscopy (EM). However, challenges arise when the proteins are not abundant, their structure is heterogeneous, and protein sample preparation is difficult. To determine the level of confidence that supports the prediction, different metrics are provided. These values are important in two ways: they offer information about the strength of the result and can supply an overall picture of the structure when different models are combined. This work provides an overview of the different deep-learning methods used to predict protein folding and the metrics that support their outputs. The confidence of the model is evaluated in detail using two proteins that contain four domains of unknown function.  Keywords: Protein Structure Prediction, Machine Learning Metrics, Model Confidence.  1 Introduction  Protein folding refers to the mechanism through which a polypeptide chain transforms into its biologically active protein in its 3D structure, and it has a significant impact on different applications, e.g., drug design, protein-protein interaction, and understanding the molecular mechanism of some diseases. There are classic methods to determine the structure of a protein, such as X-ray, NMR, and EM. These methods can be costly and time-consuming because they require significant resources and expertise. Protein folding is a complex process that is challenging for different reasons, including a large number of possible conformations, the crowded cellular environment, and the complex energy landscape required to reach the final structure. The emergence of deep learning methods for predicting protein folding has revolutionized traditional biochemistry. These methods enable in silico predictions, followed by laboratory validation of the findings, offering a new approach to the field. The Critical Assessment of Protein Structure Prediction (CASP) experiments aim to evaluate the current state of the art in protein structure prediction, track the progress made so far, and identify areas where future efforts may be most productively focused. The bi-annual CASP meeting has shown that deep learning methods like AlphaFold, Rosetta, RoseTTAFold and trRosetta, are more effective than traditional approaches that explicitly model the folding process.  AlphaFold2 was introduced as a new computational approach that can predict protein structures with near-experimental accuracy in most cases. The artificial intelligence system, AlphaFold, was submitted to the CASP 14 competition as AlphaFold2, using a completely different model from the previous AlphaFold system in CASP13 [1]. A multimer version has been released, which allows scientists to predict protein complexes and detect protein-protein interactions. The use of AlphaFold-Multimer leads to improved accuracy in predicted multimeric interfaces compared to the input-adapted single-chain AlphaFold, while maintaining a high level of intra-chain accuracy [2]. To fully utilize these methods, researchers require access to powerful computing resources, which is why alternative platforms have been developed. ColabFold is one such platform that offers an accelerated prediction of protein structures and complexes by combining the fast homology search of MMseqs2 with AlphaFold2 [3]. From early 2022, the Galaxy server is able to run AlphaFold 2.0. as part of the tools offered for bioinformatics analysis. DeepMind and EMBL’s European Bioinformatics Institute (EMBL-EBI) did indeed collaborate to create AlphaFold DB, which offers open access to over 200 million protein structure predictions, with the goal of accelerating scientific research [1, 4]. CAMEO (Continuous Automated Model EvaluatiOn) is a community project, developed by the Computational Structural Biology Group, at the SIB Swiss Institute of Bioinformatics and the Biozentrum of the University of Basel. CAMEO is a service that continuously evaluated the accuracy of protein prediction servers based on known experimental structures released by the PDB. Users can submit several models for a target protein in CAMEO, and it will evaluate up to 5 models. Through CAMEO, Robetta, a protein prediction service, undergoes continual evaluation. While deep learning models can achieve impressive accuracy on a wide range of tasks, it’s important to carefully evaluate and interpret their predictions to ensure they are reliable and useful. This involves not only selecting appropriate metrics, but also understanding the underlying assumptions and limitations of the model, and how these can impact its predictions.  The paper is organized in the following order: Section 2 provides a description of the metrics and scores, while Section 3 describes the tools and resources used in this paper. In Section 4, the results obtained are presented and discussed in Section 5. Finally, Section 6 presents the conclusions of this work and outlines future directions for research. ",
        "file_name": "10!1007%978-3-031-38079-2_9.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-38079-2_9.pdf"
    },
    {
        "title": "Declarative generation of RDF-star graphs from heterogeneous data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/morph-kgc/morph-kgc",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-243602.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The test cases are openly available at the W3C Community Group on Knowl-edge Graph Construction [10], and can be reused by any engine to test its conformance with respect to RML-star."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-243602",
        "arxiv": null,
        "abstract": "Abstract. RDF-star has been proposed as an extension of RDF to make statements about statements. Libraries and graph storeshave started adopting RDF-star, but the generation of RDF-star data remains largely unexplored. To allow generating RDF-starfrom heterogeneous data, RML-star was proposed as an extension of RML. However, no system has been developed so far thatimplements the RML-star specification. In this work, we present Morph-KGCstar , which extends the Morph-KGC materializationengine to generate RDF-star datasets. We validate Morph-KGCstar by running test cases derived from the N-Triples-star syntaxtests and we apply it to two real-world use cases from the biomedical and open science domains. We compare the performanceof our approach against other RDF-star generation methods (SPARQL-Anything), showing that Morph-KGCstar scales betterfor large input datasets, but it is slower when processing multiple smaller files.Keywords: Knowledge graphs, RDF-star, RML-star, data integration1. IntroductionRDF-star (originally, RDF* [19]) was proposed as an extension of RDF [11] to make statements about otherstatements (also known as reification [23]). RDF-star extends RDF’s conceptual data model and concrete syntaxesby providing a compact alternative to other reification approaches, such as standard reification [22] or singletonproperties [37]. Following the uptake of the initial version of RDF-star, the W3C RDF-DEV Community Group1released a W3C Final Community Group Report [21] and the RDF-star Working Group2 was formed to extendrelated W3C Recommendations.*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.**The authors contributed equally to this work.1https://www.w3.org/groups/cg/rdf-dev2https://www.w3.org/groups/wg/rdf-star1570-0844 © 2024 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:julian.arenas.guerrero@upm.esmailto:ana.iglesiasm@upm.esmailto:daniel.garijo@upm.esmailto:oscar.corcho@upm.esmailto:anastasia.dimou@kuleuven.bemailto:david.chaves@usc.esmailto:julian.arenas.guerrero@upm.eshttps://www.w3.org/groups/cg/rdf-devhttps://www.w3.org/groups/wg/rdf-starhttps://creativecommons.org/licenses/by/4.0/CORRECTED  PROOF2 J. Arenas-Guerrero et al. / Declarative generation of RDF-star graphs from heterogeneous dataAlthough several libraries and graph stores have already adopted RDF-star,3 the generation of RDF-star graphsremains largely unexplored. RDF graphs are often generated from heterogeneous semi-structured data, e.g., data inCSV, XML or JSON formats, etc. To generate RDF graphs, mapping languages are used to specify how RDF termsand triples can be generated from these data. The syntax of these mapping languages is either custom or repurposed.The syntax of custom mapping languages is designed to generate RDF graphs, such as the W3C RecommendationR2RML [13], for generating RDF from data in relational databases, and its extensions for heterogeneous data, e.g.,RDF Mapping Language (RML) [28] or xR2RML [36]. Alternatively, mapping languages may repurpose an existingsyntax proposed for other scopes, e.g., based on the query language SPARQL [40], such as SPARQL-Generate [32]or SPARQL-Anything [6,12], or on the constraints language ShEx [39], such as ShExML [17].Mapping languages have focused so far on the generation of RDF graphs, but the emergence of RDF-star presentsa new challenge. Depending on the underlying syntax, the mapping languages employ different mechanisms tosupport the generation of RDF graphs. On the one hand, SPARQL-based mapping languages can take advantage ofthe SPARQL-star extension [21] as long as their adjustments to the syntax are not affected and the implementation onwhich they are based allows it. For instance, SPARQL-Anything is built on top of Apache Jena [1], which supportsRDF-star and SPARQL-star. On the other hand, dedicated mapping languages require an extension both over their",
        "file_name": "10!3233%sw-243602.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-243602.pdf"
    },
    {
        "title": "InDaMul: Incentivized Data Mules for Opportunistic Networking Through Smart Contracts and Decentralized Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/umbral-rs",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3587696.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Retrieved from https://github.com/miker83z/umbral-rs."
                    }
                ]
            }
        ],
        "doi": "10.1145/3587696",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1145%3587696.pdf",
        "file_path": ".\\PDFs\\10!1145%3587696.pdf"
    },
    {
        "title": "Data Management Documentation in Citizen Science Projects: Bringing Formalisation and Transparency Together",
        "implementation_urls": [],
        "doi": "10.5334/cstp.538",
        "arxiv": null,
        "abstract": "ABSTRACTCitizen science (CS) is a way to open up the scientific process, to make it more accessible and inclusive, and to bring professional scientists and the public together in shared endeavours to advance knowledge. Many initiatives engage citizens in the collection or curation of data, but do not state what happens with such data. Making data open is increasingly common and compulsory in professional science. To conduct transparent, open science with citizens, citizens need to be able to understand what happens with the data they contribute. Data management documentation (DMD) can increase understanding of and trust in citizen science data, improve data quality and accessibility, and increase the reproducibility of experiments. However, such documentation is often designed for specialists rather than amateurs.This paper analyses the use of DMD in CS projects. We present analysis of a qualitative survey and assessment of projects’ DMD, and four vignettes of data management practices. Since most projects in our sample did not have DMD, we further analyse their reasons for not doing so. We discuss the benefits and challenges of different forms of DMD, and barriers to having it, which include a lack of resources, a lack of awareness of tools to support DMD development, and the inaccessibility of existing tools to citizen scientists without formal scientific education. We conclude that, to maximise the inclusivity of citizen science, tools and templates need to be made more accessible for non-experts in data management.mailto:gefion.thuermer@kcl.ac.ukhttps://doi.org/10.5334/cstp.538https://orcid.org/0000-0001-7345-0000https://orcid.org/0000-0003-4112-6825https://orcid.org/0000-0002-1044-3943https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-1722-947X2Thuermer et al. Citizen Science: Theory and Practice DOI: 10.5334/cstp.538Citizen science projects can help advance scientific knowledge, and educate participants about specific topics and the scientific process in general (Bonney et al. 2009). These projects occur at different scales, from local, such as the iSPEX project (http://ispex-eu.org), where citizen scientists use sensors to measure air quality (Volten et al. 2018), to international, such as eBird (https://ebird.org), an online platform used globally to record bird observations (Lagoze 2014). Citizens may create such projects from the bottom up, with or without the support of professional scientists; conduct data collection or analysis in scientist-led projects (Wiggins and Crowston 2011); or contribute to scientific publications (Tinati et al. 2015).The implementation of data management policies can make data and projects more scientifically sound, improve data quality and accessibility, and increase reproducibility. In CS projects, data management is an essential activity that enables citizen scientists to produce data that can be relevant and useful for, and trusted by, researchers (Hunter, Alabri and Ingen 2013). However, in many projects, data management policies or documentation are not systematically applied, leading to the perception that ",
        "file_name": "647dce1d15fad.pdf",
        "file_path": "PDFs\\647dce1d15fad.pdf"
    },
    {
        "title": "Boosting Knowledge Graph Generation from Tabular Data with RML Views",
        "implementation_urls": [
            {
                "identifier": "https://github.com/morph-kgc/morph-kgc",
                "type": "git",
                "paper_frequency": 12,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-031-33455-9_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Materials are publicly available in Zenodo [4]."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-031-33455-9_29",
        "arxiv": null,
        "abstract": "Abstract. A large amount of data is available in tabular form. RMLis commonly used to declare how such data can be transformed intoRDF. However, RML presents limitations that lead, in many cases, tothe need for additional preprocessing using scripting. Although someproposed extensions (e.g., FnO or RML fields) address some of theselimitations, they are verbose, unfamiliar to most data engineers, andimplemented in systems that do not scale up when large volumes of dataneed to be processed. In this work, we expand RML views to tabularsources so as to address the limitations of this mapping language. In thisway, transformation functions, complex joins, or mixed syntax can bedefined directly in SQL queries. We present our extension of Morph-KGCto efficiently support RML views for tabular sources. We validate ourimplementation adapting R2RML test cases with views and compareit against state-of-the-art RML+FnO systems showing that our systemis significantly more scalable. Moreover, we present specific examplesof a real use case in the public procurement domain where basic RMLmappings could not be used without additional preprocessing.Resource type: Software frameworkLicense: Apache 2.0DOI: 10.5281/zenodo.7385488URL: https://github.com/morph-kgc/morph-kgcKeywords: Knowledge Graph · RML · CSV · Data Integration1 IntroductionAn extensive amount of data is stored as CSV, Microsoft Excel spreadsheets,and other tabular formats such as Apache Parquet [3] or Apache ORC [2].Many organizations are also transforming these data sources into RDF knowl-edge graphs [30] (KGs), given their potential to integrate, represent, and publishheterogeneous data according to the model given by one or several ontologies.Data transformations from tabular sources into RDF are typically definedin a systematic manner using mapping languages [43]. These languages increasethe maintainability of the data integration pipelines and prevent the use of ex-ternal scripting [13]. In addition, mappings leverage specialized data integrationhttps://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0001-8637-6313https://orcid.org/0000-0003-1011-5023https://orcid.org/0000-0003-2949-3307https://orcid.org/0000-0002-9260-0753https://github.com/morph-kgc/morph-kgc2 Arenas-Guerrero et al.systems that come with rich functionality and are optimized for large-scale usecases.The RDF Mapping Language [23] (RML) is a popular language [10] that ex-tends theW3C Recommendation RDB to RDFMapping Language [17] (R2RML)to data formats beyond relational databases (RDBs). In real-world data integra-tion scenarios, some computations, such as transformation functions, complexjoins, or extraction of embedded values, need to be applied to the input data.R2RML enables these computations by wrangling the data using SQL queriesin the mappings that are executed over RDBs. However, RML does not allowthis for tabular sources, which limits the capabilities of the mapping languagefor these common scenarios.",
        "file_name": "10!1007%978-3-031-33455-9_29.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-33455-9_29.pdf"
    },
    {
        "title": "Editorial of transport data on the web",
        "implementation_urls": [],
        "doi": "10.3233/sw-223278",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!3233%sw-223278.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-223278.pdf"
    },
    {
        "title": "Applying the LOT Methodology to a Public Bus Transport Ontology aligned with Transmodel: Challenges and Results",
        "implementation_urls": [
            {
                "identifier": "https://github.com/CiudadesAbiertas/vocab-transporte-autobus",
                "type": "git",
                "paper_frequency": 12,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-210451.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Its corresponding GitHub repository with all the intermediate and final artefacts is available at https://github.com/CiudadesAbiertas/vocab-transporte-autobus including a Readme in English.6 The scope of this ontology on public bus transportation is focused on the representation of static information related to lines, routes, stops and timetables, and real time information on expected arrival times to bus stops."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210451",
        "arxiv": null,
        "abstract": "Abstract. We present an ontology that describes the domain of Public Transport by bus, which is common in cities around theworld. This ontology is aligned to Transmodel, a reference model which is available as a UML specification and which was devel-oped to foster interoperability of data about transport systems across Europe. The alignment with this non-ontological resourcerequired the adaptation of the Linked Open Terms (LOT) methodology, which has been used by our team as the methodologicalframework for the development of many ontologies used for the publication of open city data. The ontology is structured intothree main modules: (1) agencies, operators and the lines that they manage, (2) lines, routes, stops and journey patterns, and (3)planned vehicle journeys with their timetables and service calendars. Besides reusing Transmodel concepts, the ontology alsoreuses common ontology design patterns from GeoSPARQL and the SOSA ontology. As part of the LOT data-driven valida-tion stage, RDF data has been generated taking as input the GTFS feeds (General Transit Feed Specification) provided by theMadrid public bus transport provider (EMT). Mapping rules from structured data sources to RDF were developed using the RDFMapping Language (RML) to generate RDF data, and queries corresponding to competency questions were tested.Keywords: Ontology, Transmodel, public bus, Open Cities, RDF1. IntroductionOpen data initiatives across public administrations worldwide date back to more than a decade ago. In the specificcase of Spanish cities, the most relevant landmarks are associated to the first transposition of the EU Public SectorInformation directive in 2007,1 the publication of the UNE 178301:2015 technical norm on Open Data for SmartCities,2 and the development of the open data guide by the Spanish Federation of Municipalities and Provinces(FEMP) in 2017 [8] and the catalogue of high-value open datasets for cities in 2019 [9].*Corresponding author. E-mail: eruckhaus@fi.upm.es.1https://eur-lex.europa.eu/eli/dir/2003/98, https://www.boe.es/eli/es/l/2007/11/16/37/con.2https://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N00543181570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:eruckhaus@fi.upm.esmailto:adolfo.anton.bravo@upm.esmailto:ocorcho@fi.upm.esmailto:mario.scrocca@cefriel.commailto:eruckhaus@fi.upm.eshttps://eur-lex.europa.eu/eli/dir/2003/98https://www.boe.es/eli/es/l/2007/11/16/37/conhttps://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N0054318https://creativecommons.org/licenses/by/4.0/640 E. Ruckhaus et al. / Applying the LOT Methodology to a Public Bus Transport Ontology aligned with TransmodelDomains that have been addressed in these initiatives include public sector, demography, environment, economy,commerce, transport and treasury, among others. As part of the initiatives and projects that have led the advance-ment of open data among cities in Spain we can cite the Ciudades Abiertas3 project, a public-private collaborativeproject led by four Spanish municipalities (Zaragoza, Madrid, Santiago de Compostela and A Coruña) with thegeneral aim to facilitate the implementation of common Open Government policies that are reusable by many othermunicipalities inside and outside Spain.Among the project actions on open data, several (12) ontologies are being developed using the Linked OpenTerms (LOT) methodology [5,17]. These ontologies allow publishing Open Data homogeneously across cities,using common CSU structures, as well as following Linked Data principles [29]. They are being added to thosethat had been already developed in the context of the Spanish network of Open Data for Smart Cities,4 and theycorrespond to a subset of the catalogue of datasets included in the aforementioned FEMP open data guide [9]. Allof the ontologies are publicly available and versioned in GitHub,5 with the corresponding repositories including usecases and user stories, requirements, the ontology implementation in OWL, the ontology HTML documentation inSpanish and English, and example data and queries.In the area of transport, three ontologies have been developed so far under the umbrella of these initiatives,focused on the representation of open data about Public Bicycles, Motor Vehicle Traffic and Public Bus Transport.In this paper, we will discuss the latter, an ontology that has been specifically developed for structuring how to",
        "file_name": "10!3233%sw-210451.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-210451.pdf"
    },
    {
        "title": "Accountable Clouds Through Blockchain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/cloud-chain",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%access!2023!3276240.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/miker83z/cloud-chain [39] E."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2023.3276240",
        "arxiv": null,
        "abstract": "ABSTRACT We present a solution for accountability in Cloud infrastructures based on blockchain.We showthat, through smart contracts, it is possible to create an unforgeable log that can be used for auditing andautomatic Service Level Agreement (SLA) verification. As a practical case study, we consider Cloud storageservices and define interaction protocols for registering the outcome of each file operation in the blockchain.We developed a prototype implementation that runs on the GoQuorum, Hyperledger Besu, and Polygonblockchains, using different consensus protocols. Using a dedicated testbed, we discuss the performanceof our implementation in terms of latencies, error rates and gas usage. Results demonstrate the viabilityof our approach over permissioned blockchains, with better performance for the Polygon and GoQuorumRaft decentralized systems. Our implementation enables interoperability, given that it is supported by theEthereum Virtual Machine which currently is underlying several blockchain platforms.INDEX TERMS Blockchain, smart contracts, cloud computing.I. INTRODUCTIONCloud computing is a well-established paradigm for provid-ing computation and storage resources according to a ‘‘payas you go’’ model. In Cloud computing, service providersown computing resources and provide remote access to thoseresources to customers for a fee [2].The level of abstraction at which a customer interactswith a Cloud infrastructure is defined by the servicemodel. In a Software as a Service (SaaS) Cloud, cus-tomers are provided with application services running in theCloud infrastructure. ‘‘Google Workspace’’ and ‘‘MicrosoftOffice Online’’ are examples of widely used SaaS Clouds.A Platform as a Service (PaaS) Cloud provides program-ming languages, tools, and a hosting environment for appli-cations developed by the customer. Examples of PaaS solu-tions are AppEngine by Google, Force.com from SalesForce,The associate editor coordinating the review of this manuscript andapproving it for publication was Nitin Gupta .Microsoft’s Azure, and Amazon’s Elastic Beanstalk. Finally,an Infrastructure as a Service (IaaS) Cloud provides low-levelcomputing capabilities such as processing, storage, and net-works where the customer can run arbitrary software, includ-ing operating systems and applications. Amazon EC2 is anexample of IaaS Cloud.The mode of operation of a Cloud defines its deploymentmodel. A Private Cloud is operated exclusively for a cus-tomer organization; it might be managed or owned by thatorganization, although this is not required. A CommunityCloud is shared by several organizations and supports aspecific community with common concerns (e.g., regulatoryrequirements). A Public Cloud is made available to the gen-eral public and is owned by an organization selling Cloudservices. Finally, a Hybrid Cloud is built upon a combinationof private, public, and community Clouds.Cloud computing allows separation between constructionand operation of the infrastructure and providing end-userservices. This opportunity enables the existence of at least48358This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.",
        "file_name": "10!1109%access!2023!3276240.pdf",
        "file_path": ".\\PDFs\\10!1109%access!2023!3276240.pdf"
    },
    {
        "title": "On the Decentralization of Health Systems for Data Availability: a DLT-based Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ccnc51644.2023.10059701",
        "arxiv": null,
        "abstract": "Abstract—Mobile devices entered people’s lives by leaps andbounds, offering various applications relying on private third-party entities to manage their users’ data. Centralized control ofpersonal health data endangers the privacy of the users directlyinvolved. In the future, there will likely be a trend towarddecentralizing the health data collection, relieving central entitiesof this task. This comes with several challenges in a decentralizedenvironment, such as avoiding a single point of failure to guaran-tee data availability. The following work proposes an architecturebased on Distributed Ledger Technology to allow users to decideon their data while ensuring availability by employing socialnetworks. We will outline the mechanisms behind data storageand the implications of using smart contracts in the architecture.In concluding the work, we show the developed architecture andresults deriving from its assessment, highlighting possible usecases applied to the specific health data management context.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed Storage, Social NetworksI. INTRODUCTIONPersonal digital technologies are constantly evolving and arethe primary source of information generation. They broughta fundamental transformation in people’s lives, but the datagenerated usually ends up in private use for reasons relatedto the privacy of an individual. The shift to a decentralizedparadigm now seems immediate, not only to protect theindividual but, more importantly, to enable new technologiesrevolving around data management. Storing data in centralizeddata silos makes it inaccessible to the public and discon-nected from other data [1], [2]. This mechanism hampersinnovation above all. Based on this, interest in decentralizingdata management is proliferating and with great promise toenable these conditions. The healthcare sector can benefitsignificantly from decentralizing information from centralizedsystems. This is because there are so many new implications ofdoing this, ranging from contributing personally to advancingnew medical studies in a disintermediate way and gettingnew solutions on your own directly from devices that becomeincredibly efficient and intelligent [3], [4]. Indeed, this pathis not easy, there are considerable barriers to be addressed interms of security and privacy, but it is the most suitable visionto enable the digital health field.This work has received funding from Regione Marche with DDPF n. 1189,the EU’s Horizon 2020 research and innovation programme under the MSCAITN grant agreement No 814177 LAST-JD-RIoE and from the University ofUrbino through the “Bit4Food” research project.Assuming we had such a decentralized approach available,we could enable a new way of exploiting data in the healthcarespace by storing our data ourselves. However, new issueswould be introduced, such as the data availability problem.The availability of a piece of data indicates its ability to be",
        "file_name": "10!1109%ccnc51644!2023!10059701.pdf",
        "file_path": ".\\PDFs\\10!1109%ccnc51644!2023!10059701.pdf"
    },
    {
        "title": "Modelling of the Internet Computer Protocol Architecture: The Next Generation Blockchain",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-21229-1_1",
        "arxiv": null,
        "abstract": "Abstract. The Internet Computer Protocol is described as a third-generation blockchain system that aims to provide secure and scalabledistributed systems through blockchains and smart contracts. In thisposition paper, this innovative architecture is introduced and then dis-cussed in view of its modeling and simulation aspects. In fact, a properlydefined digital twin of the Internet Computer Protocol could help its de-sign, development, and evaluation in terms of performance and resilienceto specific security attacks. To this extent, we propose a multi-level sim-ulation model that follows an agent-based paradigm. The main issuesof the modeling and simulation, and the main expected outcomes, aredescribed and discussed.Keywords: Internet Computer · Distributed Ledger Technology · Mod-elling and Simulation · Blockchain.1 IntroductionCloud computing has undoubtedly been the fastest growing and most success-ful in delivering technical and economic benefits for application and systemdevelopment in recent years [30, 26]. Starting from startups up to large com-panies, everyone is adopting cloud computing to get rid of the risk of capitalinvestment, cutting the cost of hardware and software infrastructure, and avail-ing themselves of services according to their demand. This is why paradigmssuch as ’Infrastructure-as-a-Service (IaaS)’, ’Platform-as-a-Service (PaaS)’, and’Software-as-a-Service (SaaS)’ have emerged. In general, however, cloud serviceproviders maintain their customers with an opaque knowledge about the location⋆ This work has received funding from the EU H2020 research and innovation pro-gramme under the MSCA ITN grant agreement No 814177 LAST-JD-RIoE; and theresearch grant (No.: RP/ESCA-04/2020) offered by Macao Polytechnic University.2 Li et al.and storage of data, the privacy offered to users, and the type of hardware in-frastructure used. This leads firstly to a problem of trust by users [19]. Secondly,security and privacy are undermined by the centrality of these solutions, whichmore easily attracts cyber-attacks, i.e. single points of failure [30]. In addition, itshould not be forgotten that centralized solutions will not be able to support thehuge amount of data generated globally by users and Internet-of-Things devicesfor much longer [26]. Finally, it is commonly difficult to assess if Quality of Ser-vice (QoS) guarantees are met and Service Level Agreements (SLA) negotiatedbetween users and the cloud provider are satisfied, due to the absence of trustedlogs [5]. All this motivates the transition towards a completely decentralized ap-proach. The benefits of this solution are many. In fact, the decentralization ofthe system removes the presence of a single point of failure, allows for inher-ently increasing scalability, curbs illicit activities of malicious nodes, and canalso provide for accountability guarantees. Clearly, in order to realize a similarkind of system, it becomes necessary to encourage node participation that canbe somehow rewarded through incentive mechanisms [33].The Internet Computer Protocol (ICP) architecture5 aims to establish a net-work of networks by defining a protocol for combining the resources of severaldecentralized computers into the reading, replication, modification, and pro-curement of an application state. A network of nodes runs the protocol throughindependently-operated data centers to provide general-purpose (largely) trans-parent computations for end-users. On the other hand, the development of appli-cations on top of the ICP is facilitated by reliable message delivery, transparent",
        "file_name": "10!1007%978-3-031-21229-1_1.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-21229-1_1.pdf"
    },
    {
        "title": "A combination of supervised dimensionality reduction and learning methods to forecast solar radiation",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.6856079",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "DOI",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1007/s10489-022-04175-y",
        "arxiv": null,
        "abstract": "AbstractMachine learning is routinely used to forecast solar radiation from inputs, which are forecasts of meteorological variablesprovided by numerical weather prediction (NWP) models, on a spatially distributed grid. However, the number of featuresresulting from these grids is usually large, especially if several vertical levels are included. Principal Components Analysis(PCA) is one of the simplest and most widely-used methods to extract features and reduce dimensionality in renewableenergy forecasting, although this method has some limitations. First, it performs a global linear analysis, and second it isan unsupervised method. Locality Preserving Projection (LPP) overcomes the locality problem, and recently the LinearOptimal Low-Rank (LOL) method has extended Linear Discriminant Analysis (LDA) to be applicable when the numberof features is larger than the number of samples. Supervised Nonnegative Matrix Factorization (SNMF) also achieves thisgoal extending the Nonnegative Matrix Factorization (NMF) framework to integrate the logistic regression loss function.In this article we try to overcome all these issues together by proposing a Supervised Local Maximum Variance Preserving(SLMVP) method, a supervised non-linear method for feature extraction and dimensionality reduction. PCA, LPP, LOL,SNMF and SLMVP have been compared on Global Horizontal Irradiance (GHI) and Direct Normal Irradiance (DNI)radiation data at two different Iberian locations: Seville and Lisbon. Results show that for both kinds of radiation (GHI andDNI) and the two locations, SLMVP produces smaller MAE errors than PCA, LPP, LOL, and SNMF, around 4.92% betterfor Seville and 3.12% for Lisbon. It has also been shown that, although SLMVP, PCA, and LPP benefit from using a non-linear regression method (Gradient Boosting in this work), this benefit is larger for PCA and LPP because SMLVP is ableto perform non-linear transformations of inputs.Keywords Dimensionality reduction · Hybrid learning · Solar radiation forecast · Data mining1 IntroductionConsiderable efforts have been made in the past decadesto make solar energy a real alternative to the conventionalenergy generation system. There are two main technologies,solar thermal electricity (STE) and solar photovoltaic (PV)energy, and many countries have already reached a notablesolar share in their energy mixes. Moreover, importantgrowth is expected in the near future (International EnergyAgency, 2018).Contrary to conventional generation, solar electricitygeneration is conditioned by weather, and thus it is highly� Esteban Garcı́a-Cuestaesteban.garcia@fi.upm.esExtended author information available on the last page of the article.intermittent. Transient clouds and aerosol intermittencylead to considerable variability in the solar power plantsyield on a wide range of temporal scales, particularlyin minutes to hours time scales. This presents seriousissues regarding solar power plant management and theiryield integration into the electricity grid [1]. Currently, inaddition to expensive storage-based solutions, the use ofsolar radiation forecasts is the only plausible way to mitigatethe intermittency. Therefore, the development of accuratesolar radiation forecasting methods has become an essentialresearch topic [2].Solar forecasting methods can be classified dependingon the forecasting horizon. Nowcasting methods are mostlyrelated to one-hour ahead forecasts, short-term forecastingwith up to 6 hours ahead forecasts and forecasting methodsare aimed at producing days ahead forecasts. The techniquesassociated with these methods are essentially different [3–",
        "file_name": "s10489-022-04175-y.pdf",
        "file_path": "PDFs\\s10489-022-04175-y.pdf"
    },
    {
        "title": "Complex queries over decentralised systems for geodata retrieval",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1049%ntw2!12037.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "DATA AVAILABILITY STATEMENT The data that support the findings of this study are openly available in Zenodo at https://doi.org/10.5281/zenodo."
                    }
                ]
            }
        ],
        "doi": "10.1049/ntw2.12037",
        "arxiv": null,
        "abstract": "AbstractDecentralised systems have been proved to be quite effective to allow for trusted andaccountable data sharing, without the need to resort to a centralised party that collects allthe information. While complete decentralisation provides important advantages in termsof data sovereignty, absence of bottlenecks and reliability, it also adds some issues con-cerned with efficient data lookup and the possibility to implement complex querieswithout reintroducing centralised components. In this paper, we describe a system thatcopes with these issues, thanks to a multi‐layer lookup scheme based on Distributed HashTables that allows for multiple keyword‐based searches. The service of peer nodesparticipating in this discovery service is controlled and rewarded for their contribution.Moreover, the governance of this process is completely automated through the use ofsmart contracts, thus building a Decentralised Autonomous Organization (DAO). Finally,we present a use case where road hazards are collected in order to test the goodness ofour system for geodata retrieval. Then, we show results from a performance evaluationthat confirm the viability of the proposal.1 | INTRODUCTIONThe digitalisation process, which has been ongoing over thelast decades, has seen data management and data deliverybecome crucial issues. The transformation brought about bydigital technologies has data at its core and it had a significantimpact on economies and societies around the world. Theability to easily get hold of data has the potential to createseveral new services based on data and new markets wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is notfrom a dubious (or false) origin is often a challenge. In order tocope with the increasingly higher number of content that isdemanded through the Web, multiple solutions for efficient useof the Internet have been designed. In particular, thanks to thedecentralisation of content storage and delivery, it is possible toavoid the single point of failure, while reducing the workload atdata centres and allowing a distribution of data that remains‘closer’ to the original data producer. Decentralisation alsofosters the creation of open systems, where participants canfreely join the system and then contribute to its functioning.Recently, Distributed Ledger Technologies (DLTs) and arealm of decentralised systems, for example, Decentralised FileStorages (DFS), have emerged as Peer‐to‐Peer (P2P) technol-ogies capable of offering interesting features related to datavalidation and trustfulness [2, 3]. DLTs have gained popularitywith the advent of cryptocurrencies, which allow users to tradecrypto‐assets without any central entity being involved,ensuring transparency and data integrity. By creating a com-mon, decentralised and trustless infrastructure, it will bepossible for data consumers and providers to interact andcollaborate in P2P interactions [4, 5]. Benefits often cited ofDLTs, indeed, include the enabling of secure transactions be-tween untrusted parties through consensus mechanisms, highavailability, and the ability to automate and enforce processesthrough smart contracts [6]. Besides the financial use case,",
        "file_name": "10!1049%ntw2!12037.pdf",
        "file_path": ".\\PDFs\\10!1049%ntw2!12037.pdf"
    },
    {
        "title": "OBOE: an Explainable Text Classification Framework",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2022.11.001",
        "arxiv": null,
        "abstract": "AbstractExplainable Artificial Intelligence (XAI) has recently gained visibility as one of the main topics of Artificial Intelligence research due to, among others, the need to provide a meaningful justification of the reasons behind the decision of black-box algorithms. Current approaches are based on model agnostic or ad-hoc solutions and, although there are frameworks that define workflows to generate meaningful explanations, a text classification framework that provides such explanations considering the different ingredients involved in the classification process (data, model, explanations, and users) is still missing. With the intention of covering this research gap, in this paper we present a text classification framework called OBOE (explanatiOns Based On concEpts), in which such ingredients play an active role to open the black-box. OBOE defines different components whose implementation can be customized and, thus, explanations are adapted to specific contexts. We also provide a tailored implementation to show the customization capability of OBOE. Additionally, we performed (a) a validation of the implemented framework to evaluate the performance using different corpora and (b) a user-based evaluation of the explanations provided by OBOE. The latter evaluation shows that the explanations generated in natural language express the reason for the classification results in a way that is comprehensible to non-technical users.DOI:  10.9781/ijimai.2022.11.001OBOE: an Explainable Text Classification FrameworkRaúl A. del Águila Escobar1, Mari Carmen Suárez-Figueroa2, Mariano Fernández-López3 *1 Universidad Politécnica de Madrid (UPM), Boadilla del Monte (Spain) 2 Ontology Engineering Group (OEG), Universidad Politécnica de Madrid (UPM), Boadilla del Monte (Spain)3 Department of Information Technology, Escuela Politécnica Superior, Universidad CEU-San Pablo, Boadilla del Monte (Spain)* Corresponding author: r.delaguila@alumnos.upm.es (R. A. del Águila Escobar), mcsuarez@fi.upm.es (M. C. Suárez-Figueroa), mfernandez.eps@ceu.es (M. Fernández-López).Received 18 October 2021 | Accepted 27 September 2022 | Early Access 3 November 2022 I. IntroductionAs a consequence of the wide use of black-box algorithms and the need to provide the justification that supports a classification result, eXplainable Artificial Intelligence (XAI), set up as an initiative, is one of the most relevant research topics in the last years.Conceptually, a text classification problem is no different from other classification problems, so the same ingredients are involved in solving the problem: data, model, users (final users or model developers) and the context of the classification problem.  Therefore, the challenges and questions that text classification tries to answer from an XAI perspective are the same: the need to specify the reasons behind the decision of the model (why question), the context of the explanation (what for question), how the model arrived at a conclusion (how question) or the data and problem of the classification (what question). However, all these ingredients and questions are not being considering together in a system to provide meaningful explanations [1]. The aim of this paper is twofold. Firstly, we present a customizable framework called OBOE (explanatiOns Based On concEpts) for explaining classification of texts. This framework defines a workflow that can be customized and allows all the ingredients to play an active role in the classification process. Furthermore, these ingredients work together to answer the questions that allow the black-box to be opened for final users and model developers by defining the following features: (a) Explanation Generation Workflow (how, why): there is an explicit and defined workflow for generating meaningful explanations for the users; (b) Data as key ingredient (what question): data is ",
        "file_name": "10!9781%ijimai!2022!11!001.pdf",
        "file_path": ".\\PDFs\\10!9781%ijimai!2022!11!001.pdf"
    },
    {
        "title": "Phenotypes of non-alcoholic fatty liver disease (NAFLD) and all-cause mortality: unsupervised machine learning analysis of NHANES III",
        "implementation_urls": [],
        "doi": "10.1136/bmjopen-2022-067203",
        "arxiv": null,
        "abstract": "ABSTRACTObjectives Non- alcoholic fatty liver disease (NAFLD) is a non- communicable disease with a rising prevalence worldwide and with large burden for patients and health systems. To date, the presence of unique phenotypes in patients with NAFLD has not been studied, and their identification could inform precision medicine and public health with pragmatic implications in personalised management and care for patients with NAFLD.Design Cross- sectional and prospective (up to 31 December 2019) analysis of National Health and Nutrition Examination Survey III (1988–1994).Primary and secondary outcomes measures NAFLD diagnosis was based on liver ultrasound. The following predictors informed an unsupervised machine learning algorithm (k- means): body mass index, waist circumference, systolic blood pressure (SBP), plasma glucose, total cholesterol, triglycerides, liver enzymes alanine aminotransferase, aspartate aminotransferase and gamma glutamyl transferase. We summarised (means) and compared the predictors across clusters. We used Cox proportional hazard models to quantify the all- cause mortality risk associated with each cluster.Results 1652 patients with NAFLD (mean age 47.2 years and 51.5% women) were grouped into 3 clusters: anthro- SBP- glucose (6.36%; highest levels of anthropometrics, SBP and glucose), lipid- liver (10.35%; highest levels of lipid and liver enzymes) and average (83.29%; predictors at average levels). Compared with the average phenotype, the anthro- SBP- glucose phenotype had higher all- cause mortality risk (aHR=2.88; 95% CI: 2.26 to 3.67); the lipid- liver phenotype was not associated with higher all- cause mortality risk (aHR=1.11; 95% CI: 0.86 to 1.42).Conclusions There is heterogeneity in patients with NAFLD, whom can be divided into three phenotypes with different mortality risk. These phenotypes could guide specific interventions and management plans, thus advancing precision medicine and public health for patients with NAFLD.INTRODUCTIONThe epidemiology of non- communicable diseases (NCDs) is largely driven by cardiometabolic risk factors and diseases, namely dyslipidaemias, type 2 diabetes mellitus (T2DM), hypertension and cardio-vascular diseases. Nonetheless, there are other NCDs rapidly growing along with, and as a consequence of,1 2 the afore- mentioned cardiometabolic conditions. Non- alcoholic fatty liver disease (NAFLD) is an outstanding ",
        "file_name": "e067203.full.pdf",
        "file_path": "PDFs\\e067203.full.pdf"
    },
    {
        "title": "Challenges for FAIR Digital Object Assessment",
        "implementation_urls": [],
        "doi": "10.3897/rio.8.e95943",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "esT5s: A Spanish Model for Text Summarization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/t5-spanish-news-summarization",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%ssw220020.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "For the sake of reproducibility, the source code is available 3."
                    }
                ]
            }
        ],
        "doi": "10.3233/ssw220020",
        "arxiv": null,
        "abstract": "Abstract. Deep Learning models based on the Transformer architecture have rev-olutionized the state of the art of NLP tasks. As English is the language in whichmost significant advances are made, languages like Spanish require specific train-ing, but this training has a computational cost so high that only big corporationswith servers and GPUs are capable of generating them. This work has exploredhow to create a model for the Spanish language from a big multilingual model.Specifically, a model aimed at creating text summarization, a very common taskin NLP. The results, concerning the quality of the summarization (ROUGE score),point out that these small models, for a specific language, achieve similar resultsthan much bigger models, with a reasonable training in terms of time required andcomputational power, and are significantly faster at inference.Keywords. Deep learning, T5, Spanish, Text summarization1. IntroductionSummarization is a Natural Language Processing task that consists of condensing themost relevant information from a document. This task can be divided into two categories:extractive summarization and abstractive summarization. Extractive summarization con-sists of identifying and copying the most relevant and useful information pieces (typicallysentences) from the original content. In contrast, abstractive summarization requires adeeper understanding of the language to summarize the most relevant content, paraphras-ing the original sentences, combining and using synonyms or new words, without losinginformation and preserving cohesion and coherence [1]. Thus, abstractive summarizationis a difficult task in natural language processing.Currently, the state-of-the-art of language models based on transformers [2] havereached a high level of language comprehension. However, all research is mainly focusedon the English language and then applied to other languages. Even important languagessuch as Spanish, which is the second language spoken in the world, has an enormous1Corresponding Author: Mariano Rico; E-mail:mariano.rico@upm.es.The authors gratefully acknowledge the computer resources at Artemisa, funded by the European UnionERDF and Comunitat Valenciana as well as the technical support provided by the Instituto de Fı́sicaCorpuscular, IFIC (CSIC-UV). Also we acknowledge the Universidad Politécnica de Madrid for providingcomputing resources on Magerit Supercomputer. This work was funded partially by the project KnowledgeSpaces (PID2020-118274RB-I00), funded by MCIN/AEI/ 10.13039/501100011033; and project HCommonK(RTC2019-007134-7, funded by MCIN/AEI/ 10.13039/501100011033).Towards a Knowledge-Aware AIA. Dimou et al. (Eds.)© 2022 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution License 4.0 (CC BY 4.0).doi:10.3233/SSW220020184gap in their language models compared to English [3]. For example, the T5 model [4]is one of the best language models, which exploits the features of text-to-text transferlearning and it is usually used for the text summarization task, it is only trained forEnglish language, and there is no Spanish version yet.Despite this lack of models for non-English languages, there are multilingual mod-els (also including English). This is the case of the multilingual T5 (mT5) [5] which istrained in 101 languages, including English and Spanish among them. These multilin-gual approaches outperform monolingual models because similar languages have posi-tive transfer between them [6]. However, the effort required to create (and also execute)these multilingual models is very high. Our approach takes advantage of these multi-",
        "file_name": "10!3233%ssw220020.pdf",
        "file_path": ".\\PDFs\\10!3233%ssw220020.pdf"
    },
    {
        "title": "Analysis of ontologies and policy languages to represent information flows in GDPR",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/SotAResources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-223009.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "5148947, and its public repository can be accessed by the community at https://github.com/besteves4/SotAResources for further development."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5148947",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-223009",
        "arxiv": null,
        "abstract": "Abstract. This article surveys existing vocabularies, ontologies and policy languages that can be used to represent informationalitems referenced in GDPR rights and obligations, such as the ‘notification of a data breach’, the ‘controller’s identity’ or a‘DPIA’. Rights and obligations in GDPR are analyzed in terms of information flows between different stakeholders, and acomplete collection of 57 different informational items that are mentioned by GDPR is described. 13 privacy-related policylanguages and 9 data protection vocabularies and ontologies are studied in relation to this list of informational items. ODRLand LegalRuleML emerge as the languages that can respond positively to a greater number of the defined comparison criteria ifcomplemented with DPV and GDPRtEXT, since 39 out of the 57 informational items can be modelled. Online supplementarymaterial is provided, including a simple search application and a taxonomy of the identified entities.Keywords: Privacy policy languages, data protection ontologies, GDPR, rights, obligations1. IntroductionWestin [96] shaped the way we define online privacy before the web existed at all. One of his two major postulateswas that individuals should be able to determine to what extent information about them is communicated to others.The second of these postulates was that technological artifacts could be used to achieve this goal. His books in thelate sixties and the seventies exerted significant influence on the privacy legislation that was enacted in the followingyears, and even today, the European General Data Protection Regulation (GDPR), which came into full effect onMay 25th of 2018, owes much to his work. Any information system has data representational needs, and privacy anddata protection related information systems will have to represent ideas such as ‘consent’ or ‘the right to erasure’. Ifthese applications are to interoperate, then the need for standard formats is clear, and the adoption of semantic-webenabled technologies that facilitate privacy-related data exchange is advantageous such as in data portability.Machine-readable policy languages have been on the scene for some decades. Policy languages allow us to repre-sent the will of an individual or organization to grant access to a certain resource, and they govern the operation ofactual systems over actual data. They seem perfectly aligned with Alan Westin’s vision and indeed several privacy-related policy languages have been defined and used in real scenarios. On the other hand, computers can also help*Corresponding author. E-mail: beatriz.gesteves@upm.es.1570-0844 © 2024 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:beatriz.gesteves@upm.esmailto:beatriz.gesteves@upm.eshttps://creativecommons.org/licenses/by/4.0/710 B. Esteves and V. Rodríguez-Doncel / Analysis of ontologies and policy languages to represent information flows in GDPRin other privacy and data protection tasks different from enforcing access to personal data, and policy languages arenot enough to cover every representational need. Thus, in the last few years, vocabularies and computer ontologieshave appeared to formalize concepts and rules in the domain that can be used either to simply represent informationas RDF, or to govern ontology-based information systems. Not all of them, however, had the GDPR specifically astheir framework of reference.This paper surveys existing policy languages, vocabularies and ontologies in the domain of privacy and dataprotection, and it analyses their adequacy to support GDPR-related applications. These GDPR-related applicationsmay either support individuals to manage their personal information or to support data controllers, data processorsand other stakeholders to better manage compliance with the GDPR. This joint analysis of needs (individual-orientedand company-oriented) is based on the claim that these tools may converge in a near future, and that having commonvocabulary elements and common data models to refer to GDPR rights and obligations and to denote specific GDPRconcepts would permit heterogeneous applications to speak in the same terms and interoperate. Taking into accountthis rationale, we focus on the above motivations to address the following research question: Are the existing policylanguages and vocabularies suitable to meet the representational needs brought on by the newly applicableGDPR’s rights and obligations?.Moreover, the main contributions of this paper are:(i) a study of GDPR in terms of flows of information in different deontic modalities, systematized in Fig. 1, andfurther specified in Table 1 where the informational elements necessary for the management of each GDPRright and obligation are specified;(ii) a survey of 22 existing vocabularies, ontologies and policy languages and their analysis in relation to that",
        "file_name": "10!3233%sw-223009.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-223009.pdf"
    },
    {
        "title": "Technologies and Applications for Big Data Value",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_1",
        "arxiv": null,
        "abstract": "Abstract The continuous and significant growth of data, together with improvedaccess to data and the availability of powerful computing infrastructure, has ledto intensified activities around Big Data Value (BDV) and data-driven ArtificialIntelligence (AI). Powerful data techniques and tools allow collecting, storing,analysing, processing and visualising vast amounts of data, enabling data-drivendisruptive innovation within our work, business, life, industry and society.The adoption of big data technology within industrial sectors facilitates organisa-tions to gain a competitive advantage. Driving adoption is a two-sided coin. On oneside, organisations need to master the technology necessary to extract value frombig data. On the other side, they need to use the insights extracted to drive theirdigital transformation with new applications and processes that deliver real value.This book has been structured to help you understand both sides of this coin andbring together technologies and applications for Big Data Value.This chapter defines the notion of big data value, introduces the Big Data ValuePublic-Private Partnership (PPP) and gives some background on the Big Data ValueAssociation (BDVA)—the private side of the PPP. It then moves on to structure theE. Curry (�)Insight SFI Research Centre for Data Analytics, NUI Galway, Irelande-mail: edward.curry@nuigalway.ieS. AuerLeibniz Universität Hannover, Hanover, GermanyA. J. BerreSINTEF Digital, Oslo, NorwayA. MetzgerPaluno, University of Duisburg-Essen, Essen, GermanyM. S. PerezUniversidad Politécnica de Madrid, Boadilla del Monte, Madrid, SpainS. ZillnerSiemens AG, München, Germany© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_1&domain=pdfmailto:edward.curry@nuigalway.iehttps://doi.org/10.1007/978-3-030-78307-5_12 E. Curry et al.contributions of the book in terms of three key lenses: the BDV Reference Model,the Big Data and AI Pipeline, and the AI, Data and Robotics Framework.Keywords Data ecosystem · Big data value · Data-driven innovation · Big Data1 IntroductionThe continuous and significant growth of data, together with improved access todata and the availability of powerful computing infrastructure, has led to intensifiedactivities around Big Data Value (BDV) and data-driven Artificial Intelligence (AI).Powerful data techniques and tools allow collecting, storing, analysing, processingand visualising vast amounts of data, enabling data-driven disruptive innovationwithin our work, business, life, industry and society. The rapidly increasing volumesof diverse data from distributed sources create significant technical challenges forextracting valuable knowledge. Many fundamental, technological and deploymentchallenges exist in the development and application of big data and data-driven AI",
        "file_name": "978-3-030-78307-5_1.pdf",
        "file_path": "PDFs\\978-3-030-78307-5_1.pdf"
    },
    {
        "title": "Using a Legal Knowledge Graph for Multilingual Compliance Services in Labor Law, Contract Management, and Geothermal Energy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-78307-5_12",
        "arxiv": null,
        "abstract": "Abstract This chapter provides insights about the work done and the resultsachieved by the Horizon 2020-funded Innovation Action “Lynx—Building theLegal Knowledge Graph for Smart Compliance Services in Multilingual Europe.”The main objective of Lynx is to create an ecosystem of multilingual, smart cloudservices to manage compliance based on a Legal Knowledge Graph (LKG), whichintegrates and links heterogeneous compliance data sources including legislation,case law, regulations, standards, and private contracts. The chapter provides a shortintroduction in regards to the market needs, gives an overview of the Lynx servicesavailable on the Lynx Services Platform (LySP), and provides valuable insightsinto three real-world compliance solutions developed on top of LySP togetherwith Lynx’s industry partners, namely, (1) Labor Law (Cuatrecasas, Spain), (2)Contract Management (Cybly, Austria), and (3) Geothermal Energy (DNV.GL, theNetherlands).Keywords Compliance · Legal knowledge graph · Multilingualism · NLP ·Labor law · Contract management · Geothermal energyM. Kaltenboeck (�)Semantic Web Company, Vienna, Austriae-mail: martin.kaltenboeck@semantic-web.comP. BoilCuatrecasas, Barcelona, SpainP. VerhoevenDNV, Utrecht, The NetherlandsC. SagederCybly, Salzburg, AustriaE. Montiel-Ponsoda · P. Calleja-IbáñezUniversidad Politécnica de Madrid, Madrid, Spain© The Author(s) 2022E. Curry et al. (eds.), Technologies and Applications for Big Data Value,https://doi.org/10.1007/978-3-030-78307-5_12253http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-78307-5_12&domain=pdfmailto:martin.kaltenboeck@semantic-web.comhttps://doi.org/10.1007/978-3-030-78307-5_12254 M. Kaltenboeck et al.Insights into the industry solutions include the concrete business cases, the problemstatements and requirements, the relevant data identified and used, and the LySPAI services combined to realize powerful multilingual compliance solutions inthe respective fields. The chapter closes with findings and learnings from theimplementation phase and a future outlook for further developments, specificallyfor the three vertical solutions and LySP.The chapter relates to the technical priorities of Data Management and DataAnalytics of the European Big Data Value Strategic Research and InnovationAgenda [1]. It addresses all challenges of the horizontal concern Data Managementand some of the challenges of the horizontal concern Data Analytics of the BDVTechnical Reference Model. It addresses the vertical concerns: (a) Big Data Typesand Semantics (with a focus on Text data, including Natural Language Processingdata and Graph data, Network/Web data and Metadata) as well as (b) Standards(standardization of Big Data technology areas to facilitate data integration, sharing,and interoperability). The chapter relates to the Reasoning and Decision Makingcross-sectorial technology enablers of the AI, Data and Robotics Strategic Research,",
        "file_name": "978-3-030-78307-5_12.pdf",
        "file_path": "PDFs\\978-3-030-78307-5_12.pdf"
    },
    {
        "title": "Description of Postdata Poetry Ontology V1.0",
        "implementation_urls": [
            {
                "identifier": "https://github.com/bncolorado/CorpusSonetosSigloDeOro",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\02_diezplatas.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "url: https://www.w3.org/OWL/(visited on 03/08/2021)."
                    }
                ]
            }
        ],
        "doi": "10.51305/icl.cz.9788076580336.02",
        "arxiv": null,
        "abstract": "AbstractOne stream of work in the digital humanities focuses on interoperability processesand the description of traditional concepts using computer-readable languages. Inthe case of literary studies, there has been some research into these topics, but thecomplexity of the knowledge domain remains an issue. This complexity is based onthe different interpretations of concepts in different traditions, the use of isolated andprivate databases, unique applications of language and, thus, the richness of poeticinformation. All of this suggests the need to explore new options to represent thecomplexity in computer-readable language. This paper presents an ontology networkdesigned to capture poetry domain knowledge. The ontologies in question relate topoetic works and their structural and prosodic components.1 IntroductionThe Poetry Standardization and Linked Open Data Project, POSTDATA, aimsto provide a means for European poetry researchers to publish and accesssemantically enriched data. To achieve this goal, it was necessary to developa poetry ontology. This ontology attempts to enhance interoperability in theEuropean poetry research community and capture the concepts and propertiesthat define the domain of European poetry knowledge. The development of theontology began with an attempt to define a domain model of poetry based on ananalysis of 23 poetry repertories (i.e. poetry research databases) (Curado Malta,González-Blanco, et al. 2016; Postdata ERC project 2021). These repertorieswere selected because of their relevance, availability in digital format (i.e. allare implemented in databases) and the rich sample they provided of multi-lingual poetry. They ranged from the classical period (e.g. Pedecerto1) to themodern era (e.g. Corpus of Spanish Golden-Age Sonnets2) to the Middle Ages(e.g. Cantigas de Santa Maria for singers.3) This first step allowed us to identifythe most significant identities and properties that define a poetic work (i.e. apoem), taking into account the different traits related to this literary genre. Theresult was a domain model that reflected poetry’s complexity and heterogene-ity. We then transformed this domain model into an ontology network, whichwould allow for its effective and extensive use in computational frameworksas a Post-data computer-aided annotation tool. In this paper, we present thefirst version of our network’s four most significant ontologies (i.e. version 1.0).4These ontologies relate to poetic works, their structural and prosodic compo-nents and information about relevant dates. To populate the ontologies, weincorporated the ontology definitions into OMEKA, a framework that facilitatesthe use of these ontologies in research tasks. This article is structured as follows:In Section 2, we present some previous results related to ontologies of literature,especially of the poetry domain. Section 3 describes the methodology that weused to develop our ontologies. Section 4 presents a detailed description ofthe most relevant ontologies that we created. Finally, Section 5 outlines ourconclusions and directions for future work.1 http://www.pedecerto.eu2 https://github.com/bncolorado/CorpusSonetosSigloDeOro3 http://www.cantigasdesantamaria.com/4 http://postdata.linhd.uned.es/results/network-of-ontologies/http://www.pedecerto.euhttps://github.com/bncolorado/CorpusSonetosSigloDeOrohttp://www.cantigasdesantamaria.com/http://postdata.linhd.uned.es/results/network-of-ontologies/",
        "file_name": "02_diezplatas.pdf",
        "file_path": "PDFs\\02_diezplatas.pdf"
    },
    {
        "title": "Bringing Federated Semantic Queries to the GIS-Based Scenario",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1643204664.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A detailed explanation of connecting our approach and QGIS is available on https://github.com/Osw1997/Guide-connection-for-Apache-marmotta-and-QGIS (accessed on 23 November 2021)."
                    }
                ]
            }
        ],
        "doi": "10.3390/ijgi11020086",
        "arxiv": null,
        "abstract": "Abstract: Geospatial data is increasingly being made available on the Web as knowledge graphsusing Linked Data principles. This entails adopting the best practices for publishing, retrieving, andusing data, providing relevant initiatives that play a prominent role in the Web of Data. Despitethe appropriate progress related to the amount of geospatial data available, knowledge graphsstill face significant limitations in the GIScience community since their use, consumption, andexploitation are scarce, especially considering that just a few developments retrieve and consumegeospatial knowledge graphs from within GIS. To overcome these limitations and address somecritical challenges of GIScience, standards and specific best practices for publishing, retrieving, andusing geospatial data on the Web have appeared. Nevertheless, there are few developments andexperiences that support the possibility of expressing queries across diverse knowledge graphs toretrieve and process geospatial data from different and distributed sources. In this scenario, wepresent an approach to request, retrieve, and consume (geospatial) knowledge graphs available atdiverse and distributed platforms, prototypically implemented on Apache Marmotta, supportingSPARQL 1.1 and GeoSPARQL standards. Moreover, our approach enables the consumption ofgeospatial knowledge graphs through a lightweight web application or QGIS. The potential of thiswork is shown with two examples that use GeoSPARQL-based knowledge graphs.Keywords: GeoSPARQL; SPARQL; federated query; knowledge graph; geospatial data1. IntroductionGeospatial data is increasingly being made available on the Web [1] in the form ofknowledge graphs in the Semantic Web, often using Linked Data principles [2]. To achievethese knowledge graphs, (geospatial) resources need to be identified using HTTP URIs,indexed by search engines, and connected, or linked, to other resources [3]. Therefore, thisentails adopting the best practices for publishing, retrieving, and using data on the Web [2,3].These best practices are being embraced by a growing number of data providers, leading tobuilding a global data space containing billions of assertions—the Web of Data [3].The transformation and publication of geospatial data as knowledge graphs were pioneeredby initiatives such as GeoNames (http://www.geonames.org/ontology/documentation.html(accessed on 23 November 2021)), OpenStreetMap [4], and Ordnance Survey [5]. After theseinitiatives, many geospatial datasets have been published in the Web of Data (http://lod-cloud.net/ (accessed on 23 November 2021)). This has entailed geospatial data playing a pre-eminentrole in the Web of Data cloud, operating as central nexuses that interconnect events, people, andobjects [6] and offering an ever-growing semantic representation of the geospatial informationwealth [7].Despite the relevant progress related to the amount of data available, geospatial knowl-edge graphs still face significant limitations in the GIScience community since their use,consumption, and exploitation are really scarce in this community, especially consideringISPRS Int. J. Geo-Inf. 2022, 11, 86. https://doi.org/10.3390/ijgi11020086 https://www.mdpi.com/journal/ijgihttps://doi.org/10.3390/ijgi11020086https://doi.org/10.3390/ijgi11020086https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-5799-469Xhttp://www.geonames.org/ontology/documentation.htmlhttp://lod-cloud.nethttp://lod-cloud.nethttps://doi.org/10.3390/ijgi11020086https://www.mdpi.com/journal/ijgi",
        "file_name": "pdf?version=1643204664",
        "file_path": "PDFs\\pdfversion1643204664.pdf"
    },
    {
        "title": "Characterization of urban risks in the press applying text mining for the enrichment of open data Luis M. Vilches-Blázquez and Diana Comesaña Ocampo [Caracterización de riesgos urbanos en prensa aplicando minería de texto para el enriquecimiento de datos abiertos]",
        "implementation_urls": [],
        "doi": "10.22201/iibi.24488321xe.2022.91.58538",
        "arxiv": null,
        "abstract": "AbstractNews is freely spread and widely available to Internet users much more easily than traditional media. In the news, we can find an infinite number of hidden “minor data,” that can provide valuable information not col-lected in other sources of information. In this context, we have been interested in analyzing and characteriz-ing the urban risks contained in the Uruguayan open newspapers using text mining techniques. This pro-posal makes it possible to create a news corpus based on risk events included in open data. The corpus cov-ers 2003-2019 and is built from the digital open news-papers El Eco Digital, Montevideo Portal, and La Red 21. Various text mining techniques are applied to this corpus using the QDA-MinerLite software and the Python language (concretely, through the Scattertext library) to identify, characterize, and discover insights on these events. The corpus processing results help en-rich the existing open data on risks in Uruguay, incor-porating information on their effects, actors, and asso-ciated interventions.Keywords: Urban Risk; Text Mining; Open Digi-tal Newspapers; Open DataCARACTERIZACIÓN DE RIESGOS URBANOS EN PRENSA APLICANDO MINERÍA...87DOI: http://dx.doi.org/10.22201/iibi.24488321xe.2022.9",
        "file_name": "52254",
        "file_path": "PDFs\\52254.pdf"
    },
    {
        "title": "Enhancing Trust in Trust Services: Towards an Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [],
        "doi": "10.24251/hicss.2022.712",
        "arxiv": null,
        "abstract": "AbstractAs their name suggests, trust is of crucialimportance in “trust service”. Nevertheless, in manycases, these services suffer from a lack transparency,documentation, traceability, and inclusive multi-lateraldecision-making mechanisms. To overcome thesechallenges, in this paper we propose an integratedframework which incorporates formal argumentationand negotiation within a blockchain environmentto make the decision-making processes of fundmanagement transparent and traceable. We introducethree possible architectures and we evaluate andcompare them considering different technical, financial,and legal aspects.1. IntroductionIt is a commonplace that trust has a specialimportance in entering into contractual relations. Thereis a domain in which the importance of trust is socrucial that the whole type of service is named afterit, i.e. trust services, where the fund managers arein the position of a fiduciary acting on behalf of theprincipals. The whole service the fiduciary providesis subject to the overall duty to act in the best interestof the client. The legislator can (and does1) declarethe principal’s right to check the fiduciary’s relevantactivities in order to give some weight to this duty byits intended controlability. On one hand, though, mostprobably there is a difference between the principal’sand the fund managers’ expertise and overview giving∗∗This work has received funding from the EU H2020 researchand innovation programme under the MSCA ITN European JointDoctorate grant agreement No 814177 LAST-JD-RIoE.1For instance, the 6:315. § of the Hungarian Civil Code (Act V of2013) says: The principal and the beneficiary shall have the right tocheck the fiduciary’s activities relating to asset management.the very reason to enter in such a relationship, and onthe other hand, the lack of the decision making processesbeing documented might limit the transparency one cangain by practicing this right.While trust companies might rely on smart contractswhen engaging in their core activities in the securitiesmarket—as suggested by scholars [1, 2] and provenby the surge of Decentralized Finance (DeFi) [3]—theinvolvement of Distributed Ledger Technologies (DLTs)for the securities transactions does not address thepossible trust issues between the principal and thefiduciary: the former does not have access to the reasonwhy the transaction took place and whether it was reallyin his interest. To this regard, trust can be understoodas a relational attribute between a social actor and",
        "file_name": "10!24251%hicss!2022!712.pdf",
        "file_path": ".\\PDFs\\10!24251%hicss!2022!712.pdf"
    },
    {
        "title": "Packaging research artefacts with RO-Crate",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ResearchObject/ro-crate",
                "type": "git",
                "paper_frequency": 7,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%ds-210053.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "See https://github.com/ResearchObject/ro-crate/issues/82."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.5146227",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.3233/ds-210053",
        "arxiv": "2108.06503",
        "abstract": "Abstract. An increasing number of researchers support reproducibility by including pointers to and descriptions of datasets,software and methods in their publications. However, scientific articles may be ambiguous, incomplete and difficult to processby automated systems. In this paper we introduce RO-Crate, an open, community-driven, and lightweight approach to packagingresearch artefacts along with their metadata in a machine readable manner. RO-Crate is based on Schema.org annotations inJSON-LD, aiming to establish best practices to formally describe metadata in an accessible and practical way for their use in awide variety of situations.An RO-Crate is a structured archive of all the items that contributed to a research outcome, including their identifiers,provenance, relations and annotations. As a general purpose packaging approach for data and their metadata, RO-Crate is usedacross multiple areas, including bioinformatics, digital humanities and regulatory sciences. By applying “just enough” LinkedData standards, RO-Crate simplifies the process of making research outputs FAIR while also enhancing research reproducibility.An RO-Crate for this article1 is archived at https://doi.org/10.5281/zenodo.5146227.Keywords: Data publishing, data packaging, FAIR, Linked Data, metadata, reproducibility, research object1. IntroductionThe move towards Open Science has increased the need and demand for the publication of artefactsof the research process [104]. This is particularly apparent in domains that rely on computational ex-periments; for example, the publication of software, datasets and records of the dependencies that suchexperiments rely on [113].It is often argued that the publication of these assets, and specifically software [80], workflows [55] anddata, should follow the FAIR principles [123]; namely, that they are Findable, Accessible, Interoperableand Reusable. These principles are agnostic to the implementation strategy needed to comply with them.Hence, there has been an increasing amount of work in the development of platforms and specificationsthat aim to fulfil these goals [91].Important examples include data publication with rich metadata (e.g. Zenodo [40]), domain-specificdata deposition (e.g. PDB [16]) and following practices for reproducible research software [101] (e.g.use of containers). While these platforms are useful, experience has shown that it is important to putgreater emphasis on the interconnection of the multiple artefacts that make up the research process [71].The notion of Research Objects [12] (RO) was introduced to address this connectivity, providingsemantically rich aggregations of (potentially distributed) resources with a layer of structure over aresearch study; this is then to be delivered in a machine-readable format.A Research Object combines the ability to bundle multiple types of artefacts together, such as spread-sheets, code, examples, and figures. The RO is augmented with annotations and relationships that de-scribe the artefacts’ context (e.g. a CSV being used by a script, a figure being a result of a workflow).*Corresponding author. E-mail: soiland-reyes@manchester.ac.uk.1https://w3id.org/ro/doi/10.5281/zenodo.5146227https://orcid.org/0000-0002-4763-3943https://orcid.org/0000-0002-1112-1292https://doi.org/10.5281/zenodo.5146227mailto:soiland-reyes@manchester.ac.ukhttps://w3id.org/ro/doi/10.5281/zenodo.5146227S. Soiland-Reyes et al. / Packaging research artefacts with RO-Crate 99This notion of ROs provides a compelling vision as an approach for implementing FAIR data. How-ever, existing Research Object implementations require a large technology stack [14], are typically tai-lored to a particular platform and are also not easily usable by end-users.To address this gap, a new community came together [23] to develop RO-Crate – an approach topackage and aggregate research artefacts with their metadata and relationships. The aim of this paperis to introduce RO-Crate and assess it as a strategy for making multiple types of research artefacts FAIR.Specifically, the contributions of this paper are as follows:1. An introduction to RO-Crate, its purpose and context;2. A guide to the RO-Crate community and tooling;3. Examples of RO-Crate usage, demonstrating its value as connective tissue for different artefacts",
        "file_name": "10!3233%ds-210053.pdf",
        "file_path": ".\\PDFs\\10!3233%ds-210053.pdf"
    },
    {
        "title": "Automating the Response to GDPR’s Right of Access",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/access-right-solid",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%faia220462.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A demonstration of the developed Solid application is available at https://protect.oeg.fi.upm.es/access-right/and the public repository of the code is accessible at https://github.com/besteves4/access-right-solid for further development."
                    }
                ]
            }
        ],
        "doi": "10.3233/faia220462",
        "arxiv": null,
        "abstract": "Abstract.With the enforcement of the European Union’s General Data Protec-tion Regulation, users of Web services – the ‘data subjects’ –, which arepowered by the intensive usage of personal data, have seen their rightsbe incremented, and the same can be said about the obligations imposedon the ‘data controllers’ responsible for these services. In particular, the‘Right of Access’, which gives users the option to obtain a copy of theirpersonal data as well as relevant details such as the categories of per-sonal data being processed or the purposes and duration of said pro-cessing, is putting increasing pressure on controllers as their executionoften requires a manual response effort, and the wait time is negativelyaffecting the data subjects. In this context, the main goal of this work isthe development of an API, which builds on the previously mentionedstructured information, to assist controllers in the automation of repliesto right of access requests. The implemented API method is then usedin the implementation of a Solid application whose main goal is to assistusers in exercising their right of access to data stored in Solid Pods.Keywords. digital rights management, GDPR, right of access, Solid1. IntroductionWith the enforcement of the European Union’s General Data Protection Reg-ulation (GDPR), users of Web services have seen their rights as GDPR ‘datasubjects’ being expanded when it comes to the processing of their personal data.On the other hand, on top of other GDPR-related obligations, ‘data controllers’,the entities that effectively process the data, have seen an increase in workloadrelated to the response to data subject’s right-related requests. GDPR’s ChapterIII2 details a set of 10 data subject rights, starting with the ‘Right to be Informed’described in Articles 13 and 14 and ending with the ‘Right to object to automateddecision making’ in Article 22. Considering this, data controllers would benefitfrom having the information they need to provide to data subjects in a structuredformat to automate the response to such requests [1]. In particular, the ‘Right of1Corresponding author: beatriz.gesteves@upm.es2https://gdpr-info.eu/chapter-3/Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220462170https://gdpr-info.eu/chapter-3/Access’3 is putting more and more pressure on controllers as they not only haveto provide the purpose for which the data is being used or the types of data beingprocessed but also need to provide a copy of said data. As this task is usuallydone manually, the wait time can negatively affect data subjects.In addition, with the emergence of decentralised data storage solutions, suchas Solid4, as an alternative to the traditional centralised data silos, new challengesappear as the data subject–data controller roles are still not adequately defined inthis decentralised contexts. In this context, the creation of Application Program-ming Interface (API) services would help with the automation of right-relatedrequests as, at their core, they are a ‘request–response’ type of software interface",
        "file_name": "10!3233%faia220462.pdf",
        "file_path": ".\\PDFs\\10!3233%faia220462.pdf"
    },
    {
        "title": "WhenTheFact: Extracting Events from European Legal Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia220470",
        "arxiv": null,
        "abstract": "Abstract. This paper presents WhenTheFact, a tool that identifies relevant eventsfrom European judgments. It is able to extract the structure of the document, aswell as when the event happened and who carried it out. WhenTheFact builds thena timeline that allows the user to navigate through the annotations in the document.Keywords. event extraction, visualization, NLP, legal domain, timeline generation1. IntroductionEvents and their logical sequence are key to understanding legal decisions, being thestoryline of pivotal importance. We therefore assume that a judgment can be describedas a series of time-marked happenings (events) instead of focusing on the other entities(things), and to this aim we must be able to extract these events in an automatic fashion.Before undertaking the event extraction task itself, discourse extraction is required;since legal decisions are long and complex, where the event is detected within the docu-ment is crucial regarding its relevance. Once the relevant parts of the document are de-termined, the next step involves training a system using documents annotated manuallywith relevant events, as well as the semantic resources available. Finally, the system isable to annotate different documents, allowing to visualize the relevant events in it. Ad-ditionally, in the online demo2, a timeline with these relevant events is generated, easingnavigation through the document.The paper is organized as follows. Section 2 explores previous related work in lit-erature. Section 3 introduces the system created to extract relevant events from legal de-cisions, explaining its different stages: document structure extraction, training strategiesand extraction itself. Section 4 presents the evaluation of the system. Finally, Section 5summarizes the main contributions and the future research lines to explore.2. Related workBeside generic efforts in event extraction such as the carried out by temporal taggersfollowing TimeML [1,2] or related tasks such as frame-semantic parsing [3,4], semantic1Corresponding Author: Marı́a Navas-Loro, Ontology Engineering Group, Universidad Politécnica deMadrid, Spain; E-mail: mnavas@fi.upm.es. This work was funded partially by the project Knowledge Spa-ces:  Tecnicas y herramientas  para la gestion de grafos de conocimientos para dar soporte a  espacios de datos  (Grant  PID2020-118274RB-I00,  funded  by  MCIN/AEI/10.13039/501100011033)  and  by  H2020 MSCA PROTECT (813497).2https://whenthefact.oeg.fi.upm.es/´´Legal Knowledge and Information SystemsE. Francesconi et al. (Eds.)© 2022 The authors and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA220470219https://whenthefact.oeg.fi.upm.es/role labeling (SRL) [5,6] or open information extraction3, some proposals have beenmade specifically in the legal domain. These works often involve ad hoc definitions ofevents, ignoring general event annotation schemes.In the context of legal information retrieval, events can be considered as temporallybounded objects that have entities important as participants that played a significant rolein a case. To this aim, Lagos et al. [7] propose an NLP semi automatic approach toenable the use of entity related information corresponding to the relations among the keyplayers of a case, extracted in the form of events. They are interested in the topic, theroles, the location and the time, and consider different types of events. On the other hand,",
        "file_name": "10!3233%faia220470.pdf",
        "file_path": ".\\PDFs\\10!3233%faia220470.pdf"
    },
    {
        "title": "The impact of NFT profile pictures within social network communities",
        "implementation_urls": [],
        "doi": "10.1145/3524458.3547230",
        "arxiv": "2206.06443",
        "abstract": "components of this work owned by others than ACM must be honored. Abstractingwith credit is permitted. To copy otherwise, to republish, to post on servers, or toredistribute  to  lists,  requires  prior  specific  permission  and/or  a  fee.  Requestpermissions from permissions@acm.org or PublicationsDept., ACM, Inc., fax +1 (212)869-0481.https://www.acm.org/publications/policies/copyright-policyThis item was downloaded from IRIS Università di Bologna (https://cris.unibo.it/)When citing, please refer to the published version.https://doi.org/10.1145/3524458.3547230https://cris.unibo.it/https://www.acm.org/publications/policies/copyright-policyThe impact of NFT profile pictures within social network communitiesSIMONE CASALE-BRUNET, École Polytechnique Fédérale de Lausanne, SwitzerlandMIRKO ZICHICHI, Universidad Politécnica de Madrid, SpainLEE HUTCHINSON,WhaleAnalytica.com, SwitzerlandMARCO MATTAVELLI, École Polytechnique Fédérale de Lausanne, SwitzerlandSTEFANO FERRETTI, University of Urbino “Carlo Bo”, ItalyThis paper presents an analysis of the role of social media, specifically Twitter, in the context of non-fungible tokens, better known asNFTs. Such emerging technology framing the creation and exchange of digital object, started years ago with early projects such as\"CryptoPunks\" and since early 2021, has received an increasing interest by a community of people creating, buying, selling NFTsand by the media reporting to the general public. In this work it is shown how the landscape of one class of projects, specificallythose used as social media profile pictures, has become mainstream with leading projects such as \"Bored Ape Yacht Club\", \"CoolCats\" and \"Doodles\". This work illustrates how heterogeneous data was collected from the Ethereum blockchain and Twitter and thenanalysed using algorithms and state-of-art metrics related to graphs. The initial results show that from a social network perspective,the collections of most popular NFTs can be considered as a single community around NFTs. Thus, while each project has its ownvalue and volume of exchange, on a social level all of them are primarily influenced by the evolution of values and trades of \"BoredApe Yacht Club\" collection.CCS Concepts: • Applied computing → Sociology.Additional Key Words and Phrases: NFT, PFP, Twitter, blockchain, Ethereum, profile picture, CryptoPunks, Bored Ape Yacht ClubACM Reference Format:Simone Casale-Brunet, Mirko Zichichi, Lee Hutchinson, Marco Mattavelli, and Stefano Ferretti. 2022. The impact of NFT profilepictures within social network communities . In Proceedings of GoodIT ’22: ACM International Conference on Information Technology forSocial Good, September 07–09, 2022, Cyprus. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3524458.35472301 INTRODUCTIONNon Fungible Tokens (NFTs) are a booming technology, with a trading volume that - in just one year - has increasedfrom $150 million to over $5 billion (recorded in January 2022 by considering only the transactions that took place onthe Ethereum blockchain [2] through the main exchange platform OpenSea) [19]. Public attention towards NFTs hasexploded since the beginning of 2021, when the NFT market experienced record sales and exchange volumes, up tothe present day where individual pieces for some projects have reached prices of millions of dollars and have beenpurchased by a number of celebrities. However, little is known about the overall structure and evolution of the NFTecosystem, or how this phenomenon has also grown thanks to the communities that have formed in social networks,such as Twitter. In a simplistic way, we can define an NFT as a unique digital asset whose information (such as features,traits and ownership) is certified and managed using blockchain technology. In other words, we can think of a NFTproject as a set of football player cards. Each card of the set is different from the others because of some specific traits© 2022 ACM - Association for Computing Machinery. Personal use of this material is permitted. Permission from ACM must be obtained for all otheruses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collectiveworks, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.Manuscript submitted to ACM1HTTPS://ORCID.ORG/0000-0001-7840-1398",
        "file_name": "10!1145%3524458!3547230.pdf",
        "file_path": ".\\PDFs\\10!1145%3524458!3547230.pdf"
    },
    {
        "title": "Towards a Taxonomy of AI Risks in the Health Domain",
        "implementation_urls": [],
        "doi": "10.1109/transai54797.2022.00007",
        "arxiv": null,
        "abstract": "Abstract—The adoption of AI in the health sector has its shareof benefits and harms to various stakeholder groups and entities.There are critical risks involved in using AI systems in the healthdomain; risks that can have severe, irreversible, and life-changingimpacts on people’s lives. With the development of innovativeAI-based applications in the medical and healthcare sectors, newtypes of risks emerge. To benefit from novel AI applications inthis domain, the risks need to be managed in order to protectthe fundamental interests and rights of those affected. Thiswill increase the level to which these systems become ethicallyacceptable, legally permissible, and socially sustainable. In thispaper, we first discuss the necessity of AI risk management in thehealth domain from the ethical, legal, and societal perspectives.We then present a taxonomy of risks associated with the use ofAI systems in the health domain called HART, accessible onlineat https://w3id.org/hart. HART mirrors the risks of avariety of different real-world incidents caused by use of AIin the health sector. Lastly, we discuss the implications of thetaxonomy for different stakeholder groups and further research.Index Terms—risk, AI systems, health, AI regulation, ethics ofAI, AI public policy, taxonomyI. INTRODUCTIONApplication of AI in the health domain has great potentialfor promoting public health, improving patient care, reducingtreatment costs, assisting medics in reaching a diagnosis, anddiscovering new treatment methods and drugs. However, thereare significant risks involved in the use of AI systems such asrisk of errors which can lead to injury to patients or risk ofdisclosing patients’ sensitive data [1]. With the huge amount ofAI investment in the medical and healthcare sectors for drugs,cancer, molecular, and drug discovery [2], the uncertaintiesaround the newly developed AI systems in these sectors areincreased.This project is the result of interdisciplinary research within the PROTECT(Protecting Personal Data Amidst Big Data Innovation) project and hasreceived funding from the European Union’s Horizon 2020 research andinnovation programme under the Marie Skłodowska-Curie grant agreementNo 813497.This circumstance has led to a lively discussion within thefields of ethics, social sciences, and legal scholarship making itall the more necessary for conceptualising and identifying theexact risks to different stakeholder groups and to use this in-sight for the different contexts of risk management and impactassessment. Significantly the need for risk-based assessmentlies at the heart of four relevant legislative instruments whichapply to AI in the health sector within the European Union.However, in the current state of debate in the ethical, legal,and social science literature, it is often not clear how theaddressed risks are conceptualised. By establishing a clearunderstanding of AI risk, we aim to contribute to a better",
        "file_name": "10!1109%transai54797!2022!00007.pdf",
        "file_path": ".\\PDFs\\10!1109%transai54797!2022!00007.pdf"
    },
    {
        "title": "Fostering trust with transparency in the data economy era",
        "implementation_urls": [
            {
                "identifier": "https://github.com/besteves4/ppop",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3565011!3569061.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The templates are available at https://github.com/besteves4/ppop/tree/main/templates."
                    }
                ]
            }
        ],
        "doi": "10.1145/3565011.3569061",
        "arxiv": null,
        "abstract": "ABSTRACTWhy is it hard for online users to trust service providers when itcomes to their personal data? While users might give away theirdata when using their services, this does not mean that they nec-essarily trust these companies. Building trust in online services isparticularly relevant as digital economy policy strategies, such asthe EU Data Strategy, deposit a considerable amount of faith in thebenefits of a data-driven society. To achieve this goal, transparencyshould be considered a necessary feature, on which trust can bebuilt. According to scholarly literature, the more information pro-vided to data subjects, the less power asymmetry, caused by a lackof knowledge, between them and data controllers will exist. In thisrespect, transparency around data processing has been, and stillis, conveyed through privacy notices. But these are far from beingused as helpful tools to navigate complex data-intensive environ-ments. Technical developments, such as Solid personal datastores,provide a fertile ground for the negotiation of privacy terms be-tween the involved parties. But to do so, it is necessary to haveclear and transparent processing conditions. However, while cer-tain specifications have been developed to accommodate for therepresentation of privacy terms, there is still a lack of developedsolutions to address this problem. With this in mind, we proposethe usage of the Privacy Paradigm ODRL Profile (PPOP), whichextends ODRL and DPV to specify data processing requirementsfor personal datastores envisaged as key core elements of the dataeconomy. To demonstrate the usage of PPOP, a set of policy exam-ples will be provided, as well as a prototype implementation of agenerator of machine and human-readable PPOP policies.CCS CONCEPTS• Information systems → World Wide Web; Ontologies; • Secu-rity and privacy→Human and societal aspects of securityand privacy; Access control; Social aspects of security and privacy;Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.DE ’22, December 9, 2022, Roma, Italy© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9923-4/22/12. . . $15.00https://doi.org/10.1145/3565011.3569061Privacy protections; • Applied computing→ Law; • Social andprofessional topics → Centralization / decentralization; Cen-tralization / decentralization; Privacy policies.KEYWORDStrust, transparency, data economy, data protection, ethics, knowl-edge engineering, personal information management systemsACM Reference Format:",
        "file_name": "10!1145%3565011!3569061.pdf",
        "file_path": ".\\PDFs\\10!1145%3565011!3569061.pdf"
    },
    {
        "title": "Blockchain-Based Data Management for Smart Transportation",
        "implementation_urls": [],
        "doi": "10.1007/978-3-031-07535-3_20",
        "arxiv": null,
        "abstract": "Abstract Smart services for Intelligent Transportation Systems (ITS) are currentlydeployed over centralized system solutions. Conversely, the use of decentralizedsystems to support these applications enables the distribution of data, only to thoseentities that have the authorization to access them, while at the same time guaran-teeing data sovereignty to the data creators. This approach not only allows sharinginformation without the intervention of a “trusted” data silo, but promotes data veri-fiability and accountability. We discuss a possible framework based on decentralizedsystems, with a focus on four requirements, namely data integrity, confidentiality,access control and persistence. We also describe a prototype implementation andrelated performance results, showing the viability of the chosen approach.1 IntroductionIn the last decade, Intelligent Transportation Systems (ITS) have emerged as a wayto efficiently improve mobility, travel security and increase the options for travellers.As defined in the European Union directive 2010/40/EU [12], ITS are advancedapplications for the provision of innovative transport and traffic management ser-vices, with the ultimate purpose of aiding individuals within the infrastructure tomake safe and timely decisions. The general idea is usually that of devising a sortMirko ZichichiOntology Engineering Group, Universidad Politécnica de Madrid, Spaine-mail: mirko.zichichi@upm.esStefano FerrettiDepartment of Pure and Applied Sciences, University of Urbino “Carlo Bo\", Italye-mail: stefano.ferretti@uniurb.itGabriele D’AngeloDepartment of Computer Science and Engineering, University of Bologna, Italye-mail: g.dangelo@unibo.it12 Zichichi et al.of data management middleware to build advanced applications for the provisionof innovative transport and traffic management services, with the aim of enablingusers “to be better informed and make safer, more coordinated and ‘smarter’ useof transport networks” [12]. Vehicles and transportation infrastructures are becom-ing increasingly “smarter”, which means that they are equipped with sensors thattrack and process a huge amount of different types of information, e.g. data sensedby the interior of the vehicle, the surrounding environment, road conditions, etc.This enables the creation of applications “without embodying intelligence as such”,which brings out the real essence of an infrastructure of this kind. The interactionprocesses between two individuals, or an individual and a vehicle, or an individualand the infrastructure, within the ITS, should include the least possible presence ofa human intermediary. All of this constitute a network of user-owned and infrastruc-ture devices that is usually referred as VANET (Vehicular Ad-hoc NETwork) [31].In this vision, the intelligence shifts from that of a human third-party to that of anartificial intelligence that has been optimized for this use case. This artificial inter-vention leads to the creation of “innovative services relating to different modes oftransport and traffic management” [12], that take advantage of faster processing andbetter performances. When there are no human intermediaries, indeed, traditionalprocesses become faster to execute.In addition, the growth of smartphones and Internet-of-Things devices enablesindividuals’ ubiquitous connectivity and the ability to collect environmental and per-sonal information or crowd-sensed data [42]. Thus, users become an active part of",
        "file_name": "10!1007%978-3-031-07535-3_20.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-031-07535-3_20.pdf"
    },
    {
        "title": "Systematic Construction of Knowledge Graphs for Research-Performing Organizations",
        "implementation_urls": [
            {
                "identifier": "https://github.com/RMLio/rmlmapper-java",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1669804108.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/RMLio/rmlmapper-java (accessed on 1 October 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info13120562",
        "arxiv": null,
        "abstract": "Abstract: Research-Performing Organizations (e.g., research centers, universities) usually accumulatea wealth of data related to their researchers, the generated scientific results and research outputs, andpublicly and privately-funded projects that support their activities, etc. Even though the types of datahandled may look similar across organizations, it is common to see that each institution has developedits own data model to provide support for many of their administrative activities (project reporting,curriculum management, personnel management, etc.). This creates obstacles to the integration andlinking of knowledge across organizations, as well as difficulties when researchers move from oneinstitution to another. In this paper, we take advantage of the ontology network created by the SpanishHERCULES initiative to facilitate the construction of knowledge graphs from existing informationsystems, such as the one managed by the company Universitas XXI, which provides support tomore than 100 Spanish-speaking research-performing organizations worldwide. Our effort is notjust focused on following the modeling choices from that ontology, but also on demonstrating howthe use of standard declarative mapping rules (i.e., R2RML) guarantees a systematic and sustainableworkflow for constructing and maintaining a KG. We also present several real-world use casesin which the proposed workflow is adopted together with a set of lessons learned and generalrecommendations that may also apply to other domains. The next steps include researching in theautomation of the creation of the mapping rules, the enrichment of the KG with external sources, andits exploitation though distributed environments.Keywords: knowledge graph; research-performing organizations; declarative mapping rules1. IntroductionResearch-performing organizations such as universities and research centers collectand accumulate a large amount of data related to their activities (e.g., scientific results,project outputs, academic courses, etc.). Although there are some common informationmodels (e.g., EuroCRIS [1]), in this domain it is common practice for each of these organiza-tions to develop their own information system to support all their activities. This fact has anegative impact on integrating and exploiting knowledge across institutions and also makesit difficult for researchers to manage their data when they move from one organizationto another. The effectiveness of semantic web technologies and knowledge graphs [2] forcomplex data management tasks has already been demonstrated in several domains [3–5],by companies (e.g., Google [6], Amazon [7]) and public communities (e.g., DBpedia [8],Wikidata [9]).HERCULES is a multi-anual project, promoted by the main Spanish association ofuniversities (CRUE) [10], that aims to build a semantic layer to harmonize the knowl-edge and data of the information systems of Spanish research-performing organizations.The main objective is to address data interoperability problems across organizations interms of schemes and formats, ensuring efficient exploitation of combined knowledge.Information 2022, 13, 562. https://doi.org/10.3390/info13120562 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13120562https://doi.org/10.3390/info13120562https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/info13120562https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13120562?type=check_update&version=1Information 2022, 13, 562 2 of 14",
        "file_name": "pdf?version=1669804108",
        "file_path": "PDFs\\pdfversion1669804108.pdf"
    },
    {
        "title": "Simulation of the Internet Computer Protocol: the Next Generation Multi-Blockchain Architecture",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt55542.2022.9932122",
        "arxiv": null,
        "abstract": "Abstract—The Internet Computer Protocol is a new generationblockchain that aims to provide better security and scalabilitythan the traditional blockchain solutions. In this paper, this inno-vative distributed computing architecture is introduced, modeledand then simulated by means of an agent-based simulation. Theresult is a digital twin of the current Internet Computer, tobe exploited to drive future design and development optimiza-tions, investigate its performance, and evaluate the resilience ofthis distributed system to some security attacks. Preliminaryperformance measurements on the digital twin and simulationscalability results are collected and discussed. The study alsoconfirms that agent-based simulation is a prominent simulationstrategy to develop digital twins of complex distributed systems.Index Terms—Simulation, Performance Evaluation, InternetComputer Protocol, Blockchain, Agent-based Simulation.I. INTRODUCTIONBlockchains and more in general Distributed Ledger tech-nologies (DLTs) are successful examples of distributed sys-tems with a relevant impact on both economy and computerscience. As always happens with any technology, DLTs havesome relevant drawbacks but also some interesting advantagesthat can be used for building a new generation of distributedsystems. To name a few: decentralization, data immutability,transparency, no third parties involved in transactions are fewexamples of properties that can be useful in the design andimplementation of some specific systems aimed to provide newservices to the final users or to improve the security aspectsof supply-chains (both for physical and virtual goods, such asthe software).After a first generation of blockchains, corresponding tothe initial success of Bitcoin, many improved blockchainplatforms have been designed both to support new cryptocur-rencies and for more generic purposes (e.g., Ethereum),succeeding in significantly improving some known problems.Limited scalability, the amount of time required to validatetransactions recorded in the blockchain, decentralized man-agement of blockchain updates and its controlling organizationare all known problems of the first proposed blockchains.The research on these topics is really active and evenmore advanced blockchain solutions are currently investigated,proposed and partially deployed. For example, the InternetComputer Protocol (ICP) architecture1 is based on a networkof networks aimed at combining the resources of severalcomputers and distributing computation. This happens bymeans of a protocol that supports the reading, replication,modification, and procurement of decentralized applications.One of the goals of the ICP is to coordinate a distributedsystem that is composed of many independently-operateddata centers. The obtained system is then able to provide ageneral-purpose abstract platform that is largely transparent",
        "file_name": "10!1109%ds-rt55542!2022!9932122.pdf",
        "file_path": ".\\PDFs\\10!1109%ds-rt55542!2022!9932122.pdf"
    },
    {
        "title": "Decentralized Health Data Distribution: A DLT-based Architecture for Data Protection",
        "implementation_urls": [],
        "doi": "10.1109/blockchain55522.2022.00023",
        "arxiv": null,
        "abstract": "Abstract—The management, protection and sharing of sensitivedata such as those associated with the health domain are crucialin enabling personal care and contributing to worldwide medicaladvancements. Distributed Ledger Technologies (DLTs) allow fordata protection compliant solutions in untrusted contexts thatguarantee data immutability, protection and transparency whenneeded. This paper proposes an architecture based on DLTs,Smart Contracts and Distributed File Storage (DFS), enablinguser data sovereignty, confidentiality and secure access control.A use case on health data is presented, where we apply acombination of DLT, DFS and an access control mechanism toallow users to distribute their data. Finally, we show an ex-perimental evaluation of the overall architecture to demonstratethe feasibility of implementing practical DLT-based healthcaresolutions. The results are collected through independent tests,available opensource, that verify the system’s response time ineach of its functions and as the load increases. The results arepromising and show that the system is feasible and can scale asthe load increases.Index Terms—Distributed Ledger Technology, Smart Con-tracts, Health Data, Distributed StorageI. INTRODUCTIONDigital technologies are continuously transforming society.Personal devices are foundational to this transformation, whereindividuals are the primary sources of information generation.Storing data in inaccessible and disconnected data lakes makesthem inaccessible to the public for innovation [1]. Followingthis, the interest in data ownership arises first and foremostfrom the lack of transparency in how data is collected, storedand used by different services and companies. In this regard, alow effort has been spent on easing the data management foran individual to understand and manage the risks associatedwith exploiting his private data.Healthcare could enormously benefit from the ability toshare information, transitioning from centralized to decentral-ized system architectures. Individuals can make a substantialcontribution through personal devices to science, specificallyin personalized medicine [2], [3]. Unfortunately, there arestrong barriers represented by privacy and security for thehealth sector: sharing information without the individual’sexplicit consent constitutes a substantial violation of an in-dividual’s rights. Regulations such as the European Union’sThis work has received funding from Regione Marche with DDPF n. 1189and from the EU’s Horizon 2020 research and innovation programme underthe MSCA ITN grant agreement No 814177 LAST-JD-RIoE.General Data Protection Regulation (GDPR) [4] help promotea pro-individual view. Specifically, these regulations imposemany accountability measures on actors responsible for pro-cessing personal data and assign several rights to individuals.However, these do not always address the lack of transparency",
        "file_name": "10!1109%blockchain55522!2022!00023.pdf",
        "file_path": ".\\PDFs\\10!1109%blockchain55522!2022!00023.pdf"
    },
    {
        "title": "TermitUp: Generation and enrichment of linked terminologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Pret-a-LLOD/termitup",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-222885.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "TermitUp is available in a public GitHub repository,46 as a Python project licensed under Apache License 2.0 terms."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-222885",
        "arxiv": null,
        "abstract": "Abstract. Domain-specific terminologies play a central role in many language technology solutions. Substantial manual effortis still involved in the creation of such resources, and many of them are published in proprietary formats that cannot be easilyreused in other applications. Automatic term extraction tools help alleviate this cumbersome task. However, their results areusually in the form of plain lists of terms or as unstructured data with limited linguistic information. Initiatives such as theLinguistic Linked Open Data cloud (LLOD) foster the publication of language resources in open structured formats, specificallyRDF, and their linking to other resources on the Web of Data. In order to leverage the wealth of linguistic data in the LLODand speed up the creation of linked terminological resources, we propose TermitUp, a service that generates enriched domainspecific terminologies directly from corpora, and publishes them in open and structured formats. TermitUp is composed offive modules performing terminology extraction, terminology post-processing, terminology enrichment, term relation validationand RDF publication. As part of the pipeline implemented by this service, existing resources in the LLOD are linked with theresulting terminologies, contributing in this way to the population of the LLOD cloud. TermitUp has been used in the frameworkof European projects tackling different fields, such as the legal domain, with promising results. Different alternatives on how tomodel enriched terminologies are considered and good practices illustrated with examples are proposed.Keywords: Terminology generation, terminology enrichment, linguistic linked data, Multilingualism1. IntroductionInternational institutions have become major producers of multilingual terminology databases, understood asresources that account for the specialised words used in a particular field in multiple languages. Since its foundation,the European Union has maintained initiatives to cater for the collection, maintenance and creation of terminologies,thesauri or vocabularies, to cover their internal communication needs and to support translators. Some of the bestknown resources are available from TermCoord1 (Terminology Coordination Unit of the European Parliament), incharge of the interinstitutional terminology database IATE2 (InterActive Terminology for Europe) since 2004, or the*Corresponding author. E-mail: pmchozas@fi.upm.es.1https://termcoord.eu/2https://iate.europa.eu/1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:pmchozas@fi.upm.esmailto:kvazquez@delicias.dia.fi.upm.esmailto:pcalleja@fi.upm.esmailto:emontiel@fi.upm.esmailto:vrodriguez@fi.upm.esmailto:pmchozas@fi.upm.eshttps://termcoord.eu/https://iate.europa.eu/https://creativecommons.org/licenses/by/4.0/968 P. Martín-Chozas et al. / TermitUp: Generation and enrichment of linked terminologiesFig. 1. Motivating scenario for the development of TermitUp.EU Vocabularies site,3 maintained by the Publications Office, that is also in charge of the upkeep of the multilingualthesaurus EuroVoc.4The creation and curation of such vocabularies has not only supported translators, documentalists and legaldrafters at EU institutions, but has also become a reference for translators and language professionals outside theEU. Nowadays, curated language resources have proven to be more relevant than ever in light of natural languageprocessing (NLP) tasks that rely on sound linguistic data. For example, query expansion using WordNet,5 the well-known English lexicon [49], disambiguation based on BabelNet,6 a multilingual encyclopedic dictionary [46] andtext classification applying DBpedia,7 the semantically structured version of the Wikipedia [23], to mention but afew.Initiatives such as the Linguistic Linked Open Data cloud8 (henceforward LLOD) are focused on collecting andpublishing language resources in Semantic Web formats according to the Linked Data principles [7]. When develop-ing NLP services, one of the main challenges is finding language resources on a certain subject area with acceptablequality and ready to be reused, as revealed, for example, in previous experiments on summarisation or machine",
        "file_name": "10!3233%sw-222885.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-222885.pdf"
    },
    {
        "title": "DLT-based Data Mules for Smart Territories",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AnaNSi-research/TruDaMul",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%icccn54977!2022!9868916.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/AnaNSi-research/TruDaMul https://polygon.technology/papers/10.1049/ntw2.12037 https://github.com/luca-Serena/lunes-tdm-islands https://github.com/AnaNSi-research/TruDaMul https://github.com/AnaNSi-research/TruDaMul"
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn54977.2022.9868916",
        "arxiv": null,
        "abstract": "Abstract—Many services that are taken for granted in smartcities are not even remotely available in dislocated areas yet, dueto the lack of or too costly wide area network connectivity. Withthe aim to offer a practical and secure way to transport dataand allow for communications in such constrained scenarios, wefocus on the problem of incentivizing to data mules, i.e. devicesdedicated to enable the data transfer even in the absence of theInternet. Our solution combines the use of several distributedtechnologies for verifying the correct behavior of all the partici-pants and incentivize them. We focus on the use of state channelsto support the flow of smart-contract-based tokens as a form ofpayment, in a condition where participants communicate onlywith others in physical proximity. Furthermore, we validate theviability of the application through the simulation of peer-to-peerinteractions between the participants. In this work we achievepositive results in terms of communication latency and percentageof client nodes which are able to benefit from the system.Index Terms—Data Mules, Distributed Ledger Technologies,State Channels, Smart TerritoriesI. INTRODUCTIONNowadays, we are at a crossroad. People are to some extentbeing constrained to move to big, smart cities, due to thehigher opportunities in terms of work and offered services.Conversely, recent events, such as the COVID-19 pandemic,have shown how this trend can be reversed, with an increasingnumber of people deciding to move to the countryside andrural areas. Almost, since for sure many services that are takenfor granted in (smart) cities, are not even remotely available indislocated areas yet. For some underprivileged territories it isnot possible to implement (costly) smart cities services due tothe very different economic circumstances or due to unavail-able, unreliable or too expensive network infrastructures [1].We argue that what is needed is a set of novel opportunisticsolutions, which allows us to share and reuse data, services,computation and bandwidth. Such a solution would simplifythe development of new services and the integration of legacytechnologies into new ones. Well-known examples consist oftechnologies such as multi-homing mobile services, mobilead-hoc networks, opportunistic networks, peer-to-peer andfog computing systems [1]. In this novel “smart territory”case, however, such applications might not be supported bya wide area network connectivity, and certain networkingsolutions might result as too costly (e.g. satellite connections).This work has received funding from the EU H2020 research and innovationprogramme under the MSCA ITN European Joint Doctorate grant agreementNo 814177 LAST-JD - RIoE and from the University of Urbino through the“Bit4Food” research project.Data Mules (that is an acronym for Mobile Ubiquitous LANExtensions [2]) allow for communication and data transfereven in the absence of Internet, and they can be important tools",
        "file_name": "10!1109%icccn54977!2022!9868916.pdf",
        "file_path": ".\\PDFs\\10!1109%icccn54977!2022!9868916.pdf"
    },
    {
        "title": "Street images classification according to COVID-19 risk in Lima, Peru: a convolutional neural networks feasibility analysis",
        "implementation_urls": [
            {
                "identifier": "https://github.com/jmcastagnetto/lima-atu-covid19-paraderos",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\e063411.full.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Outcome (ie, labels: moderate and extreme COVID-19 risk) data are available online: https://sistemas.atu.gob.pe/paraderosCOVID; this information was systematised at https://github.com/jmcastagnetto/lima-atu-covid19-paraderos."
                    }
                ]
            }
        ],
        "doi": "10.1136/bmjopen-2022-063411",
        "arxiv": null,
        "abstract": "ABSTRACTObjectives During the COVID- 19 pandemic, convolutional neural networks (CNNs) have been used in clinical medicine (eg, X- rays classification). Whether CNNs could inform the epidemiology of COVID- 19 classifying street images according to COVID- 19 risk is unknown, yet it could pinpoint high- risk places and relevant features of the built environment. In a feasibility study, we trained CNNs to classify the area surrounding bus stops (Lima, Peru) into moderate or extreme COVID- 19 risk.Design CNN analysis based on images from bus stops and the surrounding area. We used transfer learning and updated the output layer of five CNNs: NASNetLarge, InceptionResNetV2, Xception, ResNet152V2 and ResNet101V2. We chose the best performing CNN, which was further tuned. We used GradCam to understand the classification process.Setting Bus stops from Lima, Peru. We used five images per bus stop.Primary and secondary outcome measures Bus stop images were classified according to COVID- 19 risk into two labels: moderate or extreme.Results NASNetLarge outperformed the other CNNs except in the recall metric for the moderate label and in the precision metric for the extreme label; the ResNet152V2 performed better in these two metrics (85% vs 76% and 63% vs 60%, respectively). The NASNetLarge was further tuned. The best recall (75%) and F1 score (65%) for the extreme label were reached with data augmentation techniques. Areas close to buildings or with people were often classified as extreme risk.Conclusions This feasibility study showed that CNNs have the potential to classify street images according to levels of COVID- 19 risk. In addition to applications in clinical medicine, CNNs and street images could advance the epidemiology of COVID- 19 at the population level.INTRODUCTIONIn COVID- 19 research, deep learning tools applied to image analysis (ie, computer vision) have informed the diagnosis and prog-nosis of patients through the classification of X- ray and computer tomography images of the chest.1–3 These tools have helped practi-tioners treating COVID- 19 patients.On the other hand, the application of computer vision to study the epidemiology of COVID- 19 has been limited. One relevant example is the use of Google Street View images to extract features of the built envi-ronment and associate these with COVID- 19 ",
        "file_name": "e063411.full.pdf",
        "file_path": "PDFs\\e063411.full.pdf"
    },
    {
        "title": "Accepted Tutorials at The Web Conference 2022",
        "implementation_urls": [],
        "doi": "10.1145/3487553.3547182",
        "arxiv": null,
        "abstract": "ABSTRACTThis paper summarizes the content of the 20 tutorials that havebeen given at The Web Conference 2022: 85% of these tutorials arelecture style, and 15% of these are hands on.CCS CONCEPTS• Computer systems organization→ Embedded systems; Re-dundancy; Robotics; • Networks→ Network reliability.KEYWORDStutorials, the web conferenceACM Reference Format:Riccardo Tommasini, Senjuti Basu Roy, Xuan Wang, Hongwei Wang, HengJi, Jiawei Han, Preslav Nakov, Giovanni Da San Martino, Firoj Alam, MarkusSchedl, Elisabeth Lex, Akash Bharadwaj, Graham Cormode, Milan Dojchi-novski, Jan Forberg, Johannes Frey, Pieter Bonte,Marco Balduini, Matteo Bel-cao, Emanuele Della Valle, Junliang Yu, Hongzhi Yin, Tong Chen, HaochenLiu, Yiqi Wang, Wenqi Fan, Xiaorui Liu, Jamell Dacon, Lingjuan Lyu, Jil-iang Tang, Aristides Gionis, Stefan Neumann, Bruno Ordozgoiti, SimonRazniewski, Hiba Arnaout, Shrestha Ghosh, Fabian Suchanek, Lingfei Wu,Yu Chen, Yunyao Li, Bang Liu, Filip Ilievski, Daniel Garijo, Hans Chalupsky,Pedro Szekely, Ilias Kanellos, Dimitris Sacharidis, Thanasis Vergoulis, Nuren-dra Choudhary, Nikhil Rao, Karthik Subbian, Srinivasan Sengamedu, Chan-dan K. Reddy, Friedhelm Victor, Bernhard Haslhofer, George Katsogiannis-Meimarakis, Georgia Koutrika, Shengmin Jin, Danai Koutra, Reza Zafarani,Yulia Tsvetkov, Vidhisha Balachandran, Sachin Kumar, Xiangyu Zhao, BoChen, Huifeng Guo, YejingWang, Ruiming Tang, Yang Zhang,WenjieWang,Peng Wu, Fuli Feng, and Xiangnan He. 2022. Accepted Tutorials at TheWeb Conference 2022. In Companion Proceedings of the Web Conference 2022(WWW ’22 Companion), April 25–29, 2022, Virtual Event, Lyon, France. ACM,New York, NY, USA, 9 pages. https://doi.org/10.1145/3487553.35471821 INTRODUCTIONThe Web Conference is pleased to host 20 tutorials for the 2022edition chaired in Lyon, France.In the attempt to foster interesting discussions as well as sup-porting the dissemination of prominent research areas, the tutorialchairs have selected high-quality contributions that cover a varietyof topics. These range from fact-checking to natural language pro-cessing, deep learning, knowledge graphs, as well as the analysisof crypto assets. In particular, following the successful approach ofprevious years, this edition will host two tutorial formats:• lecture-style (85%) tutorials will cover the state-of-the-artresearch, development, and applications in a specific webcomputing and related area, and stimulate and facilitate fu-ture work.∗Tutorial co-chairPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or a",
        "file_name": "10!1145%3487553!3547182.pdf",
        "file_path": ".\\PDFs\\10!1145%3487553!3547182.pdf"
    },
    {
        "title": "Decentralized Personal Data Marketplaces: How Participation in a DAO Can Support the Production of Citizen-Generated Data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/iotaledger/streams",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1661238706.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/iotaledger/streams/blob/develop/specification/Streams_Specification_1_0A.pdf (accessed on 24 May 2022)."
                    }
                ]
            }
        ],
        "doi": "10.3390/s22166260",
        "arxiv": null,
        "abstract": "Abstract: Big Tech companies operating in a data-driven economy offer services that rely on theirusers’ personal data and usually store this personal information in “data silos” that prevent trans-parency about their use and opportunities for data sharing for public interest. In this paper, we presenta solution that promotes the development of decentralized personal data marketplaces, exploiting theuse of Distributed Ledger Technologies (DLTs), Decentralized File Storages (DFS) and smart contractsfor storing personal data and managing access control in a decentralized way. Moreover, we focus onthe issue of a lack of efficient decentralized mechanisms in DLTs and DFSs for querying a certain typeof data. For this reason, we propose the use of a hypercube-structured Distributed Hash Table (DHT)on top of DLTs, organized for efficient processing of multiple keyword-based queries on the ledgerdata. We test our approach with the implementation of a use case regarding the creation of citizen-generated data based on direct participation and the involvement of a Decentralized AutonomousOrganization (DAO). The performance evaluation demonstrates the viability of our approach fordecentralized data searches, distributed authorization mechanisms and smart contract exploitation.Keywords: distributed ledger technology; decentralized file storage; distributed hash table; datamarketplace; keyword-based search; citizen-generated data1. IntroductionRecent scandals have shown the harm that current data collection, storage and sharingpractices can cause with regard to the misuse of personal data [1,2]. As the world isbecoming more “smart”, so-called smart environments, of which smart cities [3] stand outthe most, have in common the ability to transform data (in particular, personal data) intomeaningful information needed by the liveness of the ecosystem they generate. Basedon this transformation, indeed, they provide services that are becoming more and moretargeted towards individuals. For instance, it is commonly known that personal informationis used to recommend opportunities to individuals and to make their life easier. However,entities that control these data might not always operate with the aim of social good [4].Many Big Tech companies rely on data collected about their users, usually storing thispersonal information in corporate databases, i.e., data silos, and transacting it to thirdparties with not enough transparency for individuals.Meanwhile, among the many technologies used for general-purpose data manage-ment and storage, Distributed Ledger Technologies (DLTs) are rising up as powerful toolsfor avoiding control centralization. DLT and the realm of decentralized systems, suchas Decentralized File Storages (DFS), that are emerging as solutions able to tackle theissue of obtaining large amounts of data that are not of dubious or of false origin, whileproviding more disintermediated processes [5,6]. DLTs, in this context, provide a new wayof handling personal data, such as recording, storage and transfer. This can be carriedout in combination with cryptographic schemes to ensure data confidentiality. By theirSensors 2022, 22, 6260. https://doi.org/10.3390/s22166260 https://www.mdpi.com/journal/sensorshttps://doi.org/10.3390/s22166260https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/sensorshttps://www.mdpi.comhttps://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0003-1076-2511https://doi.org/10.3390/s22166260https://www.mdpi.com/journal/sensorshttps://www.mdpi.com/article/10.3390/s22166260?type=check_update&version=2Sensors 2022, 22, 6260 2 of 32",
        "file_name": "pdf?version=1661238706",
        "file_path": "PDFs\\pdfversion1661238706.pdf"
    },
    {
        "title": "Data governance through a multi-DLT architecture in view of the GDPR",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\s10586-022-03691-3.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Accessed 3 July 2022 5."
                    }
                ]
            }
        ],
        "doi": "10.1007/s10586-022-03691-3",
        "arxiv": null,
        "abstract": "AbstractThe centralization of control over the processing of personal data threatens the privacy of individuals due to the lack oftransparency and the obstruction of easy access to their data. Individuals need the tools to effectively exercise their rights,enshrined in regulations such as the European Union General Data Protection Regulation (GDPR). Having direct controlover the flow of their personal data would not only favor their privacy but also a ‘‘data altruism’’, as supported by the newEuropean proposal for a Data Governance Act. In this work, we propose a multi-layered architecture for the managementof personal information based on the use of distributed ledger technologies (DLTs). After an in-depth analysis of thetensions between the GDPR and DLTs, we propose the following components: (1) a personal data storage based on a(possibly decentralized) file storage (DFS) to guarantee data sovereignty to individuals, confidentiality and data portability;(2) a DLT-based authorization system to control access to data through two distributed mechanisms, i.e. secret sharing (SS)and threshold proxy re-encryption (TPRE); (3) an audit system based on a second DLT. Furthermore, we provide aprototype implementation built upon an Ethereum private blockchain, InterPlanetary File System (IPFS) and Sia and weevaluate its performance in terms of response time.Keywords Distributed Ledger Technology � GDPR � Smart Contracts � Personal Data � Decentralized File Storage �Data Governance1 IntroductionThe control, direct or indirect, that individuals currentlyexercise over their personal data is conditioned by thecentralized platform-based personal information manage-ment techniques, which are then concentrated in a fewinternet service providers (ISPs) for the purpose ofexploring, filtering and obtaining data of interest [1]. Thelack of control by individuals over access to their data is ofgrowing concern and, as a result, several regulations havebeen enacted with the aim of addressing this need. TheGeneral Data Protection Regulation (GDPR) [2] is a prin-cipal example, designed for European citizens to helppromote a view in favor of the interests of individuals,instead of large corporations. It has been followed by otherregulations around the world, such as the California Con-sumer Privacy Act [3] in the USA. The GDPR conveysdata control by imposing a number of accountabilitymeasures on the responsible actors and by assigning a setof rights to individuals, i.e. as ‘‘natural persons should havecontrol of their own personal data’’ (Recital 7). Dedicatedtechnologies can help either companies to comply withGDPR (and similar) and individuals to exercise their rights,with particular regard to address two main issues: the lackof transparency in the management of personal informationand the inability to access and make interoperable personaldata.& Stefano Ferrettistefano.ferretti@uniurb.itMirko Zichichimirko.zichichi@upm.esGabriele D’Angelog.dangelo@unibo.itVı́ctor Rodrı́guez-Doncelvrodriguez@fi.upm.es1 Ontology Engineering Group, Universidad Politécnica de",
        "file_name": "s10586-022-03691-3.pdf",
        "file_path": "PDFs\\s10586-022-03691-3.pdf"
    },
    {
        "title": "Handling qualitative preferences in SPARQL over virtual ontology-based data access",
        "implementation_urls": [
            {
                "identifier": "https://github.com/antidot/db2triples",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-212895.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Qualitative preference, Skyline, OBDA, query translation, R2RML 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-212895",
        "arxiv": null,
        "abstract": "Abstract. With the increase of data volume in heterogeneous datasets that are being published following Open Data initiatives,new operators are necessary to help users to find the subset of data that best satisfies their preference criteria. Quantitativeapproaches such as top-k queries may not be the most appropriate approaches as they require the user to assign weights thatmay not be known beforehand to a scoring function. Unlike the quantitative approach, under the qualitative approach, whichincludes the well-known skyline, preference criteria are more intuitive in certain cases and can be expressed more naturally.In this paper, we address the problem of evaluating SPARQL qualitative preference queries over an Ontology-Based Data Ac-cess (OBDA) approach, which provides uniform access over multiple and heterogeneous data sources. Our main contribution isMorph-Skyline++, a framework for processing SPARQL qualitative preferences by directly querying relational databases. Ourframework implements a technique that translates SPARQL qualitative preference queries directly into queries that can be evalu-ated by a relational database management system. We evaluate our approach over different scenarios, reporting the effects of datadistribution, data size, and query complexity on the performance of our proposed technique in comparison with state-of-the-arttechniques. Obtained results suggest that the execution time can be reduced by up to two orders of magnitude in comparison tocurrent techniques scaling up to larger datasets while identifying precisely the result set.Keywords: Qualitative preference, Skyline, OBDA, query translation, R2RML1. IntroductionEliciting and exploiting preferences in query evaluation over relational databases and triple stores has attractedsustained interest in the last two decades [1,11–13,19,25,26,30,41,44,47]. Such interest is motivated by the need ofusers who are not database experts but are willing to explore large datasets, commonly coming from the integrationof multiple and heterogeneous data sources [25,41,44,47]. Usually, these users do not know, a priori, what usefulinformation they can extract from this data or they do not have a particular result in mind until it is discovered as anoutcome of their data exploration process. During data exploration, they try to identify useful information accordingto their preferences by means of eliciting queries that best meet their criteria. Typical examples include travelers*Corresponding author. E-mail: mgoncalves@usb.ve.1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:mgoncalves@usb.vemailto:marlene.gdasilva@upm.esmailto:dchaves@fi.upm.esmailto:ocorcho@fi.upm.esmailto:mgoncalves@usb.vehttps://creativecommons.org/licenses/by/4.0/660 M. Goncalves et al. / Handling qualitative preferences in SPARQL over virtual OBDAlooking for the best deals on accommodation to visit a city or users looking for laboratories with the best offers onPCR tests in a range of hours more appropriate for them during the COVID-19 global pandemic. Database enginesshould be able to filter useful information to support the requirements of these non-expert users, i.e., evaluatingqueries that represent the desirable properties over the final result during the user’s search process. This kind of userexpects such queries to be easily posed, correctly interpreted by the engine, and computationally efficient.Introducing preference criteria into queries has been tackled in the context of relational databases (RDB) [11–13,30–32]. They are often used to filter information and thus reduce the volume of data being displayed to theuser. At the level of preference criteria, two different approaches can be pursued: qualitative and quantitative. In thequantitative approach [1,26], preference criteria are specified by means of scoring functions that assign a numericalscore to each instance of the query response. Thus, an instance A is preferred to an instance B if the score of A ishigher than the score of B. The problem of lack of expressiveness of this approach is well known in utility theory[22] where preferences are only represented by numerical scoring functions which are not necessarily easy to defineby any user, whereas preferences in the qualitative approach can be expressed more naturally. For example, a typicalscoring function for finding cheap hotels near the sea is a weighted average of price and distance, but the usershould know how to define weights. In addition, the user should normalize the values of price and distance becausethey have different units. In contrast, under the qualitative approach, the user may simply express his preferencesas minimizing price and distance, which is more intuitive. The qualitative approach is strictly more general thanthe quantitative one since preference queries are based on the observation that expressions, such as “I prefer more",
        "file_name": "10!3233%sw-212895.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-212895.pdf"
    },
    {
        "title": "Towards CBDC-based machine-to-machine payments in consumer IoT",
        "implementation_urls": [],
        "doi": "10.1145/3477314.3507078",
        "arxiv": null,
        "abstract": "ACM must be honored. Abstracting with credit is permitted. To copy otherwise,to republish, to post on servers, or to redistribute to lists, requires prior speci�cpermission and/or  a fee. Request permissions from permissions@acm.org orPublicationsDept., ACM, Inc., fax +1 (212) 869-0481.https://www.acm.org/publications/policies/copyright-policyThis item was downloaded from IRIS Università di Bologna (https://cris.unibo.it/)When citing, please refer to the published version.Lirias - Associatie KU LeuvenTowards CBDC-based Machine-to-Machine Payments inConsumer IoTNadia Pocher∗Universitat Autònoma de BarcelonaLaw, Science and Technology RIoE EJDnadia.pocher@uab.catMirko Zichichi∗Universidad Politécnica de MadridLaw, Science and Technology RIoE EJDmirko.zichichi@upm.esABSTRACTThe technological advancement of the Internet of Things (IoT) is awell-known phenomenon that mainly a�ects industrial sectors butalso consumers in everyday life. The use of Consumer IoT, i.e. CIoT,devices is increasing, and they are paving the way for a Machine-to-Machine (M2M) communication that could highly enrich consumerservices. In this paper we position ourselves in the narrowing gapbetween the world of CIoT and the world of money, and we explorethe emerging interaction between the payment needs of a M2MEconomy and the “newways of payment”. Indeed, the advent of Dis-tributed Ledger Technology and cryptocurrencies has introduceda tech-oriented dynamism in the monetary and �nancial sphere.Accordingly, central banks all over the world have started investi-gations into digital �at money , i.e., “retail” Central Bank DigitalCurrencies (CBDCs). Against this backdrop, we analyze the inte-gration of retail CBDC models into M2M and CIoT dynamics, whileheeding regulation-by-design and compliance-by/through-designmethodologies, and we propose a preliminary model of integrationbetween a two-tier retail CBDC architecture and CIoT.CCS CONCEPTS• Networks → Peer-to-peer networks; • Applied computing→ Law; Economics; • Human-centered computing → Ubiqui-tous and mobile devices;KEYWORDSCentral Bank Digital Currency, Machine-to-Machine, Internet ofThings, Distributed Ledger TechnologiesACM Reference Format:Nadia Pocher and Mirko Zichichi[1]. 2022. Towards CBDC-based Machine-to-Machine Payments in Consumer IoT. In The 37th ACM/SIGAPP Sympo-sium on Applied Computing (SAC ’22), April 25–29, 2022, Virtual Event, .ACM,New York, NY, USA, 8 pages. https://doi.org/10.1145/3477314.3507078∗This work has received funding from the EU H2020 research and innovation pro-",
        "file_name": "10!1145%3477314!3507078.pdf",
        "file_path": ".\\PDFs\\10!1145%3477314!3507078.pdf"
    },
    {
        "title": "Intelligent Human-input-based Blockchain Oracle (IHiBO)",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/IHiBO",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!5220%0010945300003116.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "a conflict resolution, to a smart contract de-ployed to the mainchain3."
                    }
                ]
            }
        ],
        "doi": "10.5220/0010945300003116",
        "arxiv": null,
        "abstract": "Abstract: The advent of Distributed Ledger Technologies (DLTs) has paved the way for a new paradigm of traceabilityin all information systems areas. In the context of decision-making processes, however, DLTs are generallyused only to trace the end results. In this work we argue that a reasoning system can be put in place formaking these decisions, in order to enhance auditability, transparency, and finally to provide explainability.We propose the Intelligent Human-input-based Blockchain Oracle (IHiBO), a cross-chain oracle that enablesthe execution and traceability of formal argumentation and negotiation processes, involving the intervention ofhuman experts. We take as reference the decision-making processes of fund managements, as trust is of crucialimportance in such “trust services”. The architecture and implementation of IHiBO are based on leveragingtwo-layer DLTs, smart contracts, argumentation and negotiation in a multi-agent setup. Finally, we providesome experimental results that support our discussion, namely that in the use-case we have considered ourmethodology can increase trust from principals to trusted services.1 INTRODUCTIONIn situations where trust plays a significant role, thedecision-making process might be considered as thepinnacle of the engagement between parties. In thecase of funds management, for instance, investorschoose managers based not only on forecasts of futureperformance but also on factors such as trust and reli-ability (Kostovetsky, 2016). Indeed, in these so called“trust services” the fund managers are in the positionof a fiduciary acting on behalf of the principal, sub-ject to the overall duty to act in the best interest ofthe client, i.e. the principal. Fund managers primar-ily research and determine the best stocks, bonds, orother securities to fit the strategy of the fund, then buyand sell them. The decisions taken by managers af-fect the principals directly, thus the legislator can anddoes declare the principal’s right to check the fidu-ciary’s relevant activities in order to give some weightto this duty by its intended controlability. However,this might not be so straightforward, as these activi-aThese authors contributed equally.b https://orcid.org/0000-0002-7200-6001c https://orcid.org/0000-0002-4159-4269d https://orcid.org/0000-0002-2488-2293e https://orcid.org/0000-0001-7784-6176ties, e.g. securities transactions, are increasingly exe-cuted as a collaborative process that involves not onlya single fund manager but also other managers, ana-lysts, and external entities that maintain business re-lationships. The beliefs and assumptions of this di-verse group of participants can be influenced by a va-riety of different background knowledge and in turnshape the decision that leads to the execution of afund activity. The fund management decision processis characterized by uncertain and changing informa-tion, dynamic opportunities, multiple goals and strate-gic considerations, interdependence among projects,and multiple decision-makers and locations (Cooperet al., 1997). This necessitates a collaborative pro-",
        "file_name": "10!5220%0010945300003116.pdf",
        "file_path": ".\\PDFs\\10!5220%0010945300003116.pdf"
    },
    {
        "title": "LOT: An industrial oriented ontology engineering framework",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/LOT-resources",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!engappai!2022!104755.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The Chowlk notation is provided as diagrams.net template to be imported 17https://spreadsheets.google.com 18An example can be found on the https://docs.google.com/spreadsheets/d/1_VcoGD5Qq6iKr8-XNGJGsOo475aiZsnKf0i01awdZkc/edit?usp=drive_web&ouid= 103242980179942151298 19A template is available at https://github.com/oeg-upm/LOT-resources 20https://www.atlassian.com/software/confluence 21https://www.diagrams.net/22https://www.yworks.com/products/yed 23https://chowlk.linkeddata.es/24 https://spreadsheets.google.com https://docs.google.com/spreadsheets/d/1_VcoGD5Qq6iKr8-XNGJGsOo475aiZsnKf0i01awdZkc/edit?usp=drive_web&ouid=103242980179942151298 https://docs.google.com/spreadsheets/d/1_VcoGD5Qq6iKr8-XNGJGsOo475aiZsnKf0i01awdZkc/edit?usp=drive_web&ouid=103242980179942151298 https://docs.google.com/spreadsheets/d/1_VcoGD5Qq6iKr8-XNGJGsOo475aiZsnKf0i01awdZkc/edit?usp=drive_web&ouid=103242980179942151298 https://github.com/oeg-upm/LOT-resources https://www.atlassian.com/software/confluence https://www.diagrams.net/https://www.yworks.com/products/yed https://chowlk.linkeddata.es/in diagrams.net so that the user could reuse the OWL building blocks as commonly used for UML (Unified Modeling Language) editors."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.engappai.2022.104755",
        "arxiv": null,
        "abstract": "AbstractOntology Engineering has captured much attention during the last decades leading to the prolif-eration of numerous works regarding methodologies, guidelines, tools, resources, etc. includingtopics which are still being investigated. Even though, there are still many open questions whenaddressing a new ontology development project, regarding how to manage the overall projectand articulate transitions between activities or which tasks and tools are recommended for eachstep. In this work we propose the Linked Open Terms (LOT) methodology, an overall and light-weight methodology for building ontologies based on existing methodologies and oriented tosemantic web developments and technologies. The LOT methodology focuses on the alignmentwith industrial development, in addition to academic and research projects, and software devel-opment, that is making ontology development part of the software industry. This methodologyincludes lessons learnt from more than 20 years in ontological engineering and its application on18 projects is reported.Keywords: Ontology engineering; Ontology development methodology; Ontology developmentsoftware support; Collaborative ontology development; Ontology industrial development1. IntroductionQ11: Should you always write ontology functional requirements in the form of CompetencyQuestions (CQs) (Grüninger and Fox, 1995) when you elaborate the requirements specificationfor an ontology? Q2: Which is the best way to communicate requirements to software engineers?Q3: And to domain experts? Q4: Which are the main problems that you will find when you reusean ontology? Q5: Which is the sequence of activities that has been shown to be successful inpractice in ontology development? Q6: What tools can you use for each activity in ontologydevelopment? Q7: Are there some tools that aid you to identify typical mistakes in modelling?Q8: What do you have to do to transform your Web Ontology Language (OWL) ontology intoan HTML document?Email addresses: mpoveda@fi.upm.es (María Poveda-Villalón), albafernandez@fi.upm.es (AlbaFernández-Izquierdo), mfernandez.eps@ceu.es (Mariano Fernández-López), rgarcia@fi.upm.es (RaúlGarcía-Castro)1The following questions are identified by “QX” to ease the reference to the questions from other sections of thepaper.Preprint submitted to Journal of LATEX Templates 24th January 2022Although it is true that a lot of useful work has been carried out on ontology engineering overthe years, such as the proposal of multiple ontology development methodologies to systematisethe development process and the alignment with agile practices, as of today, there are importantquestions on ontology development that have not been answered, This issue has been exposed bythe recent analysis of the current state, challenges and future directions in ontology engineeringpresented by Tudorache (Tudorache, 2020).The aim of the work presented in this paper is to respond to this situation and answer thequestions presented in the first paragraph by proposing the Linked Open Terms (LOT) methodo-logy, which not only presents the activities to be performed in the ontology development process,but also proposes recommendations, tips and tools to support them. The LOT methodology isbased on the experience of, at least, 18 projects where ontologies have been developed, both bythis paper authors and by external teams, involving both domain experts and software engineers.Our experience is also diverse in other senses, for example, there are projects where the creationof linked open data has been an important result, others where the ontology has been an object-ive itself, others where the ontology has been an standard schema for communication betweensystems, etc. In addition, one of the authors has contributed in the past to two of the most well-known methodologies for building ontologies which brings not only an extensive experience inpractical matters but also a broader view and knowledge about the evolution of the ontology en-gineering field during the last decades. The conclusions and lessons learnt from our experience",
        "file_name": "10!1016%j!engappai!2022!104755.pdf",
        "file_path": ".\\PDFs\\10!1016%j!engappai!2022!104755.pdf"
    },
    {
        "title": "Data Quality Barriers for Transparency in Public Procurement",
        "implementation_urls": [],
        "doi": "10.3390/info13020099",
        "arxiv": null,
        "abstract": "Abstract: Governments need to be accountable and transparent for their public spending decisionsin order to prevent losses through fraud and corruption as well as to build healthy and sustainableeconomies. Open data act as a major instrument in this respect by enabling public administrations,service providers, data journalists, transparency activists, and regular citizens to identify fraud oruncompetitive markets through connecting related, heterogeneous, and originally unconnected datasources. To this end, in this article, we present our experience in the case of Slovenia, where wesuccessfully applied a number of anomaly detection techniques over a set of open disparate data setsintegrated into a Knowledge Graph, including procurement, company, and spending data, through alinked data-based platform called TheyBuyForYou. We then report a set of guidelines for publishinghigh quality procurement data for better procurement analytics, since our experience has shown usthat there are significant shortcomings in the quality of data being published. This article contributesto enhanced policy making by guiding public administrations at local, regional, and national levels onhow to improve the way they publish and use procurement-related data; developing technologies andsolutions that buyers in the public and private sectors can use and adapt to become more transparent,make markets more competitive, and reduce waste and fraud; and providing a Knowledge Graph,which is a data resource that is designed to facilitate integration across multiple data silos by showinghow it adds context and domain knowledge to machine-learning-based procurement analytics.Keywords: public procurement; fraud and corruption; data integration; knowledge graph; linkedopen data; anomaly detection1. IntroductionPublic procurement is a business impacting the lives of millions. Globally governmentsspend trillions of dollars a year on public contracts for goods, services, and works (https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/ (ac-cessed on 19 February 2022)); for example, public authorities in the European Union(EU) spend around 14% of GDP every year (https://ec.europa.eu/growth/single-market/public-procurement_en (accessed on 19 February 2022)). A market of such a size hassubstantial challenges, such as delivering quality services with greatly reduced bud-gets; preventing losses through fraud and corruption; and building healthy and sus-tainable economies. Even a small percentage of cost increase can easily have a massiveInformation 2022, 13, 99. https://doi.org/10.3390/info13020099 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info13020099https://doi.org/10.3390/info13020099https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0002-2753-9917https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://www.open-contracting.org/what-is-open-contracting/global-procurement-spend/https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enhttps://doi.org/10.3390/info13020099https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info13020099?type=check_update&version=2Information 2022, 13, 99 2 of 21impact. Particularly, identifying fraud in public tendering is a major struggle; the Or-ganisation for Economic Co-operation and Development (OECD) estimates that a vastmajority of these cases remain undetected across all OECD countries [1]. The Economist",
        "file_name": "pdf?version=1645523351",
        "file_path": "PDFs\\pdfversion1645523351.pdf"
    },
    {
        "title": "TheyBuyForYou platform and knowledge graph: Expanding horizons in public procurement with open linked data",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/data-sources",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-210442.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The data collected from OpenOpps and OpenCorporates is openly available under the Open Database License (ODbl).23 It is available on GitHub24 in JSON format and is updated on a monthly basis."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210442",
        "arxiv": null,
        "abstract": "Abstract. Public procurement is a large market affecting almost every organisation and individual; therefore, governments needto ensure its efficiency, transparency, and accountability, while creating healthy, competitive, and vibrant economies. In this con-text, open data initiatives and integration of data from multiple sources across national borders could transform the procurementmarket by such as lowering the barriers of entry for smaller suppliers and encouraging healthier competition, in particular byenabling cross-border bids. Increasingly more open data is published in the public sector; however, these are created and main-tained in siloes and are not straightforward to reuse or maintain because of technical heterogeneity, lack of quality, insufficientmetadata, or missing links to related domains. To this end, we developed an open linked data platform, called TheyBuyForYou,consisting of a set of modular APIs and ontologies to publish, curate, integrate, analyse, and visualise an EU-wide, cross-border,and cross-lingual procurement knowledge graph. We developed advanced tools and services on top of the knowledge graph foranomaly detection, cross-lingual document search, and data storytelling. This article describes the TheyBuyForYou platform andknowledge graph, reports their adoption by different stakeholders and challenges and experiences we went through while creatingthem, and demonstrates the usefulness of Semantic Web and Linked Data technologies for enhancing public procurement.Keywords: Public procurement, knowledge graph, linked datas, open data, ontology*Corresponding author. E-mail: ahmet.soylu@oslomet.no.1570-0844 © 2022 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:ahmet.soylu@oslomet.nomailto:ocorcho@fi.upm.esmailto:cbadenes@fi.upm.esmailto:fyedro@fi.upm.esmailto:brian.elvesater@sintef.nomailto:till.lech@sintef.nomailto:dumitru.roman@sintef.nomailto:t.blount@soton.ac.ukmailto:matej.kovacic@ijs.simailto:matej.posinkovic@ijs.simailto:ian@spendnetwork.commailto:chris.taggart@opencorporates.commailto:elena.simperl@kcl.ac.ukmailto:ahmet.soylu@oslomet.nohttps://creativecommons.org/licenses/by/4.0/266 A. Soylu et al. / TheyBuyForYou platform and knowledge graph1. IntroductionThe market around public procurement is large enough so as to affect almost every single citizen and organisationacross a variety of sectors. For this reason, public spending has always been a matter of interest at local, regional, andnational levels, and even more so, in times of great austerity and increased public scrutiny. Primarily, governmentsneed to be efficient in delivering services, ensure transparency, prevent fraud and corruption, and build healthy andsustainable economies [4,20]. For example, in the European Union (EU), every year, over 250.000 public authoritiesspend around 2 trillion euros (about 14% of GDP) on the purchase of services, works, and supplies;1 while theOrganisation for Economic Co-operation and Development (OECD) estimates that more than 82% of fraud andcorruption cases remain undetected across all OECD countries [27] costing as high as 990 billion euros a year in theEU alone [40]. Moreover, small and medium-sized enterprises (SMEs) are often locked out of markets and restrictedby borders due to the high cost of obtaining the required information, where larger companies can absorb the cost.This leads to a tendency for governments to rely on monolithic suppliers without adequate competition to delivergood value for the taxpayers.The availability of good quality, open, and integrated procurement data, coming from multiple sources acrossnational borders, could alleviate the aforementioned challenges [16]. This includes government agencies assessingpurchasing options, companies exploring new business contracts and placing cross-border bids, and other parties(such as journalists, researchers, local communities, business associations, transparency activists, and individualcitizens) looking for a better understanding of the intricacies of the public procurement landscape through decision-",
        "file_name": "10!3233%sw-210442.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-210442.pdf"
    },
    {
        "title": "Annotating OGC web feature services automatically for generating geospatial knowledge graphs",
        "implementation_urls": [],
        "doi": "10.1111/tgis.12863",
        "arxiv": null,
        "abstract": "AbstractSciVal TopicsAbstractThe Spatial Data Infrastructure initiatives are now broadly developed and deployed. However, whileplenty of tools use them, some tasks are still complex to perform by non-expert users, such as finding,accessing, and using some of their related OGC Web Services (OWS). One of the main reasons for thesechallenges is associated with semantic heterogeneity within these services. This entails a lack of properdescriptions and requires knowledge about data structure and domain-specific query languages todiscover and retrieve these services. Semantic annotations of OWS play a crucial role in achievingsemantic interoperability and addressing these associated challenges. In this article we describe anapproach for enabling the automatic generation of semantic annotations of Web Feature Services(WFS) at their three request levels (GetCapabilities, DescribeFeatureType, and GetFeature), which areused to generate knowledge graphs. This approach uses various external services, ontological resources(POSM, DBpedia, GeoSPARQL, GeoNames, and datos.ign.es vocabularies), and knowledge bases(DBpedia and datos.ign.es). Moreover, this approach enables us to validate annotations obtained as aprevious step to generating geospatial knowledge graphs. Additionally, we present our proposalthrough an application case and assess it using a representative set of 21 WFS services, achieving anaverage of 46.70% annotations, of which 22.29% and 35.52% were validated using DBpedia andCited by 0 documentsInform me when this documentis cited in Scopus:Related documents,(2019) Transactions in GIS,  , (2011) GeoInformatica,(2022) Geo-Spatial InformationScience  Find more related documents inScopus based on:Set citation alert ▻A framework for connecting twointeroperability universes: OGCWeb Feature Services and LinkedData Vilches-Blázquez, L.M.Saavedra, J.Integrating semantic webtechnologies and geospatialcatalog services for geospatialinformation discovery andprocessing in cyberinfrastructure Yue, P. Gong, J. Di, L.A graph-based representation ofknowledge for managing landadministration data fromdistributed agencies–A case studyof Colombia",
        "file_name": "10!1111%tgis!12863.pdf",
        "file_path": ".\\PDFs\\10!1111%tgis!12863.pdf"
    },
    {
        "title": "Context aware ontology‐based hybrid intelligent framework for vehicle driver categorization",
        "implementation_urls": [],
        "doi": "10.1002/ett.3729",
        "arxiv": null,
        "abstract": "Castro Abstract— In public vehicles, one of the major concerns is driver’s level of expertise for its direct proportionality to safety of passengers. So before a driver is subjected to certain type of vehicle, he should be thoroughly evaluated and categorized with respect to certain parameters instead of only one-time metric of having driving license. These aspects may be driver’s expertise, vigilance, aptitude, experience years, cognition, driving style, formal education, terrain, region, minor violations, major accidents and age group etc. The purpose of this categorization is to ascertain suitability of a driver for certain vehicle type(s) to ensure passengers’ safety. Currently, no driver categorization technique fully comprehends the implicit as well as explicit characteristics of drivers dynamically. In this paper, machine learning based dynamic and adaptive technique named D-CHAIT (Driver Categorization based on Hybrid Artificial Intelligence Techniques) is proposed for driver categorization with an objective focus on driver’s attributes modeled in DriverOntology. A supervised mode of learning has been employed on a labeled dataset, having diverse profiles of drivers with attributes pertinent to drivers’ perspectives of demographics, behaviors, expertise and inclinations. A comparative analysis of D-CHAIT with three other machine learning techniques (Fuzzy Logic, Case Based Reasoning, and Artificial Neural Networks) is also presented. The efficacy of all techniques was empirically measured while categorizing the drivers based on their profiles through metrics of accuracy, precision, recall, f-measure performance and associated costs. These empirical quantifications assert D-CHAIT as a better technique than contemporary ones. The novelty of proposed technique is signified through preprocessing of feature attributes, quality of data, training of machine learning model on more relevant data and adaptivity. Index Terms— Artificial Neural Networks, Case Based Reasoning, Vehicle Driver Categorization, Fuzzy Logic, Machine Learning  I. INTRODUCTIONThe foremost concern of any transportation authority would be assuring the safety of passengers using their facility. Adoption of modern technologies in transportation has not  This work is submitted on 30 Nov 2018.  We hereby acknowledge the funds support under the project PrivSoft sponsored by Higher Education Commission under the grant number:112116-Eg043.  Dr Sohail Sarwar is with the Department of Computing University of Gujrat Pakistan (e-mail: sohail.sarwar@seecs.edu.pk).  Saad Zia is with National University of Computing and Emerging Sciences (NUCES)-FAST, Pakistan (emai: Saad.zia@nu.edu.pk) Dr Zia Ul Qayyum is working with Allama Iqbal Open University Islamabad, UK (email: vc@uog.edu.pk) Dr Muddessar is working with London South Bank University, UK (m.iqbal@lsbu.ac.uk, m.iqbal@essex.ac.uk) . He is also working as Visiting ",
        "file_name": "10!1002%ett!3729.pdf",
        "file_path": ".\\PDFs\\10!1002%ett!3729.pdf"
    },
    {
        "title": "A case-based reasoning model powered by deep learning for radiology report recommendation",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2021.08.011",
        "arxiv": null,
        "abstract": "AbstractCase-Based Reasoning models are one of the most used reasoning paradigms in expert-knowledge-driven areas. One of the most prominent fields of use of these systems is the medical sector, where explainable models are required. However, these models are considerably reliant on user input and the introduction of relevant curated data. Deep learning approaches offer an analogous solution, where user input is not required. This paper proposes a hybrid Case-Based Reasoning, Deep Learning framework for medical-related applications, focusing on the generation of medical reports. The proposal combines the explainability and user-focused approach of case-based reasoning models with the deep learning techniques performance. Moreover, the framework is fully modular to fit a wide variety of tasks and data, such as real-time sensor captured data, images, or text, to name a few. An implementation of the proposed framework focusing on radiology report generation assistance is provided. This implementation is used to evaluate the proposal, showing that it can provide meaningful and accurate corrections, even when the amount of information available is minimal. Additional tests on the optimization degree of the case base are also performed, evidencing how the proposed framework can optimize this base to achieve optimal performance.* Corresponding author.E-mail addresses: eamador@fi.upm.es (E. Amador-Domínguez), emilioserra@fi.upm.es (E. Serrano), daniel.manrique@upm.es (D. Manrique), jbajo@fi.upm.es (J. Bajo).DOI:  10.9781/ijimai.2021.08.011I. IntroductionDeep Learning is currently a fundamental approach in Artificial Intelligence applied to the medical domain. Their applications include image segmentation [1]–[3], 3D image reconstruction [4], [5], and disease diagnosis [6], [7]. While these approaches offer outstanding results, they suffer from a considerable flaw: lack of explainability. This issue is particularly concerning in the medical domain, where it is crucial to understand the inference procedure carried by a model to perform a task. Moreover, deep learning-based approaches require a considerable amount of labelled data to be truly accurate, which may not always be available.Opposite to this approach, the Case-Based Reasoning (CBR) methodology [8], [9] provides computational models closely related to human reasoning. In CBR, the resolution of problems provides knowledge that permits to solve new, similar ones. A CBR model discovers the closest situation to the current one to solve and adapt its solution to fit the present scenario. One of CBR’s essential advantages is that it is easy to follow and understand the inference process they conduct, which has prompted its use in, for example, the medical domain [10], [11]. This paper proposes a hybrid CBR-deep learning model to tackle the problem of radiology report writing assistance. The main efforts in the radiology domain reside within image-related tasks, such as diagnosis or X-ray image segmentation. In this image-dominated field, medical reports play a secondary role, mostly used to support the aforementioned tasks. Thus, high quality labelled textual data in this domain may not always be available, which hinders the use of deep learning techniques.The proposed approach uses a CBR model to work with a few cases that can scale up, assisted by deep learning models to improve its performance. Therefore, it is a blended solution between a knowledge-based system [12], where the knowledge must be elicited, and a deep ",
        "file_name": "ijimai.2021.08.011",
        "file_path": "PDFs\\ijimai.2021.08.011.pdf"
    },
    {
        "title": "An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science",
        "implementation_urls": [
            {
                "identifier": "https://github.com/actionprojecteu/dmptool",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1108%dta-10-2020-0253.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "A technical description can be found at https://zenodo.org/record/3885566 and the software is available with an open source license at https://github.com/actionprojecteu/dmptool and https://github.com/actionprojecteu/dmptool-generator."
                    }
                ]
            }
        ],
        "doi": "10.1108/dta-10-2020-0253",
        "arxiv": null,
        "abstract": "Abstract. Purpose: Citizen Science – public participation in scientific projects – is becoming a global practice engaging volunteer participants, often non-scientists, with scientific research. Citizen Science is facing major challenges, such as quality and consistency, to reap open the full potential of its outputs and outcomes, including data, software, and results. In this context, the principles put forth by Data Science and Open Science domains are essential for alleviating these challenges, which have been addressed at length in these domains. The purpose of this study is to explore the extent to which Citizen Science initiatives capitalise on Data Science and Open Science principles. Approach: We analysed 48 Citizen Science projects related to pollution and its effects. We compared each project against a set of Data Science and Open Science indicators, exploring how each project defines, collects, analyses and exploits data to present results and contribute to knowledge. Roman, D., Reeves, N., Gonzalez, E., Celino, I., Abd El Kader, S., Turk, P., Soylu, A., Corcho, O., Cedazo, R., Re Calegari, G., Scandolari, D. and Simperl, E. (2021), \"An analysis of pollution Citizen Science projects from the perspective of Data Science and Open Science\",Data Technologies and Applications, Vol. ahead-of-print No. ahead-of-print. https://doi.org/10.1108/DTA-10-2020-0253This author accepted manuscript is deposited under a Creative Commons Attribution Non-commercial 4.0 International (CC BY-NC) licence. This means that anyone may distribute, adapt, and build upon the work for non-commercial purposes, subject to full attribution. If you wish to use this manuscript for commercial purposes, please contact permissions@emerald.com.https://creativecommons.org/licenses/by-nc/4.0/2   Findings: The results indicate several shortcomings with respect to commonly accepted Data Science principles, including lack of a clear definition of research problems and limited description of data management and analysis processes, and Open Science principles, including lack of the necessary contextual information for reusing project outcomes.  Originality: In the light of this analysis, we provide a set of guidelines and recommendations for better adoption of Data Science and Open Science principles in Citizen Science projects, and introduce a software tool to support this adoption, with a focus on preparation of data management plans in Citizen Science projects.  Keywords: Citizen Science, Data Science, Open Science, pollution projects, Data Management Plan, software 1.  Introduction Citizen Science (CS) describes the active engagement of volunteer participants within scientific research. Projects vary greatly in terms of the role that volunteers play and the degree of agency that they have, from more passive models where volunteers install software and sensors to more collaborative models where volunteers actively define problem spaces and research topics (Haklay, 2013). Nevertheless, CS commonly entails the gathering of data by volunteers for later dissemination and publication (Haklay, 2013; Pocock et al., 2017).  CS data are of significant value not only in the projects in which they are gathered, but for subsequent analysis and re-use (Wang et al., 2015). Volunteer-contributed data may complement and offer the opportunity to contextualise and expand upon existing monitoring efforts by professional organisations, without significant added cost (Hadj-Hammou et al., 2017). In some contexts, the vast majority of available data are from CS sources (Groom et al., 2017; Poisson et al., 2020). This is equally true of software, with CS projects developing a wide variety of software ",
        "file_name": "10!1108%dta-10-2020-0253.pdf",
        "file_path": ".\\PDFs\\10!1108%dta-10-2020-0253.pdf"
    },
    {
        "title": "Impact of Text Length for Information Retrieval Tasks based on Probabilistic Topics",
        "implementation_urls": [],
        "doi": "10.26342/2021-67-2",
        "arxiv": null,
        "abstract": "Abstract: Information retrieval has traditionally been approached using vectormodels to describe texts. In large document collections, these models need to reducethe dimensions of the vectors to make the operations manageable without compro-mising their performance. Probabilistic topic models (PTM) propose smaller vectorspaces. Words are organized into topics and documents are related to each otherfrom their topic distributions. As in many other AI techniques, the texts used totrain the models have an impact on their performance. Particularly, we are inter-ested on the impact that length of texts may have to create PTM. We have studiedhow it influences to semantically relate multilingual documents and to capture theknowledge derived from their relationships. The results suggest that the most ade-quate texts to train PTM should be of equal or greater length than those used tomake inferences later and documents should be related by hierarchy-based similaritymetrics at large-scale.Keywords: probabilistic topics, text similarity, hierarchical topics, document re-trieval.Resumen: La recuperación de información ha utilizado tradicionalmente modelosvectoriales para describir los textos. A gran escala, estos modelos necesitan reducirlas dimensiones de los vectores para que las operaciones sean manejables sin com-prometer su rendimiento. Los modelos probabiĺısticos de tópicos (MPT) proponenespacios vectoriales más pequeños. Las palabras se organizan en tópicos y los doc-umentos se relacionan entre śı a partir de sus distribuciones de tópicos. Como enmuchas otras técnicas de IA, los textos utilizados para entrenar los modelos influyenen su rendimiento. En particular, nos interesa el impacto de la longitud de los textosal crear MPT. Hemos estudiado cómo influye al relacionar semánticamente docu-mentos multilingües y al capturar el conocimiento derivado de sus relaciones. Losresultados sugieren que los textos más adecuados deben ser de igual o mayor longi-tud que los utilizados para hacer inferencias posteriormente y las relaciones debenbasarse en métricas de similitud jerárquicas.Palabras clave: topicos probabiĺısticos, semejanza de textos, jerarqúıa de tópicos,recuperación de documentos.1 IntroductionProbabilistic Topic Models (PTM) (Hof-mann, 2001) (Blei, Ng, and Jordan, 2003)are statistical methods based on bag-of-wordsthat analyze the words of the original textsto discover the themes that run throughthem, how those themes are connected toeach other, or how they change over time.PTM do not require any prior annotationsor labeling of the documents. The topicsemerge, as hidden structures, from the anal-ysis of the original texts. These structuresare topic distributions, per-document topicdistributions or per-document per-word topicassignments. In turn, a topic is a distribu-tion over terms that is biased around thosewords associated to a single theme. Figure 1shows some topics that have emerged whencreating a topic model with the collection ofWikipedia articles to better understand what",
        "file_name": "PLN_67_02.pdf",
        "file_path": "PDFs\\PLN_67_02.pdf"
    },
    {
        "title": "A Dialogical Approach to Readiness for Change towards Sustainability in Higher Education Institutions: The Case of the SDGs Seminars at the Universidad Politecnica de Madrid",
        "implementation_urls": [],
        "doi": "10.3390/su13169168",
        "arxiv": null,
        "abstract": "Abstract: The transformation for sustainability requires a paradigm shift towards systems thinkingand interdisciplinary collaboration, which entails, above all, a process of cultural change affectingindividual mindsets, organizations and society as a whole. Sustainability in higher educationinstitutions (HEIs) has been a recurrent research field in the past decades. However, little attention hasbeen paid to the processes of internal and cultural change and, in particular, to the first steps to prepareacademic communities for change. Understanding “readiness for change” as a core organizationalcompetency to overcome continuous environmental changes and considering the diluted hierarchyat HEIs, this article proposes the adoption of dialogical and developmental approaches in a singleaction case, the SDGs Seminars at the Universidad Politécnica de Madrid. This methodology wasused to diagnose organizational and individual readiness for change considering cognitive, affectiveand behavioural components, and to identify consequences in organizational structures and culture.Our findings reveal that reframing dialogical spaces in HEIs to experience a collaborative andsustainability culture can unlock change, breaking down organizational silos, reducing resistancesand engaging academic communities in the cocreation of institutional strategies. Furthermore, thecase suggests that acting at the group level has impacts both on the individual and institutional levels.Keywords: readiness for change; higher education; academic culture; collaboration; dialogical;developmental; conversations; transformation; sustainable development goals (SDGs)1. IntroductionThe 2030 Agenda for Sustainable Development [1] requires a paradigm shift towardssystems thinking, collaboration and interdisciplinarity [2]. This transformation entails,above all, a process of cultural change which affects individual mindsets, organizationsand society as a whole [3–5].Higher education institutions (HEIs) are not an exception and should transform theirorganizational structure, culture and communication practices in order to overcome disci-plinary and sectoral boundaries [2,6]. In fact, their highly fragmented and monodisciplinarystructures inherited from the 19th century, as well as a conservative and competitive culturewhich encourages individualism, hierarchy, incrementalism, bureaucracy and market-oriented strategies [7–11], hinder internal and external collaborative potential [12,13]. As aconsequence, their response to societal needs is slowed down [14].Sustainability 2021, 13, 9168. https://doi.org/10.3390/su13169168 https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.comhttps://orcid.org/0000-0003-1077-5611https://orcid.org/0000-0003-2728-5731https://orcid.org/0000-0002-7441-3241https://orcid.org/0000-0003-3444-1501https://doi.org/10.3390/su13169168https://doi.org/10.3390/su13169168https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/su13169168https://www.mdpi.com/journal/sustainabilityhttps://www.mdpi.com/article/10.3390/su13169168?type=check_update&version=2Sustainability 2021, 13, 9168 2 of 29HEIs’ transformation is particularly relevant given their critical role in the achieve-ment of Sustainable Development Goals (SDGs) [1,15]. As institutions that catalyse culture,knowledge and innovation, HEIs have the potential to become change agents for sus-tainability [16–19]. Furthermore, they are in a position to support almost every SDG [20]through their five interlinked traditional dimensions, namely education; research; organiza-",
        "file_name": "pdf?version=1629191461",
        "file_path": "PDFs\\pdfversion1629191461.pdf"
    },
    {
        "title": "SPARQL2Flink: Evaluation of SPARQL Queries on Apache Flink",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oscarceballos/sparql2flink",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1628044316.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/oscarceballos/sparql2flink (accessed on 24 March 2020)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app11157033",
        "arxiv": null,
        "abstract": "Abstract: Existing SPARQL query engines and triple stores are continuously improved to handlemore massive datasets. Several approaches have been developed in this context proposing the storageand querying of RDF data in a distributed fashion, mainly using the MapReduce ProgrammingModel and Hadoop-based ecosystems. New trends in Big Data technologies have also emerged (e.g.,Apache Spark, Apache Flink); they use distributed in-memory processing and promise to deliverhigher data processing performance. In this paper, we present a formal interpretation of some PACTtransformations implemented in the Apache Flink DataSet API. We use this formalization to providea mapping to translate a SPARQL query to a Flink program. The mapping was implemented in aprototype used to determine the correctness and performance of the solution. The source code of theproject is available in Github under the MIT license.Keywords: massive static RDF data; SPARQL; PACT Programming Model; Apache Flink1. IntroductionThe amount and size of datasets represented in the Resource Description Framework(RDF) [1] language are increasing; this leads to challenging the limits of existing triple storesand SPARQL query evaluation technologies, requiring more efficient query evaluationtechniques. Several proposals have been documented in state of the art use of Big Datatechnologies for storing and querying RDF data [2–6]. Some of these proposals havefocused on executing SPARQL queries on the MapReduce Programming Model [7] and itsimplementation, Hadoop [8]. However, more recent Big Data technologies have emerged(e.g., Apache Spark [9], Apache Flink [10], Google DataFlow [11]). They use distributedin-memory processing and promise to deliver higher data processing performance thantraditional MapReduce platforms [12]. These technologies are widely used in researchprojects and all kinds of companies (e.g., Google, Twitter, and Netflix, or even by smallstart-ups).To analyze whether or not we can use these technologies to provide query evaluationover large RDF datasets, we will work with Apache Flink, an open-source platform fordistributed stream and batch data processing. One of the essential components of theFlink framework is the Flink optimizer called Nephele [13]. Nephele is based on the Paral-lelization Contracts (PACTs) Programming Model [14] which is in turn a generalization ofthe well-known MapReduce Programming Model. The output of the Flink optimizer is acompiled and optimized PACT program which is a Directed Acyclic Graphs (DAG)-baseddataflow program. At a high level, Flink programs are regular programs written in Java,Appl. Sci. 2021, 11, 7033. https://doi.org/10.3390/app11157033 https://www.mdpi.com/journal/applscihttps://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0001-5214-4127https://orcid.org/0000-0002-6488-8649https://orcid.org/0000-0002-2858-0276https://orcid.org/0000-0002-0236-4284https://orcid.org/0000-0002-9260-0753https://doi.org/10.3390/app11157033https://doi.org/10.3390/app11157033https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/app11157033https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app11157033?type=check_update&version=2Appl. Sci. 2021, 11, 7033 2 of 24Scala, or Python. Flink programs are mapped to dataflow programs, which implement",
        "file_name": "pdf?version=1628044316",
        "file_path": "PDFs\\pdfversion1628044316.pdf"
    },
    {
        "title": "Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
        "implementation_urls": [],
        "doi": "10.1145/3453172",
        "arxiv": null,
        "abstract": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requiresprior specific permission and/or a fee. Request permissions from permissions@acm.org.© 2021 Association for Computing Machinery.2160-6455/2021/06-ART11 $15.00https://doi.org/10.1145/3453172ACM Transactions on Interactive Intelligent Systems, Vol. 11, No. 2, Article 11. Publication date: June 2021.mailto:permissions@acm.orghttps://doi.org/10.1145/3453172http://crossmark.crossref.org/dialog/?doi=10.1145%2F3453172&domain=pdf&date_stamp=2021-07-2111:2 Y. Gil et al.KELLY COBOURN and ZEYA ZHANG, Department of Forest Resources and EnvironmentalConservation, Virginia Tech, Blacksburg, VA 24061CHRISTOPHER DUFFY, Department of Civil Engineering, The Pennsylvania State University,University Park, PA 16802LELE SHU, Department of Land, Air and Water Resources, University of California Davis, Davis, CA95616Major societal and environmental challenges involve complex systems that have diverse multi-scale inter-acting processes. Consider, for example, how droughts and water reserves affect crop production and howagriculture and industrial needs affect water quality and availability. Preventive measures, such as delay-ing planting dates and adopting new agricultural practices in response to changing weather patterns, canreduce the damage caused by natural processes. Understanding how these natural and human processesaffect one another allows forecasting the effects of undesirable situations and study interventions to takepreventive measures. For many of these processes, there are expert models that incorporate state-of-the-art theories and knowledge to quantify a system’s response to a diversity of conditions. A major chal-lenge for efficient modeling is the diversity of modeling approaches across disciplines and the wide vari-ety of data sources available only in formats that require complex conversions. Using expert models forparticular problems requires integration of models with third-party data as well as integration of modelsacross disciplines. Modelers face significant heterogeneity that requires resolving semantic, spatiotempo-ral, and execution mismatches, which are largely done by hand today and may take more than 2 years ofeffort.We are developing a modeling framework that uses artificial intelligence (AI) techniques to reduce mod-eling effort while ensuring utility for decision making. Our work to date makes several innovative contri-butions: (1) an intelligent user interface that guides analysts to frame their modeling problem and assiststhem by suggesting relevant choices and automating steps along the way; (2) semantic metadata for mod-els, including their modeling variables and constraints, that ensures model relevance and proper use for agiven decision-making problem; and (3) semantic representations of datasets in terms of modeling variablesthat enable automated data selection and data transformations. This framework is implemented in the MINT(Model INTegration) framework, and currently includes data and models to analyze the interactions betweennatural and human systems involving climate, water availability, agricultural production, and markets. Ourwork to date demonstrates the utility of AI techniques to accelerate modeling to support decision-makingand uncovers several challenging directions for future work.CCS Concepts: • Computing methodologies → Artificial intelligence; Ontology engineering; Planning andscheduling; Neural networks; Visual analytics; Modeling methodologies; • Applied computing → Decisionanalysis; Agriculture;Additional Key Words and Phrases: Intelligent user interfaces, integrated modeling, model metadata, regional-level decision-making, remote sensing dataACM Reference format:Yolanda Gil, Daniel Garijo, Deborah Khider, Craig A. Knoblock, Varun Ratnakar, Maximiliano Osorio, HernánVargas, Minh Pham, Jay Pujara, Basel Shbita, Binh Vu, Yao-Yi Chiang, Dan Feldman, Yijun Lin, Hayley Song,Vipin Kumar, Ankush Khandelwal, Michael Steinbach, Kshitij Tayal, Shaoming Xu, Suzanne A. Pierce, Lissa",
        "file_name": "10!1145%3453172.pdf",
        "file_path": ".\\PDFs\\10!1145%3453172.pdf"
    },
    {
        "title": "An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2021.02.018",
        "arxiv": null,
        "abstract": "AbstractKnowledge graphs (KGs) are one of the most common frameworks for knowl-edge representation. However, they suffer from a severe scalability problemthat hinders their usage. KG embedding aims to provide a solution to thisissue. Nonetheless, general approaches are incapable of representing and rea-soning about information not previously contained in the graph. This paperproposes to leverage semantic and ontological information for a significant ben-efit of knowledge graph completion, focusing on triple classification. The goalof this task is to determine whether a given fact holds. Furthermore, this paperalso considers the classification of facts that include entities that have not beenseen during training, denoted out-of-knowledge-base or OOKB entities. An in-cremental method is presented, composed of six stages. Although the proposalcan be applied to any KG embedding model, this work focuses on its applicationfor semantic matching models, such as ComplEx and DistMult. Compared toother approaches, our proposal is model-agnostic, computationally inexpensive,and does not require retraining. The results show that triple classification ac-curacy scales up to 15% with the proposed approach, as well as accelerating theconvergence of the model to its optimal solution. Furthermore, facts containingOOKB entities can be classified with a reasonable accuracy.Key words: Knowledge Graph Embeddings, Entity Initialization, KnowledgeGraph Completion, Word Embeddings, Ontological Information1. IntroductionKnowledge graphs (KGs) are currently one of the primary forms of knowledgerepresentation. In these structures, knowledge is stored in the form of facts, or∗Corresponding authorEmail address: emilioserra@fi.upm.es (Emilio Serrano)Preprint submitted to Information Science December 1, 2020 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ",
        "file_name": "10!1016%j!ins!2021!02!018.pdf",
        "file_path": ".\\PDFs\\10!1016%j!ins!2021!02!018.pdf"
    },
    {
        "title": "On learning context-aware rules to link RDF datasets",
        "implementation_urls": [
            {
                "identifier": "https://github.com/AndreaCimminoArriaga/Sorbas",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1093%jigpal%jzaa043.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "1Our prototype is available at https://github.com/AndreaCimminoArriaga/Sorbas."
                    }
                ]
            }
        ],
        "doi": "10.1093/jigpal/jzaa043",
        "arxiv": null,
        "abstract": "AbstractIntegrating RDF datasets has become a relevant problem for both researchers and practitioners. In the literature, there aremany genetic proposals that learn rules that allow to link the resources that refer to the same real-world entities, which isparamount to integrating the datasets. Unfortunately, they are context-unaware because they focus on the resources and theirattributes but forget about their neighbours. This implies that they fall short in cases in which different resources have similarattributes but refer to different real-world entities or cases in which they have dissimilar attributes but refer to the same real-world entities. In this article, we present a proposal that learns context-aware rules that take into account both the attributes ofthe resources and their neighbours. We have conducted an extensive experimentation that proves that it outperforms the mostadvanced genetic proposal. Our conclusions were checked using statistically sound methods.1 IntroductionAn RDF dataset is a collection of resources that describe real-world entities. Such datasets arecommonly used to feed a variety of automated business processes [5]. Typically, this requires tointegrate them by linking the resources that refer to the same real-world entities [1]. The resourcesare described by means of properties that can be either data properties or object properties; theformer model the attributes of the resources and the latter relate them to their neighbours.There are several state-of-the-art proposals that use genetic approaches to learn link rules [9, 10,16, 17]. Such rules basically compute the similarity of two resources by comparing their attributesusing a series of string transformations and similarity functions. If the resources are similar enough,then they are linked because they are assumed to describe the same real-world entity; otherwise, theyare kept apart. It is not difficult to realize that such context-unaware rules are imprecise in cases inwhich two resources have similar attributes but describe different real-world entities (e.g. differentpeople who have similar names or ages) or have dissimilar attributes but refer to the same real-worldentities (e.g. resources that describe different facets of a person).In this article, we present an approach to learn context-aware rules building on the context-unawarerules learnt by any of the previous proposals. By context-aware, we mean that the rule takes intoaccount the attributes of the resources being linked and the attributes of their neighbours. This is anovel approach since our analysis of the related work reveals that this is the first time that context-aware rules have been explored in this context. We have also performed an extensive experimental∗E-mail: cimmino@fi.upm.es∗∗E-mail: corchu@us.esstudy in which we sought to prove two hypothesis, namely, (i) exploring the context helps improve the effectiveness of the link rules and (ii) learning context-aware rules helps improve the efficiency of the linking process. Our experimental results and the statistical analysis that we have conducted validate these hypotheses, which prove that our proposal is very promising. Our proposal is related to a previous one in which our goal was to link two datasets on the f ly [3]; our experimentation confirms that our new proposal is as effective as the previous one but increases efficiency significantly.The rest of the article is organized as follows: Section 2 reports on the related work; Section 3 provides the details of our proposal; Section 4 presents our experimental analysis; finally, Section 5 summarizes our conclusions.2 Related workLearning link rules originated in the field of relational databases, where the problem was known as de-duplication [15], collective matching [18] or entity matching [13]. Unfortunately, it is not straightforward to adapt these results to RDF datasets because of the gap between the underlying data models.Some authors have developed a number of proposals that are specifically tailored to working with RDF datasets. Unfortunately, some of them work on a single dataset [8, 14] and others require the datasets to be modelled using OWL ontologies [4, 6, 7, 12], which hinders their general applicability. There are a few proposals that work on two RDF datasets without an explicit model [9, 10, 16, 17, 20], which makes them generally applicable. We analyse them below.Isele and Bizer [9, 10] devised GenLink, which is a genetic approach that uses a tournament ",
        "file_name": "10!1093%jigpal%jzaa043.pdf",
        "file_path": ".\\PDFs\\10!1093%jigpal%jzaa043.pdf"
    },
    {
        "title": "Towards metrics-driven ontology engineering",
        "implementation_urls": [],
        "doi": "10.1007/s10115-021-01545-9",
        "arxiv": null,
        "abstract": "AbstractThe software engineering field is continuously making an effort to improve the effectivenessof the software development process. This improvement is performed by developing quan-titative measures that can be used to enhance the quality of software products and to moreaccurately describe, better understand and manage the software development life cycle. Evenif the ontology engineering field is constantly adopting practices from software engineering,it has not yet reached a state in which metrics are an integral part of ontology engineeringprocesses and support making evidence-based decisions over the process and its outputs. Upto now, ontology metrics are mainly focused on the ontology implementation and do not takeinto account the development process or other artefacts that can help assessing the quality ofthe ontology, e.g. its requirements. This work envisions the need for a metrics-driven ontol-ogy engineering process and, as a first step, presents a set of metrics for ontology engineeringwhich are obtained from artefacts generated during the ontology development process andfrom the process itself. The approach is validated by measuring the ontology engineeringprocess carried out in a research project and by showing how the proposed metrics can beused to improve the efficiency of the process by making predictions, such as the effort neededto implement an ontology, or assessments, such as the coverage of the ontology according toits requirements.Keywords Metrics · Ontology engineering · Requirements · Ontology developmentB Alba Fernández-Izquierdoalbafernandez@fi.upm.esMaría Poveda-Villalónmpoveda@fi.upm.esAsunción Gómez-Pérezasun@fi.upm.esRaúl García-Castrorgarcia@fi.upm.es1 Ontology Engineering Group, Escuela Técnica Superior de Ingenieros Informáticos, UniversidadPolitécnica de Madrid, Madrid, Spain123http://crossmark.crossref.org/dialog/?doi=10.1007/s10115-021-01545-9&domain=pdfhttp://orcid.org/0000-0003-2011-3654https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-3037-0331https://orcid.org/0000-0002-0421-452X868 A. Fernández-Izquierdo et al.1 IntroductionSoftware metrics play an important role in the software engineering field, supporting bothdevelopment and managerial decision-making during the software life cycle. These softwaremetrics are not only related to the source code itself, but also to other artefacts that are part ofthe software main product (e.g. requirements, documentation and tests) and to the activitiesneeded to obtain such artefacts. This diversity of metrics enables software engineers to haveenough information to make different types of predictions, assessments and trade-offs, suchas effort and time predictions or software quality analysis [15].Similarly, in the ontology engineering field different metrics exist which try to assess thequality of ontologies by measuring reliability, reusability or cohesion, among other aspects.However, themetrics proposed until now aremostly focused on the ontology implementation,and they do not take into account other artefacts produced during the ontology developmentprocess or even the development process itself. Moreover, they only consider the structureof the ontology [55].",
        "file_name": "10!1007%s10115-021-01545-9.pdf",
        "file_path": ".\\PDFs\\10!1007%s10115-021-01545-9.pdf"
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%nca51143!2020!9306721.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [5] M."
                    }
                ]
            }
        ],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "abstract": "Abstract—This paper presents an architecture of a PersonalInformation Management System, in which individuals can definethe access to their personal data by means of smart contracts.These smart contracts, running on the Ethereum blockchain,implement access control lists and grant immutability, traceabil-ity and verifiability of the references to personal data, which isstored itself in a (possibly distributed) file system. A distributedauthorization mechanism is devised, where trust from multiplenetwork nodes is necessary to grant the access to the data.To this aim, two possible alternatives are described: a SecretSharing scheme and Threshold Proxy Re-Encryption scheme. Theperformance of these alternatives is experimentally comparedin terms of execution time. Threshold Proxy Re-Encryptionappears to be faster in different scenarios, in particular whenincreasing message size, number of nodes and the threshold value,i.e. number of nodes needed to grant the data disclosure.I. INTRODUCTIONThe transformation introduced by digital technologies hashad (and is having) a significant impact on economy and soci-ety. Data is at the heart of this transformation and individualsare the main sources generating more and more of it. There isan urgent need to place (again) individuals at the center and torelieve the absence of technical instruments and standards thatmake the exercise of one’s rights simple and not excessivelyburdensome [1], [2]. The EU’s GDPR 1 helps to promote thisvision and at the same time seeks to pave the way for opendata spaces for the social and economic good 2.Our aim is to seek such a technology by enabling userswith the sovereignty over their data, while guaranteeing itsconfidentiality. In our view, the data owner can define accessby limiting the scope of data utility, delegating these privilegesor giving up ownership completely, without the need to relyon (un)trusted entities to facilitate this task. The developmentof a Personal Information Management System (PIMS) 3 that∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - RIoE.1Council of European Union, Regulation 2016/679 - directive 95/462European Commission, COM(2020) 66, “A European strategy for data”3European Data Protection Supervisors, Opinion 9/2016, “EDPS Opinionon Personal Information Management Systems”fulfils these goals can be based on a distributed softwarearchitecture, where each individual is associated to a digitalspace containing personal data. This space will be used toattend the data access requests coming from data providersand data consumers. Distributed Ledger Technologies (DLT)and Decentralized File Storages (DFS) combination providesa range of features suitable for data management and sharing,such as transparency, immutability and reliability [2], [3].",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": ".\\PDFs\\10!1109%nca51143!2020!9306721.pdf"
    },
    {
        "title": "A scoping review on the use, processing and fusion of geographic data in virtual assistants",
        "implementation_urls": [
            {
                "identifier": "https://github.com/cgranell/ideais-scopingreview-2019",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1111%tgis!12720.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The computational workflow is provided as a GitHub repository at https://github.com/cgranell/ideais-scopingreview-2019 and archived in (Granell, 2020)."
                    }
                ]
            }
        ],
        "doi": "10.1111/tgis.12720",
        "arxiv": null,
        "abstract": "Abstract Virtual assistants are a growing area of research in academia and industry with an impact on people’s daily life. Many disciplines in science are moving towards the incorporation of intelligent virtual assistants in multiple scenarios and application domains, and GIScience is not external to this trend since they may be connected to Intelligent Spatial Decision Support Systems. This article presents a scoping review to indicate relevant literature pertinent to intelligent virtual assistants and their usage of geospatial information and technologies. In particular, the study was designed to find critical aspects of GIScience and how to contribute to the development of virtual assistants. Moreover, this work explores the most prominent research lines as well as relevant technologies/platforms to determine the main challenges and current limitations regarding the use and implementation of virtual assistants in the geospatial-related fields. As a result, this review shows the current state of the geospatial applications regarding the use of intelligent virtual assistants, as well as reveals gaps and limitations in the use of spatial methods, standards, and resources available in Spatial Data Infrastructures to develop intelligent decision systems based on virtual assistants for a wide array of application domains. Keywords Scoping review, virtual assistant, geographic data, processing, fusion, decision support  1 Introduction Amazon’s Alexa, Apple’s Siri, Google’s Assistant and Google Home, and Cortana are well-known examples of intelligent virtual assistants created by big technological companies with a high market penetration (Mennicken et al., 2018). A recent study about these assistants has estimated that 111.8 million people in the U.S. use them for various routine activities such as work commuting, cooking, and searching on the Internet (Petrock, 2019). Therefore, it is clear that voice-activated assistants, also known as intelligent virtual assistants, are a growing area of research in academia and industry with a significant impact on people’s daily life. In the geospatial-related fields, these kinds of assistants are recently receiving attention from the GIScience community, including them in future roadmaps on the interaction of geospatial and Artificial Intelligence (AI) (Janowicz et al., 2020; UN, 2020); thus, virtual assistants in conjunction with location can be definitively connected to Intelligent Spatial Decision Support Systems.  From a technical viewpoint, a virtual assistant is a natural language processing pipeline that includes automated speech recognition, natural language understanding, natural language generation, and text-to-speech methods to improve the interaction experience with users through questions and answers. Moreover, these assistants can incorporate search, knowledge graphs, agent back-ends, and agents of different platforms (Mennicken et al., 2018). Thereby, they can fit many different purposes. As an example, a user can initiate an intelligent virtual assistant by voice commands to perform an action (such as searching for suitable services/data, interacting with existing services) for which the standardization and protocols, as well as the processing and/or fusion of geographic information, are crucial. The results can be displayed in the form of a map, contextualized in a virtual world, sound, text, or even a combination thereof. For example, rest spaces on the route by car from Valencia to Madrid can be represented on a map, in the form of a virtual tour in a virtual globe, orally communicated, or even sent as alert messages to mobile devices. In short, virtual assistants can use machine learning and other AI techniques to extract and process knowledge from spatial data.    In this context, the IDEAIS project (Bernabé et al., 2019) pursues the creation and consolidation of an Ibero-American collaboration environment to undertake the design, exploration, and development of a new generation of virtual assistants that facilitate the access, recovery, ",
        "file_name": "10!1111%tgis!12720.pdf",
        "file_path": ".\\PDFs\\10!1111%tgis!12720.pdf"
    },
    {
        "title": "An analysis of existing production frameworks for statistical and geographic information: Synergies, gaps and integration",
        "implementation_urls": [],
        "doi": "10.3390/ijgi10060374",
        "arxiv": null,
        "abstract": "Abstract: The production of official statistical and geospatial data is often in the hands of highlyspecialized public agencies that have traditionally followed their own paths and established their ownproduction frameworks. In this article, we present the main frameworks of these two areas and focuson the possibility and need to achieve a better integration between them through the interoperabilityof systems, processes, and data. The statistical area is well led and has well-defined frameworks.The geospatial area does not have clear leadership and the large number of standards establish aframework that is not always obvious. On the other hand, the lack of a general and common legalframework is also highlighted. Additionally, three examples are offered: the first is the applicationof the spatial data quality model to the case of statistical data, the second of the application of thestatistical process model to the geospatial case, and the third is the use of linked geospatial andstatistical data. These examples demonstrate the possibility of transferring experiences/advancesfrom one area to another. In this way, we emphasize the conceptual proximity of these two areas,highlighting synergies, gaps, and potential integration.Keywords: geospatial information; statistical data; framework; interoperability1. IntroductionThe production of statistical data is nowadays more and more “geo”, and the produc-tion of geospatial data is more and more “statistical”; therefore, it is logical to envision agreater integration of these two areas. In this way, and according to the United NationsCommittee of Experts on Global Geospatial Information Management [1], the integrationof statistical and geospatial information and the resulting geospatially enabled statistics aresignificant components in meeting the data demands that inform decision-making needsat either the local, national, regional, or global level. Thereby, linking data about people,businesses, or the environment to a geographic location and their integration with othergeospatial information through their location can promote a much better understanding ofeconomic, social, and environmental perspectives.ISPRS Int. J. Geo-Inf. 2021, 10, 374. https://doi.org/10.3390/ijgi10060374 https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/journal/ijgihttps://www.mdpi.comhttps://orcid.org/0000-0001-6491-7430https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0002-8415-9531https://orcid.org/0000-0002-2983-4629https://orcid.org/0000-0001-6521-0760https://orcid.org/0000-0002-6373-4410https://doi.org/10.3390/ijgi10060374https://doi.org/10.3390/ijgi10060374https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://doi.org/10.3390/ijgi10060374https://www.mdpi.com/journal/ijgihttps://www.mdpi.com/article/10.3390/ijgi10060374?type=check_update&version=1ISPRS Int. J. Geo-Inf. 2021, 10, 374 2 of 34In this sense, geospatial information and statistical information are intrinsically related.The former uses statistical information as part of the attributes of each geospatial entity,and the latter uses geospatial information as a basis for describing the spatial distributionof some of the statistical variables. Both types of information provide a description and amodel of the real world at general scales. In this context, geospatial information describesthe geometric object of discrete and continuous phenomena characterized by positions inspace and some attributes that can record physical variables or reflect human activities. On",
        "file_name": "pdf?version=1622623685",
        "file_path": "PDFs\\pdfversion1622623685.pdf"
    },
    {
        "title": "Thesaurus enhanced extraction of Hohfeld's relations from Spanish Labour Law",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe the design of an experiment to extract Ho-hfeld’s deontic relations from legal texts. Our approach intends to minimise themanual effort in the annotation process by expanding a set of initial annotationswith the legal domain knowledge contained in thesauri represented in SemanticWeb formats. With such annotations, we perform a set of iterations to train a deeplearning relation extraction model. After analysing the results, we will adapt theprocess to work on the extraction of Hohfeld’s potestative relations. We also planto use that model to recognise relations in unseen legal sub-domains.Keywords: Relation Extraction · Thesaurus · Terminology · Semantic Web1 IntroductionNew legal documentation is being generated daily, which implies new regulations andlaws that need to be processed and, most importantly, understood. Several works havealready tackled the difficulties in legal information processing, such as [5], which identi-fies five major aggravating factors: multijurisdictionality, volume, accessibility, updatesand consolidation and vagueness of legal document classification.Natural language processing tools help solving such challenges, and they can reachgreat performance on many language understanding tasks [25]. Yet, these models re-quire significantly large annotated datasets and language resources to train. We found,however, that legal language resources are scarce, mostly monolingual, and sometimespublished in close and proprietary formats. This may be one of the reasons why most In-formation Extraction systems, and Relation Extraction tools specifically, do not handlelegal texts properly and, if they do, they tend to return very general results (see Sec-tion 2). Therefore, with the aim of making legal information understandable and easieraccessible, in this paper we describe the design of an experiment to extract relationsamongst terms in legal texts. We further represent them as part of rich domain-specificmulti-lingual resources, that can be ultimately exploited for different use cases.This work is framed within Lynx3 project, an Innovation Action funded by the Eu-ropean Union’s Horizon 2020, whose goal is to create a Knowledge Graph of legal and3 http://lynx-project.eu/2 P. Martı́n-Chozas et al.regulatory data to ease the access to information from different jurisdictions, languagesand domains. Such a Legal Knowledge Graph (LKG) could be of a great help to complywith current regulations, specially for non-legal-expert users.Amongst all legal relations the Hohfeld’s fundamental legal relations are the mostgeneral ones [10]. The Hohfeld’s relations, being the highest abstraction of all possi-ble legal relations, may serve the basis for more detailed domain-specific legal rela-tions. In other words, the legal relations appearing in legal sub-domains may be seenas sub-relation of Hohfeld’s relations. They are divided in two sets of relations: deon-tic relations (Right, Duty, No-Right and Priviledge) and potestative relations (Power,Liability, Disability and Immunity). The term “deontic” refers to a branch of the logicthat is responsible for studying the inferential relationships between normative formulasthat include the operators of permission (P), obligation (O) and prohibition (F), amongstothers [24]. While deontic relations (Figure 1) are those that modify (ordinary) actions,potestative relations modify deontic relations. In this preliminary experiment we willput the focus on the deontic relations, leaving potestative relations for future work.Right DutyNo-right Privilegeoppositeoppositecorrelativecorrelative",
        "file_name": "paper4.pdf",
        "file_path": "PDFs\\paper4.pdf"
    },
    {
        "title": "A visual SHACL shapes editor based on ontopad",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractOn the Semantic Web, vocabularies and ontologies play a fundamental role to express the terminologyand rules of certain domains. New technologies like SHACL provide the possibility to express dataschemata specific to certain data sets, applications, and domains. However, the domain modeling pro-cess is collaborative and when using RDF, it requires technical knowledge. In this paper, we presenta tool to support a two-step-process to model a terminology and a schema with a combined graphicalRDF Schema editor and visual SHACL editor. This tool allows domain experts to create a terminologyand schema without the need for a deep understanding of RDF Schema or SHACL.Demo URL: https://ontopad.aksw.org/1. IntroductionThe W3C has promoted the Shapes Constraint Language (SHACL) [6] as recommendation toconstruct schematic blueprints as shapes of RDF data. These shapes can be used to validatedata, to construct input forms to author new RDF data, and to express a domain model. Inthis way, they provide a pragmatic and flexible way to express how the individual terms ina vocabulary (classes and properties) relate to each other and how the instance data shouldlook like. To model a domain means to understand and express its language and rules. Toformally express this domain model with means of the Semantic Web is a technical process.The overall process of domainmodeling is a collaborative process that requires the involvementof domain experts. Providing a graphical tool that allows to interact with SHACL shapes byusing a visual diagram component would allow to make the RDF layer transparent to its usersand provide a visual language to interact with the data model. To support the collaborativedomain modeling process, visual editors could help to increase the involvement of domainSEMANTiCS 2021, September 6–9, 2021, Amsterdam, NLarndt@informatik.uni-leipzig.de (N. Arndt); valdestilhas@informatik.uni-leipzig.de (A. Valdestilhas);gustavo.publio@informatik.uni-leipzig.de (G. Publio); cimmino@fi.upm.es (A. Cimmino);konrad.hoeffner@uni-leipzig.de (K. Höffner); thomas.riechert@htwk-leipzig.de (T. Riechert)https://aksw.org/NatanaelArndt (N. Arndt); https://aksw.org/AndreValdestilhas (A. Valdestilhas); https://aksw.org/GustavoPublio (G. Publio); https://aksw.org/KonradHoeffner (K. Höffner); https://aksw.org/ThomasRiechert(T. Riechert)0000-0002-8130-8677 (N. Arndt); 0000-0002-0079-2533 (A. Valdestilhas); 0000-0002-3853-3588 (G. Publio);0000-0002-1823-4484 (A. Cimmino); 0000-0001-7358-3217 (K. Höffner); 0000-0003-2053-5347 (T. Riechert)© 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).CEURWorkshopProceedingshttp://ceur-ws.orgISSN 1613-0073 CEUR Workshop Proceedings (CEUR-WS.org)https://ontopad.aksw.org/mailto:arndt@informatik.uni-leipzig.demailto:valdestilhas@informatik.uni-leipzig.demailto:gustavo.publio@informatik.uni-leipzig.demailto:cimmino@fi.upm.esmailto:konrad.hoeffner@uni-leipzig.demailto:thomas.riechert@htwk-leipzig.dehttps://aksw.org/NatanaelArndthttps://aksw.org/AndreValdestilhashttps://aksw.org/GustavoPubliohttps://aksw.org/GustavoPubliohttps://aksw.org/KonradHoeffnerhttps://aksw.org/ThomasRiechert",
        "file_name": "paper16.pdf",
        "file_path": "PDFs\\paper16.pdf"
    },
    {
        "title": "RML-star: A Declarative Mapping Language for RDF-star Generation",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. RDF-star was recently proposed as a convenient represen-tation to annotate statements in RDF with metadata by introducingthe so-called RDF-star triples, bridging the gap between RDF and prop-erty graphs. However, even though there are many solutions to generateRDF graphs, there is no systematic approach so far to generate RDF-star graphs from heterogeneous data sources. In this paper, we proposeRML-star, an extension of the RML mapping language to generate RDF-star. We introduce the extension of the RML ontology and the associatedspecification with representative examples.URL: https://w3id.org/kg-construct/rml-starKeywords: RML · R2RML · RDF-star · Knowledge Graphs.1 IntroductionRDF-star was proposed as a compact representation to annotate statements inRDF with metadata [4]. For instance, the following declares that Bob claims Al-ice was born in 1996: :bob :claims <<:alice :birthYear 1996>>. Followingthe uptake of the proposed solution, a W3C Community Group was formed3 anda W3C Draft Report [5] was recently released with improvements over the orig-inal proposal. By now, several RDF-related programming libraries, e.g., EclipseRDF4J, Apache Jena, RDF.rb, and N3.js, and RDF graph database systems,e.g., Blazegraph, AnzoGraph, Stardog and GraphDB, have adopted RDF-star4.However, no mapping language supports the generation of RDF-star graphsso far. Most data are still heterogeneous, represented in different formats (e.g.,relational databases, CSV, JSON, or XML). One of the most common approachesnowadays to integrate them into RDF graphs is the use of declarative mappingCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.w3.org/community/rdf-dev/4 https://blog.liu.se/olafhartig/https://orcid.org/0000-0001-9521-2185https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://orcid.org/0000-0003-2138-7972https://w3id.org/kg-construct/rml-starhttps://www.w3.org/community/rdf-dev/https://blog.liu.se/olafhartig/Delva et al.Fig. 1: The RML-star extension (Chowlk notation [3]). Orange classes and darkorange object properties show the additions to the RML ontology, light orangeobject properties represent extensions (i.e., change in domain and/or range).languages such as R2RML [1] and RML [2]. R2RML is the W3C Recommenda-tion mapping language to generate RDF graphs from relational databases. RMLis a superset of R2RML that generates RDF graphs from data formats beyondrelational databases, such as CSV, JSON, or XML. Extending a mapping lan-guage to specify how RDF-star datasets can be generated from heterogeneousdata sources can potentially increase the amount of available RDF-star datasetsand, thus, foster the adoption of the RDF-star proposal.In this paper, we propose RML-star, an extension of RML to generate RDF-star graphs from heterogeneous data sources. We introduce a set of new classes",
        "file_name": "paper374.pdf",
        "file_path": "PDFs\\paper374.pdf"
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": ".\\PDFs\\10!23919%annsim52504!2021!9552126.pdf"
    },
    {
        "title": "ETSI SmartM2M Technical Report 103717; Study for oneM2M; Discovery and Query specification development",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": null,
        "file_name": "document",
        "file_path": "PDFs\\document.pdf"
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "arxiv": null,
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": ".\\PDFs\\10!3233%ssw51.pdf"
    },
    {
        "title": "Simulation of Hybrid Edge Computing Architectures",
        "implementation_urls": [],
        "doi": "10.1109/ds-rt52167.2021.9576121",
        "arxiv": "2108.12592",
        "abstract": "Abstract—Dealing with a growing amount of data is a crucialchallenge for the future of information and communicationtechnologies. More and more devices are expected to transfer datathrough the Internet, therefore new solutions have to be designedin order to guarantee low latency and efficient traffic manage-ment. In this paper, we propose a solution that combines theedge computing paradigm with a decentralized communicationapproach based on Peer-to-Peer (P2P). According to the proposedscheme, participants to the system are employed to relay messagesof other devices, so as to reach a destination (usually a serverat the edge of the network) even in absence of an Internetconnection. This approach can be useful in dynamic and crowdedenvironments, allowing the system to outsource part of the trafficmanagement from the Cloud servers to end-devices. To evaluateour proposal, we carry out some experiments with the help ofLUNES, an open source discrete events simulator specificallydesigned for distributed environments. In our simulations, wetested several system configurations in order to understand theimpact of the algorithms involved in the data dissemination andsome possible network arrangements.Index Terms—simulation, edge computing, peer-to-peer, com-munication, performance evaluationI. INTRODUCTIONWe are living in an era in which digital services areconstantly transformed and revised. All the tools that peopleuse are now digital, producing some kind of data that is notnecessarily stored in a local storage, but that often needs to beuploaded to distributed systems, through some communicationmeans. With the rise of the Internet of Things (IoT), anincreasingly higher number of devices is expected to join theInternet in a near future, interacting with some form of Cloudor decentralized platforms [1]. In order to manage the growingamount of traffic different novel technological solutions arebeing proposed. Among them, 5G stands out, which is capableof offering Internet access to a significantly higher number ofmobile devices with an improved efficiency [2].In this context, smart cities and smart shires [3] are supposedto emerge, with the employment of hybrid physical-digital andintelligent infrastructures that use data-driven technologies toadapt to changes in the physical environment [4]. However,the growth of data exchanges between devices needs to bemanaged not only from a network infrastructure point of view,but also from the perspective of Cloud platforms, in order toavoid an overload of requests to the servers and the resultingincreased latencies or service unavailability [5].Edge computing thus emerges as a paradigm for improvingthe efficiency of the content delivery, by decentralizing themanagement of the system and bringing computation and datastorage in locations geographically closer to the users. Withan edge computing approach, most of the activities usually",
        "file_name": "10!1109%ds-rt52167!2021!9576121.pdf",
        "file_path": ".\\PDFs\\10!1109%ds-rt52167!2021!9576121.pdf"
    },
    {
        "title": "A Community Roadmap for Scientific Workflows Research and Development",
        "implementation_urls": [],
        "doi": "10.1109/works54523.2021.00016",
        "arxiv": "2110.02168",
        "abstract": "Abstract—The landscape of workflow systems for scientificapplications is notoriously convoluted with hundreds of seeminglyequivalent workflow systems, many isolated research claims, anda steep learning curve. To address some of these challenges andlay the groundwork for transforming workflows research anddevelopment, the WorkflowsRI and ExaWorks projects partneredto bring the international workflows community together. Thispaper reports on discussions and findings from two virtual“Workflows Community Summits” (January and April, 2021).The overarching goals of these workshops were to develop a viewof the state of the art, identify crucial research challenges in theworkflows community, articulate a vision for potential communityefforts, and discuss technical approaches for realizing this vision.To this end, participants identified six broad themes: FAIR com-putational workflows; AI workflows; exascale challenges; APIs,interoperability, reuse, and standards; training and education;and building a workflows community. We summarize discussionsand recommendations for each of these themes.Index Terms—Scientific workflows, community roadmap, datamanagement, AI workflows, exascale computing, interoperabilityI. INTRODUCTIONScientific workflow systems are used almost universallyacross scientific domains for solving complex and large-scale computing and data analysis problems, and have un-derpinned some of the most significant discoveries of thepast decades [1]. Many of these workflows have significantcomputational, storage, and communication demands, and thusmust execute on a wide range of large-scale platforms, fromlocal clusters over science or public clouds to upcomingexascale HPC platforms [2]. Managing these executions isoften a significant undertaking, requiring a sophisticated andversatile software infrastructure.Historically, many of these infrastructures for workflowexecution consisted of complex, integrated systems, developedin-house by workflow practitioners with strong dependencieson a range of legacy technologies—even including sets ofad-hoc scripts. Due to the increasing need to support work-flows, dedicated workflow systems were developed to provideabstractions for creating, executing, and adapting workflowsconveniently and efficiently while ensuring portability. Whilethese efforts are all worthwhile individually, there are nowhundreds of independent workflow systems [3]. These arecreated and used by thousands of researchers and developers,leading to a rapidly growing corpus of workflows researchpublications. The resulting workflow system technology land-scape is fragmented, which may present significant barriersfor future workflow users due to the tens of seemingly com-parable, yet usually mutually incompatible, systems that exist.In the current workflow research, there are conflicting theo-retical bases and abstractions for what constitutes a workflow",
        "file_name": "10!1109%works54523!2021!00016.pdf",
        "file_path": ".\\PDFs\\10!1109%works54523!2021!00016.pdf"
    },
    {
        "title": "FOOPS!: An ontology pitfall scanner for the FAIR principles",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. This paper presents FOOPS!, a web service designed to as-sess the compliance of vocabularies or ontologies against the FAIR princi-ples. FOOPS! performs a total of 24 different checks from the four FAIRdimensions, reflecting the best practices and latest community discus-sions to adapt FAIR to semantic artefacts. The web service not onlydetect best practices according to each principle, but also offers an ex-planation of why a particular principle fails, and helpful suggestions toovercome common issues.Keywords: Ontology development · FAIR principles · FAIR semanticsPaper type: Demo (available at https://w3id.org/foops)1 IntroductionThe Findable, Accessible, Interoperable, Reusable (FAIR) data principles [6]have become increasingly relevant in the context of research data managementand reproducibility; being a main subject of discussion and adoption in commu-nity initiatives such as the Research Data Alliance, FORCE 11 and the EuropeanOpen Science Cloud. As a result, the FAIR principles have been adapted to otherresearch artifacts, such as software,1 and semantic resources such as ontologies.2In order to help researchers adopt best practices around FAIR, the scien-tific community has developed self-assessment tools and validators that helpresearchers assess the FAIRness of their resources. These are typically targeted? Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).?? The authors would like to thank Raúl Alcazar and Jacobo Mata for their help.This work has been supported by the Madrid Government under the MultiannualAgreement with Universidad Politécnica de Madrid in the line Support for R&Dprojects for Beatriz Galindo researchers, the HORIZON2020 project OntoCommons:Ontology-driven data documentation for Industry Commons (H2020-958371) and byKnowledgeSpaces: Técnicas y herramientas para la gestión de grafos de conocimien-tos para dar soporte a espacios de datos (PID2020-118274RB-I00).1 https://www.rd-alliance.org/groups/fair-research-software-fair4rs-wg2 https://www.fairsfair.eu/fair-semantics-interoperability-and-services-0Daniel Garijo, Oscar Corcho, and Maŕıa Poveda-Villalóntowards research data, such as AmIFAIR [7],3 F-UJI,4 or fair-checker,5 withsome recent additions for research software (e.g., howfairis6). However, there isno FAIR validator specifically targeted towards ontologies.In this demo we present FOOPS!, an ontology pitfall scanner for the FAIRprinciples. FOOPS! works for both OWL and SKOS vocabularies, and distin-guishes itself from existing services such as Vapour7 (focused on the quality ofthe content negotiation of resources) and OOPS! [3] (focused on common pitfallson the ontology itself); to provide a comprehensive overview of how a vocabularycomplies with current FAIR best practices for ontologies [4, 2].2 FOOPS! featuresFOOPS! is a web service and application that takes as input an OWL ontologyor SKOS thesauri and runs 24 different checks distributed across the FAIR di-mensions. These checks are based on the best practices and recommendations in[1], [4], [2], and can be summarized as follows:– Findable (9 checks): the service assesses whether the ontology URI is persis-tent, resolvable, has a resolvable version IRI, and whether that IRI is uniquefor that version. FOOPS! will also assess if minimum descriptive metadatais included (e.g., title, description, etc.) and whether the ontology prefix and",
        "file_name": "paper321.pdf",
        "file_path": "PDFs\\paper321.pdf"
    },
    {
        "title": "The Use of Decentralized and Semantic Web Technologies for Personal Data Protection and Interoperability",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-89811-3_23",
        "arxiv": null,
        "abstract": "Abstract. The enactment of the General Data Protection Regulation(GDPR) has been the response of the European Union to the growingdata-driven economy backed up by the largest companies in the world.It provides the data protection and portability needed by individualsthat “unconsciously” generate personal data for “free” services offeredby providers that lack transparency on their use. Meanwhile, the rise ofDistributed Ledger Technologies (DLTs) offers new possibilities for themanagement of general purpose data, hence being suitable for handlingpersonal data in a trustless scenario. These decentralized technologiesbring a new concept of contract called smart because of its ability tobe self-executable. DLTs and smart contracts, together with the use ofSemantic Web standards, allows the creation of a decentralized digitalspace controlled entirely by an individual, where his personal data canbe stored and transacted.Keywords: GDPR· personal data· distributed ledger technologies· smartcontracts· semantic web1 IntroductionWith the introduction of the General Data Protection Regulation (GDPR) [5]in 2018, operations carried out regarding the management and the movement ofpersonal data have radically changed. Data privacy of European Union’s Citizenhas been empowered through a series of rights that provide data protection andportability. GDPR can be seen as a necessary response to the challenges posedby technological advances brought about mainly by Big Tech companies, whichgenerate huge amounts of data without sufficient safeguards for individuals. Ahuge business, indeed, lies behind the trade of personal data and several compa-nies make consistent profits operating in this sector. GDPR and current literacyhelp the individual to understand how their personal data is often generate un-consciously and where, how or why the data is being collected, but still, furtherwork is needed to let them develop the necessarily practical and interpretive2 M. Zichichi et al.skills [15]. More efforts are needed to reach both transparency and a balancebetween privacy and data sharing.Even if GDPR requires data controllers, i.e. entities that collect and manageindividuals’ personal data, to release to their users the complete dataset theycollected on them, upon request, there are currently no standards for this kindof requests and there is the tendency to hinder the progress of these, causingthe entire process to become almost useless. These data controllers usually storethis personal information in corporate databases, but they can become dataproviders to other parties if the individual agrees –and even if the individualdoes not agree they are obliged to act as data providers in extraordinary cases,e.g. national security. As of today, when these data transactions happen there isno transparency on the individual’s data usage.Meanwhile, between the many technologies that regards general-purpose datamanagement and storage, Distributed Ledger Technologies (DLTs) are raising aspowerful tools to avoid the control centralization. The current use of DLTs is infinancial (i.e. cryptocurrencies) and data sharing scenarios. In both cases thereare several parties that concur in handling some data, there is no complete trustamong parties and often these ones compete to the data access/ownership. Suchfeatures suit perfectly with the process of moving the data sovereignty towardsusers and releasing them more influence over access control, while allowing any-",
        "file_name": "10!1007%978-3-030-89811-3_23.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-89811-3_23.pdf"
    },
    {
        "title": "R4R: Template-based REST API Framework for RDF Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs (KGs) are increasingly being used to makestructured information available on the Web, by means of REST APIsand/or SPARQL endpoints. In many cases, these REST APIs are gen-erated on top of the SPARQL endpoints, using existing technology ap-proaches that are based on proprietary configuration files or ontologies tocreate the APIs. These approaches may impose content-based or struc-tural constraints when composing Web resources. To relax these con-straints we propose R4R, a more flexible solution based on Web stan-dards and REST principles that creates and publishes customizable APIsexposing Web resources from SPARQL queries organized in file systemdirectories. R4R features include individual and nested resources, pagi-nated queries, optional fields, web authentication, query parameters andsorting lists.Resource type: SoftwareLicense: Apache License 2.0DOI: https://doi.org/10.5281/zenodo.3543320Keywords: API · Knowledge Graph · REST · SPARQL1 IntroductionKnowledge graphs (KGs) are drawing increasing attention from both academiaand industry for representing, sharing and using knowledge in applications [7, 3].They may be made available as RDF-based datasets, including a SPARQL end-point (e.g., DBpedia), and/or via REST APIs (e.g., Google Knowledge Graph).In both cases, KGs share many commonalities from the data representation pointof view (both use triples to represent facts), but they are radically different interms of query capabilities: SPARQL provides a more expressive query languagethan what can be normally done with a REST API, but it can be a barrier fornon-expert users. Web APIs usually present data according to REpresentativeState Transfer (REST) architecture principles mapping HTTP verbs (POST,GET, PUT, DELETE) to CRUD operations (Create, Read, Update, Delete).Copyright © 2021 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0)Fig. 1. Step-by-step processing of a request in R4RHowever, API resources do not necessarily match to KG resources. A publicprocurement-focused API and a technology-focused API may present, in a dif-ferent way, the information retrieved from the same KG about companies. Onemore focused on merit and the other on innovations.In this demo we present R4R, an open source framework that facilitates thepublication of a KG via a REST API over HTTP. Our approach proposes afully customizable definition of resources, both naming and content (and evennesting), through a hierarchical organization of data (Figure 1). It deploys a webservice based on SPARQL queries to retrieve the information and provides tem-plates to compose resources that are organized in folders in a system directory.Finally, we describe a motivating example where R4R is used to enhance KGaccess.2 KG data consumption via Web APISeveral approaches are available to provide Web developers with mechanismsto ease KG data consumption without dealing with the complexity of SemanticWeb standards and technologies, namely SPARQL. Some of these approacheshave been focused on the provision of Web APIs that allow developers to in-teract with KG data. There are tools [1][5] that generate Web APIs from set of",
        "file_name": "paper339.pdf",
        "file_path": "PDFs\\paper339.pdf"
    },
    {
        "title": "Toward the Ontological Modeling of Smart Contracts: A Solidity Use Case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Solidity-ontology",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%access!2021!3115577.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The ontology is published in a GitHub repository.3 The ontology includes the code in OWL, a human-friendly documentation with a descrip-tion of the classes, properties and data properties, and a graphical representation of the ontology."
                    }
                ]
            }
        ],
        "doi": "10.1109/access.2021.3115577",
        "arxiv": null,
        "abstract": "ABSTRACT Blockchain has become a pervasive technology in a wide number of sectors like industry,research, and academy. With the emergence of blockchain, new solutions with this technology to existingproblems were devised, leading to the introduction of smart contracts. Smart contracts are similar totraditional contracts with the benefits provided by blockchain, such as immutability, privacy, and decen-tralisation. These contracts are usually defined based on a specific domain, and this domain knowledge canbe represented through an ontology. Researches have explored the benefits of using domain ontologies withsmart contracts, such as code generation, discovering other contracts in the network, or interaction with othercontracts. Notwithstanding, the representation of smart contract languages themselves has not been studied.In this paper, we present an ontology for a well-known smart contract language, Solidity, defining all entitiesneeded to cover the whole language and aligning it to other standardised ontologies such as EthOn, in a wayto improve the knowledge of the ontology developed. Furthermore, the ontology has also been validatedwith already deployed contracts in the Ethereum blockchain. Thus, Solidity will be able to benefit from theadvantages provided by ontologies, such as interoperability and the use of semantic web technologies.INDEX TERMS Ontology, smart contracts, solidity, blockchain.I. INTRODUCTIONIn the last decade, blockchain technologies have becomepervasive in our world [1]. The reason is due to some keycharacteristics of these technologies such as decentralisation,immutability, transparency, privacy, and auditability [2].Withthe growth in the adoption of these technologies, the conceptof smart contract was introduced and has become a relevantcharacteristic of blockchain.Smart contracts find their roots in the formal contracts inwhich two or more trusted entities perform an agreement onsomething. In order to ensure that the agreement is fulfilledand the derived obligations are preserved, a trusted third-partyentity acts as a referee. Smart contracts are computer pro-grams in which a blockchain eliminates the requirement forthese third-party entities by ensuring that all ledger partici-pants know the details of the contracts and, due to the pro-gramming logic of smart contracts, the terms and conditionsThe associate editor coordinating the review of this manuscript andapproving it for publication was Fabrizio Messina .of the contract are automatically applied once the conditionsare met.Sectors like industry, government, or research have ben-efited from blockchain and smart contracts’ advantages.Notable companies such as Walmart [3], [4] use blockchainand smart contracts for keeping a reliable record of foodwaste, supply chain, food chain, food origin, or inventory.Even governments have used smart contracts for managingidentities and credentials in the context of governmentaldata [5], [6].Smart contracts are usually part of a domain [7], [8] suchas agreements on music rights, video games, supply chains,buying and selling, etc. In programming languages, thesedomains can be represented using Knowledge Representa-tion (a field of artificial intelligence dedicated to represent-ing information of a domain and usually use to captureinformation about the world for solving complex problems),",
        "file_name": "10!1109%access!2021!3115577.pdf",
        "file_path": ".\\PDFs\\10!1109%access!2021!3115577.pdf"
    },
    {
        "title": "Enhancing virtual ontology based access over tabular data with Morph-CSV",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-210432.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The description and features of each query are also available online24."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-210432",
        "arxiv": "2001.09052",
        "abstract": "Abstract. Ontology-Based Data Access (OBDA) has traditionally focused on providing a unified view of heterogeneous datasets(e.g., relational databases, CSV and JSON files), either by materializing integrated data into RDF or by performing on-the-fly querying via SPARQL query translation. In the specific case of tabular datasets represented as several CSV or Excel files,query translation approaches have been applied by considering each source as a single table that can be loaded into a relationaldatabase management system (RDBMS). Nevertheless, constraints over these tables are not represented (e.g., referential integrityamong sources, datatypes, or data integrity); thus, neither consistency among attributes nor indexes over tables are enforced.As a consequence, efficiency of the SPARQL-to-SQL translation process may be affected, as well as the completeness of theanswers produced during the evaluation of the generated SQL query. Our work is focused on applying implicit constraints onthe OBDA query translation process over tabular data. We propose Morph-CSV, a framework for querying tabular data thatexploits information from typical OBDA inputs (e.g., mappings, queries) to enforce constraints that can be used together withany SPARQL-to-SQL OBDA engine. Morph-CSV relies on both a constraint component and a set of constraint operators. Fora given set of constraints, the operators are applied to each type of constraint with the aim of enhancing query completenessand performance. We evaluate Morph-CSV in several domains: e-commerce with the BSBM benchmark; transportation withthe GTFS-Madrid benchmark; and biology with a use case extracted from the Bio2RDF project. We compare and report theperformance of two SPARQL-to-SQL OBDA engines, without and with the incorporation of Morph-CSV. The observed resultssuggest that Morph-CSV is able to speed up the total query execution time by up to two orders of magnitude, while it is able toproduce all the query answers.Keywords: Knowledge Graphs, Tabular Data, Mapping Languages, Constraints1. IntroductionGuided by the Open Data principles, governmentsand private organizations are regularly publishing vastamounts of public data in open data portals. For exam-ple, almost a million datasets are available in the Eu-ropean Open Data Portal (EODP)1, and many of them*Corresponding author. E-mail: dchaves@fi.upm.es.1https://www.europeandataportal.euare available in tabular formats (e.g., CSV, Excel), asobserved in Table 1. Both the simplicity of a tabularrepresentation and the variety of tools to manage a ta-ble (e.g., Excel, Calc) have influenced the popularityof tabular formats to represent open data.Albeit extensively utilized, tabular representationsimposed various data management challenges to ad-vanced users (e.g., developers, data scientists). Thelack of a unified way to query tabular data, some-thing available in other formats (e.g., RDB, JSON,1570-0844/0-1900/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:dchaves@fi.upm.esmailto:eruckhaus@fi.upm.esmailto:fpriyatna@fi.upm.esmailto:ocorcho@fi.upm.esmailto:maria.vidal@tib.eumailto:dchaves@fi.upm.eshttps://www.europeandataportal.eu2 Chaves-Fraga et al. / Enhancing Virtual OBDA over Tabular Data with Morph-CSV1 12 23 34 45 5",
        "file_name": "10!3233%sw-210432.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-210432.pdf"
    },
    {
        "title": "MOVO: a dApp for DLT-based Smart Mobility",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/movoApp",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%icccn52240!2021!9522257.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of Movo is available on GitHub: https://github.com/miker83z/movoApp."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522257",
        "arxiv": "2104.13813",
        "abstract": "Abstract—Plenty of research on smart mobility is currentlydevoted to the inclusion of novel decentralized software archi-tectures to these systems, due to the inherent advantages interms of transparency, traceability, trustworthiness. MOVO is adecentralized application (dApp) for smart mobility. It includes:(i) a module for collecting data from vehicles and smartphonessensors; (ii) a component for interacting with Distributed LedgerTechnologies (DLT) and Decentralized File Storages (DFS), forstoring and validating sensor data; (iii) a module for \"offline\"interaction between devices. The dApp consists of an Androidapplication intended for use inside a vehicle, which helps theuser/driver collect contextually generated data (e.g. a driver’sstress level, an electric vehicle’s battery level), which can then beshared through the use of DLT (i.e., IOTA DLT and Ethereumsmart contracts) and DFS (i.e., IPFS). The third module consistsof an implementation of a communication channel that, via Wi-FiDirect, allows two devices to exchange data and payment informa-tion with respect to DLT (i.e. cryptocurrency and token) assets.In this paper, we describe the main software components andprovide an experimental evaluation that confirms the viability ofthe MOVO dApp in real mobility scenarios.Index Terms—Smart Mobility, Distributed Ledger Technolo-gies, Decentralized Application, VANETI. INTRODUCTIONIn the last decade smart mobility has emerged as way toefficiently improve mobility, travel security and increase theoptions for travellers. The general idea is usually that ofdevising a sort of middleware to build advanced applicationsfor the provision of innovative transport and traffic manage-ment services, with the aim of enabling users “to be betterinformed and make safer, more coordinated and ‘smarter’ useof transport networks” [1].Vehicles and infrastructures are becoming increasingly“smarter”, which means that they are equipped with sensorsthat track a huge amount of different types of information,e.g. data sensed by the interior of the vehicle, the surroundingenvironment, road conditions, etc. In addition, the growthof smartphones and Internet-of-Things (IoT) devices enablesindividuals’ ubiquitous connectivity and the ability to collectpersonal information or crowdsensing data. All of this consti-tute a network of devices that is usually referred as VANET(Vehicular Ad-hoc NETwork) [2], [3].This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.In this context, a middleware platform can be designedto share and reuse data, services and computation, simpli-fying the development of new services and the integration",
        "file_name": "10!1109%icccn52240!2021!9522257.pdf",
        "file_path": ".\\PDFs\\10!1109%icccn52240!2021!9522257.pdf"
    },
    {
        "title": "Towards Decentralized Complex Queries over Distributed Ledgers: a Data Marketplace Use-case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%icccn52240!2021!9522165.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [6] J."
                    }
                ]
            }
        ],
        "doi": "10.1109/icccn52240.2021.9522165",
        "arxiv": "2104.13819",
        "abstract": "Abstract—Distributed Ledger Technologies (DLT) and Decen-tralized File Storages (DFS) are becoming increasingly used tocreate common, decentralized and trustless infrastructures whereparticipants interact and collaborate in Peer-to-Peer interactions.A prominent use case is represented by decentralized datamarketplaces, where users are consumers and providers at thesame time, and trustless interactions are required. However, datain DLTs and DFS are usually unstructured and there are noefficient mechanisms to query a certain type of data for thesearch in the market. In this paper, we propose the use of aDistributed Hash Table (DHT) as a layer on top of DLTs where,once the data are acquired and stored in the ledger, these can besearched through multiple keyword based queries, thanks to thelookup functionalities offered by the DHT. The DHT network is ahypercube overlay structure, organized for an efficient processingof multiple keyword-based queries. We provide the architectureof such solution for a decentralized data marketplace and ananalysis based on a simulation that proves the viability of theproposed approach.Index Terms—Distributed Ledger Technology, DecentralizedFile Storage, Distributed Hash Table, Data Marketplace, Key-word SearchI. INTRODUCTIONThe transformation brought about by digital technologieshas data at its core and has had a significant impact oneconomies and societies around the world. The ability to easilyget hold of data has the potential to create a data market wheremore and more users are consumers and providers at the sametime. However, obtaining large amounts of data that is not ofdubious or false origin is often a challenge.In order to tackle this issue, Distributed Ledger Technolo-gies (DLT) and the realm of decentralized technologies (e.g.Decentralized File Storages (DFS)), that are emerging aroundthem, come to the rescue [1], [2]. By creating a common,decentralized and trustless infrastructure, i.e. a decentralizeddata marketplace, it will be possible for data consumers andproviders to interact and collaborate in Peer-to-Peer interac-tions [3], [4]. DLTs enable peers to engage in financial transac-tions without establishing a trust relationship. Benefits oftencited of DLTs, indeed, include enabling secure transactionsThis work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - Rights of Internet ofEverything.between untrusted parties through consensus mechanisms,high availability, and the ability to automate and enforceprocesses through smart contracts [5].With the management of market interactions based on theuse of DLT and decentralized technologies, what remains is",
        "file_name": "10!1109%icccn52240!2021!9522165.pdf",
        "file_path": ".\\PDFs\\10!1109%icccn52240!2021!9522165.pdf"
    },
    {
        "title": "Latest enhancements in the Spanish DBpedia",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. The Spanish DBpedia is a data source used initially to sup-port the Spanish community. However, our logs show that the Spanishlanguage goes beyond Spanish speakers and many non-Spanish speakersuse the Spanish DBpedia on a daily basis. In the last months we havemade two important enhancements to the Spanish DBpedia: (1) we pub-lish a nonstandard dataset containing the type of resources that in thestandard distribution have no type, and (2) we update automatically ourdata every week by using the DBpedia databus. In this way, we satisfya frequent request made by companies and we foster the usage of theSpanish language, the second mother language by the number of speak-ers (after Chinese), and the second in scientific papers (after English).Keywords: Spanish DBpedia · Resource type · DBpedia data bus.1 Introduction1.1 The rising of the Spanish languageThe data published by the Cervantes Institute in its 2020 report [4] are over-whelming: Spanish speakers have increased by 30% in the last decade, and thenumber of foreigners who study it has grown by 60%. More than 585 millionpeople speak Spanish. Of these, almost 489 million are native Spanish speakers.Furthermore, Spanish is the second mother tongue by number of speakers afterMandarin Chinese, and the third language in the global count of users after En-glish and Mandarin Chinese. On the Internet, it is the third most used and isthe second language, behind English, publishing scientific texts.The DBpedia project has long generated semantic information from EnglishWikipedia. Since June 2011, the information generation process has extractedinformation from Wikipedia in 111 of its languages, but only 18 languages have aDBpedia chapter with a website. One of them is Spanish. The DBpedia Interna-tionalization Committee has assigned a website and a SPARQL [7] endpoint for? Partially supported by HcommonK (RTC2019-007134-7) and Datos4.0 (TIN2013-46238-C4-3-R) projects.Copyright ©2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 S. Sanz-Lucio et al.each of these languages1. In the case of Spanish (with website es.dbpedia.org),the extraction process produces more than 100 million RDF triples from theSpanish Wikipedia. All these triples are available on the SPARQL endpointes.dbpedia.org/sparql using Semantic Web [2] and Linked Data [1] technolo-gies.1.2 The DBpedia datasetsAs we have mentioned previously, DBpedia extracts data from 111 differentlanguage editions of Wikipedia. Then, for each language we have a knowledgebase (a “Knowledge Graph” in modern terminology, abbreviated as KG). Thelargest DBpedia KG is extracted from the English edition of Wikipedia, witharound 400 million facts (triples) that describe 3.7 million resources (Wikipediaentries). The DBpedia knowledge graphs that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 millionadditional resources. Therefore, two-thirds of the information in DBpedia comesfrom non-English Wikipedias.From a technical perspective, the DBpedia project maps Wikipedia infoboxes [12]from 27 different language editions into the DBpedia ontology, a single shared on-tology consisting of 320 classes and 1,650 properties. The mappings are created",
        "file_name": "paper8.pdf",
        "file_path": "PDFs\\paper8.pdf"
    },
    {
        "title": "Knowledge graph construction with R2RML and RML: An ETL system-based overview",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have proven to be a powerful technologyto integrate and structure the myriad of data available nowadays. Thesemantic web community has actively worked on data integration sys-tems, providing an important set of engines and mapping languages tofacilitate the construction of knowledge graphs. Despite these impor-tant efforts, there is a lack of objective evaluations of the capabilities ofthese engines in terms of performance, scalability, and conformance withmapping specifications. In this work, we conduct such evaluation consid-ering several R2RML and RML processors to identify their strengths andweaknesses. We (i) perform a qualitative analysis of the distinctive fea-tures of each engine, (ii) examine their conformance with the mappinglanguage specification they support, and (iii) assess their performanceand scalability using the GTFS-Madrid-Bench benchmark.Keywords: Knowledge Graphs · RML · R2RML · GTFS-Madrid-Bench1 IntroductionIn recent years, knowledge graphs (KGs) have become one of the most widelyused technologies in data integration reaching the top positions of the GartnerHype Cycle for Artificial intelligence in 20203. This popularity has resulted inopen KGs like Wikidata [25] or YAGO [12], and in the adoption of this tech-nology by major technology companies such as Facebook, Google, or eBay [19].To construct KGs from non-RDF data sources, mapping languages allow practi-tioners to define the relationships between input data sources and ontologies inCopyright© 2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).3 https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://orcid.org/0000-0002-3029-6469https://orcid.org/0000-0002-8235-7331https://orcid.org/0000-0001-5375-8024https://orcid.org/0000-0002-2924-7272https://orcid.org/0000-0002-5603-6390https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/Arenas-Guerrero et al.a declarative and maintainable manner [17]. Although there are several mappinglanguages in the state of the art (e.g., SPARQL-Generate [16] or ShExML [9]),there are two of them that stand out: R2RML [5], which is the W3C standardlanguage for RDB2RDF mapping, and RML [8], which is a well-known extensionof R2RML for data formats beyond relational databases (RDBs).KGs can be constructed with [R2]RML-compliant engines that can imple-ment two strategies: materialization or virtualization [20]. The former is theETL approach that generates the entire KG (i.e., all the triples), while the lat-ter generates results for SPARQL queries by translating them to the native querylanguage of the input data source (e.g., SQL queries in the case of RDBs) [3,21].Given the high number of engines available [7,13,3,23,22,10], it is easy for anypractitioner to get lost in deciding which one best fits their use case. Whilethere are comprehensive and structured evaluations for the virtualization ap-proach [4,15] that ease the user’s choice, there is a lack of such an evaluation for",
        "file_name": "paper11.pdf",
        "file_path": "PDFs\\paper11.pdf"
    },
    {
        "title": "Understanding the phenomenology of reading through modelling",
        "implementation_urls": [
            {
                "identifier": "https://github.com/eureadit/reading-experience-ontology",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-200396.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "interview part 2, https://soundcloud.com/memoriesoffiction/ferelith-part-2."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-200396",
        "arxiv": null,
        "abstract": "Abstract. Large scale cultural heritage datasets and computational methods for the Humanities research framework are the two pillars of Digital Humanities (DH), a research field aiming to expand Humanities studies beyond specific sources and periods to address macro-scale research questions on broad human phenomena. In this regard, the development of machine-readable semantically enriched data models based on a cross-disciplinary “language” of phenomena is critical for achieving the interoperability of research data. This paper reports on, documents, and discusses the development of a model for the study of reading experiences as part of the EU JPI-CH project Reading Europe Advanced Data Investigation Tool (READ-IT). Through the discussion of the READ-IT ontology of reading experience, this contribution will highlight and address three challenges emerging from the development of a conceptual model for the support of research on cultural heritage. Firstly, this contribution addresses modelling for multi-disciplinary research. Secondly, this work describes the development of an ontology of reading experience, under the light of the experience of previous projects, and of ongoing and future research developments. Lastly, this contribution addresses the validation of a conceptual model in the context of ongoing research, the lack of a consolidated set of theories and of a consensus of domain experts.  Keywords: Reading Experience, Conceptual Modelling, Experience Ontology, Digital Humanities, Modelling Methods  1*Corresponding author. E-mail: alessio.antonini@open.ac.uk.  1.  Introduction The combination of digital sources and computational methods is at the centre of a change of paradigm and of research breakthroughs on cultural heritage. Firstly, the discoverability of sources described and enriched through the Semantic Web enables the construction of integrated datasets of sources based on different archives. Secondly, data integration, as well as quantitative and qualitative studies, complement the in-depth analysis of individual sources. The use of large-scale datasets and computational methods applied within a Humanities research framework is the pillar of the revolution of the Digital Humanities (DH).  The current challenge for the Digital Humanities is how to scale up from the established paradigm of focused studies of specific sources and periods, to macro-scale research addressing broad human phenomena over the longue durée, as represented in cultural heritage [1]. While the humanistic research of the past has focused on scarce and hence exceptional case studies, the radical digital reconstruction of the cultural heritage archive permits for the first time the study of more extensive contexts or ideas [2]. In this vision, the systematic study of data generated by research case studies that focus on the human phenomenon of reading (see Section 2) could unlock advancements on macro-scale questions related to understanding the human condition through time [3]. To realise this vision, a crucial issue to be addressed is the development of a shared “language” for the formalisation of the phenomenon of reading to be used in the production of computable research data [4]. ",
        "file_name": "10!3233%sw-200396.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-200396.pdf"
    },
    {
        "title": "ENIGMA and global neuroscience: A decade of large-scale studies of the brain in health and disease across more than 40 countries",
        "implementation_urls": [
            {
                "identifier": "https://github.com/npnl/PALS",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\s41398-020-0705-1.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Preprint at https://doi.org/10.31234/osf.io/jnsb2."
                    }
                ]
            }
        ],
        "doi": "10.1038/s41398-020-0705-1",
        "arxiv": null,
        "abstract": "AbstractThis review summarizes the last decade of work by the ENIGMA (Enhancing NeuroImaging Genetics through MetaAnalysis) Consortium, a global alliance of over 1400 scientists across 43 countries, studying the human brain in healthand disease. Building on large-scale genetic studies that discovered the first robustly replicated genetic loci associatedwith brain metrics, ENIGMA has diversified into over 50 working groups (WGs), pooling worldwide data and expertiseto answer fundamental questions in neuroscience, psychiatry, neurology, and genetics. Most ENIGMA WGs focus onspecific psychiatric and neurological conditions, other WGs study normal variation due to sex and gender differences,or development and aging; still other WGs develop methodological pipelines and tools to facilitate harmonizedanalyses of “big data” (i.e., genetic and epigenetic data, multimodal MRI, and electroencephalography data). Theseinternational efforts have yielded the largest neuroimaging studies to date in schizophrenia, bipolar disorder, majordepressive disorder, post-traumatic stress disorder, substance use disorders, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, autism spectrum disorders, epilepsy, and 22q11.2 deletion syndrome. More recentENIGMA WGs have formed to study anxiety disorders, suicidal thoughts and behavior, sleep and insomnia, eatingdisorders, irritability, brain injury, antisocial personality and conduct disorder, and dissociative identity disorder. Here,we summarize the first decade of ENIGMA’s activities and ongoing projects, and describe the successes and challengesencountered along the way. We highlight the advantages of collaborative large-scale coordinated data analyses fortesting reproducibility and robustness of findings, offering the opportunity to identify brain systems involved in clinicalsyndromes across diverse samples and associated genetic, environmental, demographic, cognitive, and psychosocialfactors.IntroductionThe ENIGMA (Enhancing NeuroImaging Geneticsthrough Meta Analysis) Consortium is a collaboration ofmore than 1400 scientists from 43 countries studying thehuman brain. ENIGMA started 10 years ago, in 2009, withthe initial aim of performing a large-scale neuroimaginggenetic study, and has since diversified into 50 workinggroups (WGs), pooling worldwide data, resources andexpertise to answer fundamental questions in neu-roscience, psychiatry, neurology, and genetics (Fig. 1shows a world map of participating sites, broken down byworking group). Thirty of the ENIGMA WGs focus onspecific psychiatric and neurologic conditions. Four studydifferent aspects of development and aging. Others studykey transdiagnostic constructs, such as irritability, and theimportance of evolutionarily interesting genomic regionsin shaping human brain structure and function. Central tothe success of these WGs are the efforts of dedicatedmethods development groups within ENIGMA. There arecurrently 12 WGs that develop and disseminate multi-scale and ‘big data’ analysis pipelines to facilitate harmo-nized analyses using genetic and epigenetic data,multimodal (anatomical, diffusion, functional) magnetic© The Author(s) 2020OpenAccessThis article is licensedunder aCreativeCommonsAttribution 4.0 International License,whichpermits use, sharing, adaptation, distribution and reproductionin any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate ifchangesweremade. The images or other third partymaterial in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to thematerial. Ifmaterial is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtainpermission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.Correspondence: Paul M. Thompson (pthomp@usc.edu)Full list of author information is available at the end of the article.",
        "file_name": "s41398-020-0705-1.pdf",
        "file_path": "PDFs\\s41398-020-0705-1.pdf"
    },
    {
        "title": "Genetic correlations and genome-wide associations of cortical structure in general population samples of 22,824 adults",
        "implementation_urls": [
            {
                "identifier": "https://github.com/bulik/ldsc",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\s41467-020-18367-y.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Additional information Supplementary information is available for this paper at https://doi.org/10.1038/s41467-020-18367-y."
                    }
                ]
            }
        ],
        "doi": "10.1038/s41467-020-18367-y",
        "arxiv": null,
        "abstract": null,
        "file_name": "s41467-020-18367-y.pdf",
        "file_path": "PDFs\\s41467-020-18367-y.pdf"
    },
    {
        "title": "GTFS-Madrid-Bench: A benchmark for virtual knowledge graph access in the transport domain",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/gtfs-bench",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!websem!2020!100596.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "All the resources described in this section are available online.10 3.1."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2020.100596",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1016%j!websem!2020!100596.pdf",
        "file_path": ".\\PDFs\\10!1016%j!websem!2020!100596.pdf"
    },
    {
        "title": "Personal Data Access Control Through Distributed Authorization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/ethereum/wiki",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%nca51143!2020!9306721.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/ethereum/wiki/wiki/White-Paper [5] M."
                    }
                ]
            }
        ],
        "doi": "10.1109/nca51143.2020.9306721",
        "arxiv": "2101.10464",
        "abstract": "Abstract—This paper presents an architecture of a PersonalInformation Management System, in which individuals can definethe access to their personal data by means of smart contracts.These smart contracts, running on the Ethereum blockchain,implement access control lists and grant immutability, traceabil-ity and verifiability of the references to personal data, which isstored itself in a (possibly distributed) file system. A distributedauthorization mechanism is devised, where trust from multiplenetwork nodes is necessary to grant the access to the data.To this aim, two possible alternatives are described: a SecretSharing scheme and Threshold Proxy Re-Encryption scheme. Theperformance of these alternatives is experimentally comparedin terms of execution time. Threshold Proxy Re-Encryptionappears to be faster in different scenarios, in particular whenincreasing message size, number of nodes and the threshold value,i.e. number of nodes needed to grant the data disclosure.I. INTRODUCTIONThe transformation introduced by digital technologies hashad (and is having) a significant impact on economy and soci-ety. Data is at the heart of this transformation and individualsare the main sources generating more and more of it. There isan urgent need to place (again) individuals at the center and torelieve the absence of technical instruments and standards thatmake the exercise of one’s rights simple and not excessivelyburdensome [1], [2]. The EU’s GDPR 1 helps to promote thisvision and at the same time seeks to pave the way for opendata spaces for the social and economic good 2.Our aim is to seek such a technology by enabling userswith the sovereignty over their data, while guaranteeing itsconfidentiality. In our view, the data owner can define accessby limiting the scope of data utility, delegating these privilegesor giving up ownership completely, without the need to relyon (un)trusted entities to facilitate this task. The developmentof a Personal Information Management System (PIMS) 3 that∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieInternational Training Network European Joint Doctorate grant agreement No814177 Law, Science and Technology Joint Doctorate - RIoE.1Council of European Union, Regulation 2016/679 - directive 95/462European Commission, COM(2020) 66, “A European strategy for data”3European Data Protection Supervisors, Opinion 9/2016, “EDPS Opinionon Personal Information Management Systems”fulfils these goals can be based on a distributed softwarearchitecture, where each individual is associated to a digitalspace containing personal data. This space will be used toattend the data access requests coming from data providersand data consumers. Distributed Ledger Technologies (DLT)and Decentralized File Storages (DFS) combination providesa range of features suitable for data management and sharing,such as transparency, immutability and reliability [2], [3].",
        "file_name": "10!1109%nca51143!2020!9306721.pdf",
        "file_path": ".\\PDFs\\10!1109%nca51143!2020!9306721.pdf"
    },
    {
        "title": "Enhancing Public Procurement in the European Union Through Constructing and Exploiting an Integrated Knowledge Graph",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/ocds-ontology",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-62466-8_27.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The OCDS ontology is available on GitHub in two versions29: one with the core OCDS terms and another with the extensions."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62466-8_27",
        "arxiv": null,
        "abstract": "Abstract. Public procurement is a large market affecting almost ev-ery organisation and individual. Governments need to ensure efficiency,transparency, and accountability, while creating healthy, competitive andvibrant economies. In this context, we built a platform, consisting of aset of modular APIs and ontologies to publish, curate, integrate, analyse,and visualise an EU-wide, cross-border, and cross-lingual procurementknowledge graph. We developed end-user tools on top of the knowledgegraph, for anomaly detection and cross-lingual document search. Thispaper describes our experiences and challenges faced in creating sucha platform and knowledge graph and demonstrates the usefulness ofSemantic Web technologies for enhancing public procurement.Keywords: Public procurement · Knowledge graph · Linked data.1 IntroductionThe market around public procurement is large enough so as to affect almostevery single citizen and organisation across a variety of sectors. For this reason,public spending has always been a matter of interest at local, regional, andnational levels, and even more so, in times of great austerity and increased publicscrutiny. Primarily, governments need to be efficient in delivering services, ensuretransparency, prevent fraud and corruption, and build healthy and sustainableeconomies [1, 12]. In the European Union, every year, over 250.000 public authori-ties spend around 2 trillion euros (about 14% of GDP) on the purchase of services,works, and supplies1; while OECD estimates that more than 82% of fraud andcorruption cases remain undetected across all OECD countries [18] costing as1 https://ec.europa.eu/growth/single-market/public-procurement_en2 A. Soylu et al.high as 990 billion euros a year in the EU [10]. Moreover, SMEs are often lockedout of markets due to the high cost of obtaining the required information, wherelarger companies can absorb the cost. This leads to a tendency for governmentsto rely on monolithic suppliers without adequate competition to deliver goodvalue for the taxpayers.The availability of high quality, open, and integrated procurement data couldalleviate some of the aforementioned challenges. This includes government agen-cies assessing purchasing options, companies exploring new business contracts,and other parties (such as journalists, researchers, local communities, businessassociations, transparency activists, and individual citizens) looking for a betterunderstanding of the intricacies of the public procurement landscape throughdecision-making and analytic tools. Projects such as the UK’s GCloud (Govern-ment Cloud)2 have already shown that small businesses can compete effectivelywith their larger counterparts, given the right environment. However, managingthese competing priorities at a national level and coordinating them across differ-ent states and many disparate agencies is notoriously difficult. There are severaldirectives put forward by the European Commission (e.g., Directive 2003/98/ECand Directive 2014/24/EU8) for improving public procurement practices. Theseled to the emergence of national public procurement portals living togetherwith regional, local as well as EU-wide public portals [9]. Yet, there is a lack ofcommon agreement across the EU (in many cases, even inside the same country)on the data formats for exposing such data sources and on the data models forrepresenting such data, leading to a highly heterogeneous technical landscape.To this end, in order to deal with the technical heterogeneity and to con-nect disparate data sources currently created and maintained in silos, we built a",
        "file_name": "10!1007%978-3-030-62466-8_27.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-62466-8_27.pdf"
    },
    {
        "title": "Using LOT methodology to develop a noise pollution ontology: a Spanish use case",
        "implementation_urls": [
            {
                "identifier": "https://github.com/opencitydata/medio-ambiente-contaminacion-acustica",
                "type": "git",
                "paper_frequency": 5,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%s12652-019-01561-2.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The ontology code is available in our GitHub repository, as explained in subsection 3.1.1."
                    }
                ]
            }
        ],
        "doi": "10.1007/s12652-019-01561-2",
        "arxiv": null,
        "abstract": "Abstract Local administrations generate large amounts of data due to theprocesses followed to attend administrative governance issues and the needsof its citizenry. Sadly, in most cases this data is not fully exploited and re-mains within the institutions, making their reutilization difficult. Currently,open data initiatives had gained ground worldwide and more cities are takingadvantage of adopting an open data strategy, which are visible at the orga-nizational level and at user level. In this respect, there is a need to generateguidelines that allow cities: a) to identify datasets to be shared, for examplepollution, commercial premises, public services, etc. and b) to publish qualitydata on their portals. Data should be accurate and interoperable among citiesto facilitate reuse. This work describes the development process of an ontologyto represent the acoustic pollution data collected by measurement stations lo-cated in cities, providing a common model for data publication. The developedontology reuses several well-known ontologies and includes classes, propertiesand instances specifically created to cover this domain. This work also includesreal examples about how to instantiate the ontology.Keywords noise pollution · open data · ontologyPaola Espinoza-Arias* Corresponding authorE-mail: pespinoza@fi.upm.esMaŕıa Poveda-VillalónE-mail: mpoveda@fi.upm.esOscar CorchoE-mail: ocorcho@fi.upm.es1 Ontology Engineering Group, ETSI Informáticos, Universidad Politécnica de Madrid,28660 Madrid, Spain.2 Paola Espinoza-Arias et al.1 IntroductionNoise pollution is one of the environmental problems affecting the health andwell-being of citizens in large towns and cities. According to the World HealthOrganization, noise is the second largest environmental cause of health prob-lems, just after the impact of air quality [5]. Bearing in mind this public healthissue, many cities around the world have defined several regulations in order toassess and manage environmental noise. In the case of the European Union, in2002 the Directive 2002/49/EC [10], also known as the Environmental NoiseDirective (END), was passed in order to identify noise pollution levels and totrigger the necessary action both at Member State and at European Unionlevel. The END defines the environmental noise pollution as noise caused byroad, rail and airport traffic, industry, construction, as well as some otheroutdoor activities. In accordance with one of the key areas related with thisdirective, Member States are committed to reporting their strategic noise mapsevery five years. The purpose of these maps is presenting and assessing the cal-culated/measured noise levels over a geographical area in order to determinethe population exposed by this pollution. In the context of national legislationand local regulations, in Spain the Ley del Ruido 37/2003 [3] has also beendefined for protecting citizens from excessive noise pollution.In order to detect the noise level of cities, several sensors, located at strate-gic places (e.g. train stations, airports, wide boulevards, etc.), are used to col-lect these data. These sensors, in most cases, are part of an interconnected sen-sor network deployed over the city. Although cities are reporting their strategic",
        "file_name": "10!1007%s12652-019-01561-2.pdf",
        "file_path": ".\\PDFs\\10!1007%s12652-019-01561-2.pdf"
    },
    {
        "title": "MOATcoin",
        "implementation_urls": [
            {
                "identifier": "https://github.com/interwork-alliance/TokenTaxonomyFramework",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3410699!3413798.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Published Version: MOATcoin: Exploring Challenges and Legal Implications of Smart Contracts Through a Gamelike DApp Experiment This version is available at: https://hdl.handle.net/11585/768261 since: 2021-05-25 Published: DOI: http://doi.org/10.1145/3410699.3413798 Terms of use: (Article begins on next page) Some rights reserved."
                    }
                ]
            }
        ],
        "doi": "10.1145/3410699.3413798",
        "arxiv": null,
        "abstract": "ABSTRACT In this paper we present MOATcoin, a gamelike experiment that enabled us to practically investigate the open issues related to the governance and legal facets of smart contract based decentralized applications. After presenting the MOATcoin system architecture, we first tackle the problems of decentralized governance, its limits and the shift of trust that it entails; then, we explore the possible legal implications of the given scenario and, particularly, the consequences of code-written contracts. Finally, we offer taxonomical remarks on the concepts of “token” and “coin” and offer an insight from a regulatory perspective. CCS CONCEPTS Applied computing → Law; Computer systems organization → Distributed architectures. KEYWORDS Distributed Ledger Technologies, Smart Contract, Blockchain, Governance, Tokens, Crypto-assets, dApp, Law and Technology   mailto:biagio.distefano@univie.ac.atmailto:nadia.pocher@uab.catmailto:mirko.zichichi@upm.eshttps://www.last-jd-rioe.eu/http://lbl.cirsfid.unibo.it/1 INTRODUCTION Distributed Ledger Technologies (DLTs), originally introduced in the form of blockchain [20], enable the advent of a new vision to consider finance, trust in communication and governance. The distributed ledger ensures the immutable persistence of data, thus providing untampered data to applications when necessary. For this reason, DLTs represent an interesting technology for the development of reliable, decentralized applications (hereinafter: DApp) and services, based on Smart Contracts [3, 5]. In this paper we investigate the multidisciplinary facets, challenges and legal implications of smart contract-powered social interaction governance tools exploiting a “decentralized” game called MOATcoin1. Although this Dapp is just a game, with its simplification of a real-life scenario, it showcases crucial implications of real-world smart contract use cases. This made it possible to critically analyse what we consider to be the crucial matters of DApps that claim to have legal relevance. In order to perform this experiment, we set up a simple set of rules for a game that had to be executed on a decentralized environment. The game scenario pictures twenty Law, Science and Technology Joint Doctorate PhD candidates talking incessantly about work, even after office hours. To solve this, a simple rule was imposed: whoever pronounces certain work-related keywords (e.g., “GDPR”, “blockchain”) outside the office premises, has to buy a beer to another candidate. Since all twenty candidates are peers and no governing bodies were to be formed, we adopted a decentralized architecture based on Ethereum smart contracts [5]2. MOATcoin enables us to question the actual level of decentralization of DApps, as well as to explore governance and legal impacts of software architecture choices and the relevance of the concept of “token”, to the end of highlighting problems and possible solutions. Along with this, a complementary issue needs to be addressed. DLT-related discourse is flooded with terms such as “coins”, “cryptoassets”, “tokens”, “virtual currencies”, “cryptocurrencies”, “digital assets”, “virtual assets”, etc. From a legal and regulatory perspective, these expressions give way to endless considerations and uncertainties. From a practical perspective, it is pivotal to find a balance that avoids the loss of any technical and non-technical common ground. The remainder of this paper is organized as follows: Sections 2 and Section 3 introduce some background and related work and outline the architecture of MOATcoin. From Section 4 we start discussing the experiment results by providing an overview of MOATcoin’s governance model and highlighting weak points and possible solutions. Then, Section 5 offers a perspective on the relation between smart and legal contracts. In Section 6 we place MOATcoin within the world of tokens, by introducing taxonomy considerations and relevant implications. Finally, Section 7 provides some concluding remarks.  ",
        "file_name": "10!1145%3410699!3413798.pdf",
        "file_path": ".\\PDFs\\10!1145%3410699!3413798.pdf"
    },
    {
        "title": "Discovering hidden mental states in open multi-agent systems by leveraging multi-protocol regularities with machine learning",
        "implementation_urls": [],
        "doi": "10.3390/s20185198",
        "arxiv": null,
        "abstract": "Abstract: The agent paradigm and multi-agent systems are a perfect match for the design of smartcities because of some of their essential features such as decentralization, openness, and heterogeneity.However, these major advantages also come at a great cost. Since agents’ mental states are hiddenwhen the implementation is not known and available, intelligent services of smart cities cannotleverage information from them. We contribute with a proposal for the analysis and prediction ofhidden agents’ mental states in a multi-agent system using machine learning methods that learnfrom past agents’ interactions. The approach employs agent communication languages, which is acore property of these multi-agent systems, to infer theories and models about agents’ mental statesthat are not accessible in an open system. These mental state models can be used on their own orcombined to build protocol models, allowing agents (and their developers) to predict future agents’behavior for various tasks such as testing and debugging them or making communications moreefficient, which is essential in an ambient intelligence environment. This paper’s main contribution isto explore the problem of building these agents’ mental state models not from one, but from severalinteraction protocols, even when the protocols could have different purposes and provide distinctambient intelligence services.Keywords: open multi-agent system; smart city; agent communication languages; agent-orientedsoftware engineering1. IntroductionSmart cities are technologically based on the combination of several socio-technical innovationssuch as: the Internet of Things (IoT), mobile Internet access, smartphones, data analytics, open datainitiatives, and sharing economy models, among others [1]. This allows these cities to manage assetsand resources efficiently by services enhanced with intelligence such as: traffic management, hospitals,transportation systems, power and water plants, waste management, etcetera.The open and heterogeneous nature of multi-agent systems (MASs) addresses naturally thedynamism and scalability problems of smart cities and ambient intelligence [2]. High-level interactionprotocols and communications are a cornerstone of MASs, which are capable of establishing conversationby following these protocols, sending and receiving messages, or sharing vocabularies.However, how do we leverage information from an open MAS to provide citizens with intelligentservices? MAS platforms and frameworks usually allow developers to analyze agents’ mental statesand interactions among agents for testing, debugging, and verification purposes [3]. Regarding theSensors 2020, 20, 5198; doi:10.3390/s20185198 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/ORCID ID: 0000-0002-4392-4743http://www.mdpi.com/1424-8220/20/18/5198?type=check_update&version=1http://dx.doi.org/10.3390/s20185198http://www.mdpi.com/journal/sensorsSensors 2020, 20, 5198 2 of 19study of agents’ mental states, these tools tend to assume that the agents’ implementation is available.This is not the case in an open MAS for a smart city or large-scale ambient intelligence systems wherethe use of MASs is motivated, among others, because of agents’ capacity of migration from one toanother platform at run-time. Moreover, these agents can be designed and implemented by differentdevelopers and operators, which may want to use proprietary code. Concerning interaction analysis,these tools usually study fixed elements in messages and protocols such as the performative or thesender without analyzing the message semantics and content. This hinders smart city users, developers,and agents from performing a number of interesting tasks beyond using a black-box service suchas verifying or validating agents or analyzing their trustworthiness. We address these drawbacksand limitations in the specialized literature by leveraging semantic information in agents’ interactionprotocols to build mental state models and protocol models with machine learning methods.",
        "file_name": "pdf?version=1599893740",
        "file_path": "PDFs\\pdfversion1599893740.pdf"
    },
    {
        "title": "Potentially inappropriate medications in older adults living with HIV",
        "implementation_urls": [],
        "doi": "10.1111/hiv.12883",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1111%hiv!12883.pdf",
        "file_path": ".\\PDFs\\10!1111%hiv!12883.pdf"
    },
    {
        "title": "Mission possible: Unify HPC and Big Data stacks towards application-defined blobs at the storage layer",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2018.07.035",
        "arxiv": null,
        "abstract": "AbstractHPC and Big Data stacks are completely separated today. The storage layer offers opportunities for convergence,as the challenges associated with HPC and Big Data storage are similar: trading versatility for performance. Thismotivates a global move towards dropping file-based, POSIX-IO compliance systems. However, on HPC platformsthis is made difficult by the centralized storage architecture using file-based storage. In this paper we advocate that thegrowing trend of equipping HPC compute nodes with local storage redistributes the cards by enabling object storageto be deployed alongside the application on the compute nodes. Such integration of application and storage not onlyallows fine-grained configuration of the storage system, but also improves application portability across platforms. Inaddition, the single-user nature of such application-specific storage obviates the need for resource-consuming storagefeatures like permissions or file hierarchies offered by traditional file systems. In this article we propose and evaluateBlobs (Binary Large Objects) as an alternative to distributed file systems. We factually demonstrate that it offersdrop-in compatibility with a variety of existing applications while improving storage throughput by up to 28%.1. IntroductionHPC and Big Data platforms are carving new datastorage models. This is made necessary by the ever-increasing scale of the computation and of the datasetsingested and produced by large-scale applications. Thesuccess of key-value stores [1, 2] or block storagesystems [3, 4] on Clouds, and the advent of burstbuffers [5, 6] or advanced I/O libraries [7, 8] for HPCclearly highlight this need.At the heart of these different methods is the movefrom legacy POSIX-compliant storage systems towardssimple storage paradigms designed especially for onepurpose, trading versatility for performance. Indeed,Email addresses: pmatri@fi.upm.es (Pierre Matri),alforov@dkrz.de (Yevhen Alforov), abrandon@fi.upm.es(Álvaro Brandon), mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),gabriel.antoniu@inria.fr (Gabriel Antoniu),michael.kuhn@informatik.uni-hamburg.de (Michael Kuhn),carns@mcs.anl.gov (Philip Carns), ludwig@dkrz.de(Thomas Ludwig)POSIX-IO imposes functionality such as hierarchicalnamespaces or file permissions. While these featuresare often provided for convenience, they are in practicerarely needed by modern applications and can signifi-cantly hinder the storage performance. Indeed, the li-braries and frameworks commonly used to access thestorage on HPC [9] and Big Data platforms [10, 11] pro-vide relaxed semantics (i.e., the set of rules and guaran-tees provided by the system regarding the behavior of itsstorage operations) compared to those of the underlyingfile system.Yet, deploying new storage models on HPC platformsused to be hard or simply impossible. Indeed, paral-lel file systems such as Lustre or GPFS on HPC havebeen the cornerstone of HPC storage for decades and arelikely to remain so in the next few years. This is largelyexplained by the high level of versatility and support for",
        "file_name": "10!1016%j!future!2018!07!035.pdf",
        "file_path": ".\\PDFs\\10!1016%j!future!2018!07!035.pdf"
    },
    {
        "title": "Polypharmacy and Drug–Drug Interactions in People Living With Human Immunodeficiency Virus in the Region of Madrid, Spain: A Population-Based Study",
        "implementation_urls": [],
        "doi": "10.1093/cid/ciz811",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1093%cid%ciz811.pdf",
        "file_path": ".\\PDFs\\10!1093%cid%ciz811.pdf"
    },
    {
        "title": "Exploiting Declarative Mapping Rules for Generating GraphQL Servers with Morph-GraphQL",
        "implementation_urls": [
            {
                "identifier": "https://github.com/facebook/dataloader",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1142%s0218194020400070.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "ahttps://github.com/facebook/dataloader."
                    }
                ]
            }
        ],
        "doi": "10.1142/s0218194020400070",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1142%s0218194020400070.pdf",
        "file_path": ".\\PDFs\\10!1142%s0218194020400070.pdf"
    },
    {
        "title": "The genetic architecture of the human cerebral cortex",
        "implementation_urls": [],
        "doi": "10.1126/science.aay6690",
        "arxiv": null,
        "abstract": null,
        "file_name": "grant-acknowledgements.pdf",
        "file_path": "PDFs\\grant-acknowledgements.pdf"
    },
    {
        "title": "Development Experience of a Context-Aware System for Smart Irrigation Using CASO and IRRIG Ontologies",
        "implementation_urls": [],
        "doi": "10.3390/app10051803",
        "arxiv": null,
        "abstract": "Abstract: The rapid development of information and communication technologies and wirelesssensor networks has transformed agriculture practices. New tools and methods are used to supportfarmers in their activities. This paper presents a context-aware system that automates irrigationdecisions based on sensor measurements. Automatic irrigation overcomes the water shortageproblem, and automatic sensor measurements reduce the observational work of farmers. This paperfocuses on a method for developing context-aware systems using ontologies. Ontologies are usedto solve heterogeneity issues in sensor measurements. Their main goal is to propose a shared dataschema that precisely describes measurements to ease their interpretations. These descriptions arereusable by any machine and understandable by humans. The context-aware system also containsa decision support system based on a rules inference engine. We propose two new ontologies:The Context-Aware System Ontology addresses the development of the context-aware systemin general. The Irrigation ontology automates a manual irrigation method named IRRINOV®.These ontologies reuse well-known ontologies such as the Semantic Sensor Network (SSN) and SmartAppliance REFerence (SAREF). The decision support system uses a set of rules with ontologies toinfer daily irrigation decisions for farmers. This project uses real experimental data to evaluate theimplementation of the decision support system.Keywords: agriculture; smart irrigation; context-aware system; ontology; rules1. IntroductionIn the agricultural domain, farmers need to observe natural phenomena to engage in appropriateactivities on their fields. For example, in traditional irrigation, farmers go to their fields to examineAppl. Sci. 2020, 10, 1803; doi:10.3390/app10051803 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3517-0945https://orcid.org/0000-0002-3076-5499https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0003-3459-8568https://orcid.org/0000-0002-7011-4535http://dx.doi.org/10.3390/app10051803http://www.mdpi.com/journal/applsciAppl. Sci. 2020, 10, 1803 2 of 41the crop development stage and measure the soil moisture provided by probes in the soil. Then, theyuse practical experience or follow an irrigation method to estimate manually the water needs of theircrops. Based on their estimations, the farmers decide whether to irrigate the fields. This conventionalapproach has two significant drawbacks. First, irrigation requires daily observations, often made byfarmers. Manual observations are influenced by other factors such as the weather or the situation offarmers. For example, a daily observation could be skipped if the farmer is sick. Second, the resourceshortage problem demands that farmers use water sparingly.Context-aware systems (CASs) can overcome the above-mentioned situations. A CAS uses awireless sensor network (WSN) to monitor environmental phenomena and uses those measurementsfor further processes. In a CAS, context refers to “any information that can characterize the situation ofan entity. An entity could be a person, a place or an object that is considered relevant to the interactionbetween a user and an application, including the user and the application themselves” [1]. A CAShas two contexts: a low-level context and a high-level context [2]. The low-level context containsquantitative data. The high-level context contains qualitative data that synthesize a situation and easethe decision. For example, the statement “soil moisture is 160 centibar (cbar)” presents a low-levelcontext; however, the statement “soil is dry” presents a high-level context.The CAS has some peculiar characteristics. First, sensors in the system are heterogeneous. Eachtype of phenomenon demands a different type of sensor. For example, in agriculture, pluviometersmeasure rain quantity, and tensiometers measure soil moisture. Thus, the CAS should address",
        "file_name": "pdf?version=1583938924",
        "file_path": "PDFs\\pdfversion1583938924.pdf"
    },
    {
        "title": "The zaragoza's knowledge graph: Open data to harness the city knowledge",
        "implementation_urls": [
            {
                "identifier": "https://github.com/zaragoza-sedeelectronica/zaragoza-sedeelectronica.github.io",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdf.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of this visualization example is available on the Zaragoza’s GitHub repository (https://github.com/zaragoza-sedeelectronica/zaragoza-sedeelectronica.github.io/tree/master/sparql/ejemplos/monumentos.html)."
                    }
                ]
            }
        ],
        "doi": "10.3390/info11030129",
        "arxiv": null,
        "abstract": "Abstract: Public administrations handle large amounts of data in relation to their internal processesas well as to the services that they offer. Following public-sector information reuse regulations andworldwide open data publication trends, these administrations are increasingly publishing their dataas open data. However, open data are often released without agreed data models and in non-reusableformats, reducing interoperability and efficiency in data reuse. These aspects hinder interoperabilitywith other administrations and do not allow taking advantage of the associated knowledge in anefficient manner. This paper presents the continued work performed by the Zaragoza city councilover more than 15 years in order to generate its knowledge graph, which constitutes the key piece oftheir data management system, whose main strengthen is the open-data-by-default policy. The mainfunctionalities that have been developed for the internal and external exploitation of the city’s opendata are also presented. Finally, some city council experiences and lessons learned during this processare also explained.Keywords: public administration; linked open data; knowledge graph; interoperability; ontologies;open government data; vocabularies; API1. IntroductionPublic administrations are large data producers and collectors due to the need to provide supportfor their internal processes, as well as for services offered to citizens. These data are traditionallyavailable in several formats, according to their own information models, and are managed in different,sometimes isolated, data sources. From the point of view of the organization, the importance of datais undeniable. A proper organizational data management improves decision-making, operationalefficiency and service provision. From the side of citizens, the public administration’s data are anessential asset that must be made available in order to enhance transparency and the accountability [1].From the point of view of re-users, having these data available enables developing value-addedservices and applications, consequently stimulating innovation and economy [2]. Having all thesebenefits in mind, public administrations are increasingly publishing their data as Open Data, definedas data that can be freely used, re-used and redistributed by anyone—subject only, at most, to therequirement to attribute and sharealike [3]. An Open Government Data strategy ensures that dataare made available across city departments and to third parties, contributes to citizen engagement,increases democracy and serves to drive economic growth and social improvement [4,5].However, these data are mostly represented with information models defined without previousagreements or consensus processes with other institutions, which hinders the possibility of using andInformation 2020, 11, 129; doi:10.3390/info11030129 www.mdpi.com/journal/informationhttp://www.mdpi.com/journal/informationhttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0002-9260-0753http://dx.doi.org/10.3390/info11030129http://www.mdpi.com/journal/informationhttps://www.mdpi.com/2078-2489/11/3/129?type=check_update&version=2Information 2020, 11, 129 2 of 18sharing them within the same organization or with third parties. This lack of adoption of shared modelsis one of the major causes of inefficiency of information management within large organizations [6].In this regard, ontologies, defined as formal specifications of shared conceptualizations [7], have beenused to describe data without ambiguities. Hence, these may be good artifacts to facilitate reusability,interoperability and data quality assurance.When all these data are organized according to ontologies (also known as vocabularies in thecontext of Open Government Data) and represented in some graph-like format, we can talk abouta Knowledge Graph. Despite there being no consensus about a single definition for a KnowledgeGraph [8–11], in this work it is understood as a graph of data with the intent to compose knowledge [12].Knowledge Graphs have become a powerful resource in big industries, for example, Google’s",
        "file_name": "pdf",
        "file_path": "PDFs\\pdf.pdf"
    },
    {
        "title": "The Challenge: From MPEG Intellectual Property Rights Ontologies to Smart Contracts and Blockchains [Standards in a Nutshell]",
        "implementation_urls": [],
        "doi": "10.1109/msp.2019.2955207",
        "arxiv": null,
        "abstract": "the abstract creation, the work, which is the result of any intellectual endeavour with enough creativity. Works are pure, abstract entities, with no material incarnation whatsoever. Derivative works are a special type of works,  that have been derived from an existing work. Works are fixated into physical manifestations, which are the very first incarnation of works. Manifestations can be instanced, and copied, or they can be transformed into commercial products. Whereas the logical schema of IP entities resembles the Functional Requirements for Bibliographic Records (FRBR) chain [13], the source is somewhat different: MVCO catering for the needs of music and media stakeholders codifies the IP entities mentioned by copyright legislation (as defined by worldwide agreed treaties such as the Berne Convention), whereas FRBR is inspired by the needs of librarians.  A user is defined as an individual or organization, acting in the media value chain. The types of roles, a user  could undertake, revolve around the IP entities, e.g., a creator is defined as the user who creates a work, an adaptor is the user who adapts a work to produce an adaptation. These roles or very similar ones are also acknowledged by copyright legislation. Other roles include producer, distributor and, finally the end-user.  The types of actions that can be performed also revolve around the IP entities. Create work is the action whose result is a new work, produce is the action whose result is a product and so forth. In addition, some other actions do not produce any new IP entity, such as, a public communication or an end-user action (e.g., play and print) but they are legal concepts with explicit mentions and provisions in copyright legislation.  The relationship between a user and a particular IP entity type (e.g., work, adaptation, product, copy) is specified through the concept of role. The actions that a user performs on a given IP entity determine the role of that user with respect to the IP entity in question. Users get roles (e.g., creator, adaptor, producer, end-user) that attribute them rights over actions (e.g., create work, make adaptation, produce, distribute, synchronise) that can be exercised on specific IP entities. Any given user may undertake any number of roles within a given value chain. Figure 1 illustrates these relationships between actions, users and IP entities.  Fig. 1. MVCO defined relationships between actions, users and IP entities.  Authorisation Model The MVCO by defining the relationships between users, actions and IP entities serves well to depict a static picture of the IP information. However, in real life rights are transferable and this dynamic nature of rights was required to be supported in the MVCO.  Transfer of rights are born with the signature of agreements or contracts which grant permissions. A permission relates an IP entity with a right in transit between the original rights owner and the new rights owner. Permissions have an intrinsic dynamic nature: they are granted, invoked and revoked. Instances of a user class will probably be actual companies or persons; instances of works will be actual works. However, instances of permissions are far more interesting due to that they could refer either to the past or in the future. That is, an instance permission (e.g., Alice’s permission to play a song) would be related to both: an end-user instance (e.g., Alice) and an action instance (e.g., play a song). However, what is the interpretation of an action instance? It might be an action effectively executed in the past (e.g., Alice played a song), but it might also be  an action to be performed in the future, as a mere possibility (e.g., Alice can play a song). This is commonly referred in the literature as event factuality, and suggests that action instances can be marked as executed acts or as possible acts. Permissions can also be granted conditionally, that is, subject to certain conditions (facts). Facts can be seen as propositions with an alethic (e.g., true or false) value. These propositions can be combined with logical operators (e.g., conjunction and disjunction) to create more complex ",
        "file_name": "10!1109%msp!2019!2955207.pdf",
        "file_path": ".\\PDFs\\10!1109%msp!2019!2955207.pdf"
    },
    {
        "title": "EWOT: A semantic interoperability approach for heterogeneous IoT ecosystems based on the web of things",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/eWoT",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1580812003.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Availability: the implementation of eWoT is publicly available at https://github.com/oeg-upm/eWoT."
                    }
                ]
            }
        ],
        "doi": "10.3390/s20030822",
        "arxiv": null,
        "abstract": "Abstract: With the constant growth of Internet of Things (IoT) ecosystems, allowing them tointeract transparently has become a major issue for both the research and the software developmentcommunities. In this paper we propose a novel approach that builds semantically interoperableecosystems of IoT devices. The approach provides a SPARQL query-based mechanism to transparentlydiscover and access IoT devices that publish heterogeneous data. The approach was evaluated inorder to prove that it provides complete and correct answers without affecting the response time andthat it scales linearly in large ecosystems.Keywords: semantic interoperability; IoT devices; context-based search; content-based search1. IntroductionIn the last decade, the IoT devices available through the Web have become pervasive [1],allowing users and applications to easily monitor and interact with their devices [2]. Far fromreaching a scenario in which the number of IoT devices will stop growing, researchers foresee aconstant growth in the number of such devices available through the Web in the mid-term future [3,4].The different IoT devices are developed and distributed by different vendors [5]; as a result, these IoTdevices rely on a wide number of heterogeneous models, data formats, and APIs [6].Semantic interoperability aims at dealing with IoT device heterogeneity [7], by enablingan environment in which IoT devices can be included, forming an IoT ecosystem. The ecosystem mustenable transparent mechanisms through which such devices are discoverable [8]. Discovery shouldenable context- and content-based searching (i.e., the discovery of sensors by some meta-data(context-based) or by values that devices are capturing (content-based)). For this purpose,W3C standards have become widely adopted [9], specifically SPARQL [10], to transparently interactwith IoT ecosystems, and ontologies are used to describe the meta-data of the IoT devices. In particular,the Thing Description (TD) model defined by the W3C Web of Things working group [11] is one of themost adopted to profile IoT devices and developing discovery mechanisms [12].A thorough analysis of the current proposals that address semantic interoperability for IoTecosystems was presented by Zout et al. [9], involving approximately 50 proposals. There are mainlytwo approaches for Web-available IoT devices based on the W3C standards that perform both context-and content-based searches: federation and centralised approaches. The former requires each IoTdevice in the ecosystem to enable a SPARQL endpoint, so when a query is issued it is federatedto all the devices in the ecosystem [13]. The latter consists of storing all the sensors’ meta-data ina central triple store, and it requires the IoT devices to push their fresher data into the triple store.Sensors 2020, 20, 822; doi:10.3390/s20030822 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-1823-4484https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttp://www.mdpi.com/1424-8220/20/3/822?type=check_update&version=1http://dx.doi.org/10.3390/s20030822http://www.mdpi.com/journal/sensorsSensors 2020, 20, 822 2 of 19The federation approaches do not deal with the IoT heterogeneity; they rely on the restriction that IoTdevices manage SPARQL queries, which considering the heterogeneity nature of such devices is notlikely to occur. The centralised approaches deal with the heterogeneity of the devices by requiringthem to periodically send data, which compromises the freshness of the information and requirespractitioners to implement synchronization mechanisms.In this article, we introduce a novel approach called eWoT that allows transparent interactionby means of SPARQL queries with an IoT ecosystem that is formed by Web-available IoT devices.The approach relies on Thing Descriptions (TDs) to profile the different IoT devices [11]. Unfortunately,these TDs are not expressive enough to deal with the heterogeneity of the IoT devices [14,15]. For this",
        "file_name": "pdf?version=1580812003",
        "file_path": "PDFs\\pdfversion1580812003.pdf"
    },
    {
        "title": "Morph-CSV: Virtual knowledge graph access for tabular data",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Virtual knowledge graph access has traditionally focused onproviding ontology-based access to relational databases (RDB) propos-ing SPARQL-to-SQL query translation techniques and optimizations.With the advent of mapping languages or annotations such as RML orCSVW, these techniques have been applied over tabular data by con-sidering each source as a single table that can be loaded into an RDB.However, such techniques do not take into account those characteristicsthat are normally present in real-world CSV files (e.g., normalization,constraints, joins). In this paper we present Morph-CSV, a frameworkfor enhancing virtual knowledge graph access over a set of CSV files byusing a combination of CSVW annotations and RML mappings withFnO transformation functions. Exploiting these inputs, the frameworkcreates an enriched RDB representation of the CSV files together withthe corresponding R2RML mappings, enabling the use of existing querytranslation (SPARQL-to-SQL) techniques and tools.Keywords: Knowledge Graphs · CSV · RML · CSVW1 IntroductionSemi-structured data formats, and particularly spreadsheets in the form of CSVor Excel files, are one of the most widely-used formats to publish data on theWeb. There are several reasons why tabular formats are so popular for datapublication. First, they are easy to generate by data providers. In many cases,they are even used as one of the main ways to manage data inside organiza-tions. Second, they are easy to consume with common office tools (e.g., Excel,LibreOffice) and there are advanced tools that can be used to process them (e.g.,OpenRefine, Tableau). However, more advanced consumers (e.g., application de-velopers, knowledge workers) often have to face some relevant challenges whenconsuming tabular data: there is no standard way to query data in them as itcan be done with other types of data formats, such as RDB, JSON or XML; dataCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).Chaves-Fraga et al.are difficult to integrate since data constraints and relationships across differentfiles are not explicit; data are often difficult to understand since column namesare generally heterogeneous.Some of these challenges may be dealt following a Semantic Web approach.Virtual knowledge graphs (VKG) provide a unified view and common access toa set of data sources based on ontologies and mappings, translating SPARQLqueries into queries that are supported by the underlying source. Although cur-rent proposals [7] provide support for querying this kind of formats, they treateach source as if it was a single not-normalized RDB table with no keys orintegrity constraints, important elements that are used by SPARQL-to-SQL en-gines for efficient querying. Several languages have been proposed to specifyannotations to deal with the heterogeneity of tabular datasets such as CSVW[8] metadata and RML+FnO [5] mapping rules, but engines or systems have totake them into account in their VKG access pipeline.In this demo we present Morph-CSV, an open source engine1 that extendsthe typical VKG workflow to enhance performance and query completeness overtabular datasets. Our approach exploits the information from CSVW anno-tations and RML+FnO mappings so as to obtain details on the underlyingschema, required transformation functions, missing information, etc., pushing",
        "file_name": "paper478.pdf",
        "file_path": "PDFs\\paper478.pdf"
    },
    {
        "title": "Mapeathor: Simplifying the specification of declarative rules for knowledge graph construction",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. In recent years we have observed an increasing interest bythe scientific community, from social sciences to biomedicine, in the gen-eration and publication of RDF-based knowledge graphs. One possibilityfor creating knowledge graphs consists in using declarative mappings to-gether with their associated parsers. These mappings describe the rela-tionship between the source data and a reference ontology. However, thelearning curve to create these mapping files is steep, hindering its use bya wider community. In this paper we present a user-friendly mapping-language-independent tool, Mapeathor, to declare transformation rulesbased on spreadsheets and translate them into two different mappinglanguages with the purpose of easing the mappings creation process.Keywords: Knowledge Graph · Declarative mapping · Spreadsheet1 IntroductionIn the last few decades, we have seen a significant increase in the publicationof data in a machine understandable manner following Linked Data principles1(e.g., DBpedia2, Wikidata3). Knowledge Graph construction requires integratingdifferent data sources in a structured way, usually following the schema of anontology or group of ontologies. This facilitates the posterior task of mining theknowledge graph with several applications, such as searching recommendationsand learning implicit data patterns.Knowledge graphs can be built in diverse ways. One option is creating ad-hocscripts to transform data, which requires the user to repeat the process of scriptCopyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0).1 https://5stardata.info/en/2 https://wiki.dbpedia.org/3 https://www.wikidata.org/A. Iglesias-Molina et al.writing in every specific use case. Another option is using tools like OpenRe-fine4 to perform data transformation through the creation of an RDF skeleton,which includes proprietary transformation rules and a functionality for knowl-edge graph construction. Lastly, there is an option to keep the transformationrules in specific files that can be later processed by engines that either transformthe data to RDF or create a virtual knowledge graph that can be queried with-out transforming the source data. These rules can be written in a wide variety oflanguages (e.g., R2RML [2], RML [3]) that cover different user’s needs (e.g., thesource data format or the engine that will be used). Although the use of thesemapping files is more flexible and independent, since they can be processed bya wide variety of engines, their creation is still not easy for new users. Expertsare usually needed to carry out these tasks, hindering the use of semantic webtechnologies across the scientific community. That is why it is necessary to lowerthe learning curve and improve mapping reuse and reproducibility.Since mapping languages started to be used by the community, there havebeen multiple approaches for the development of editors to ease their specifica-tion. Most of them enable editing through graphical visualization [4, 6], othersprovide a writing environment (e.g. the Protégé extension OntopPro). Theseeditors are language-oriented, they help to create one kind of mapping, not tak-ing into account the wide variety of mapping languages that currently exist.Moreover, when managing a considerable amount of mapping rules, a graphicalapproach may not be easily handled.",
        "file_name": "paper488.pdf",
        "file_path": "PDFs\\paper488.pdf"
    },
    {
        "title": "Simulation of Dissemination Strategies on Temporal Networks",
        "implementation_urls": [],
        "doi": "10.23919/annsim52504.2021.9552126",
        "arxiv": "2107.06771",
        "abstract": "AbstractIn distributed environments, such as distributed ledgers technologies and other peer-to-peer architectures,communication represents a crucial topic. The ability to efficiently disseminate contents is strongly influ-enced by the type of system architecture, the protocol used to spread such contents over the network andthe actual dynamicity of the communication links (i.e. static vs. temporal nets). In particular, the dissemi-nation strategies either focus on achieving an optimal coverage, minimizing the network traffic or providingassurances on anonymity (that is a fundamental requirement of many cryptocurrencies). In this work, thebehaviour of multiple dissemination protocols is discussed and studied through simulation. The perfor-mance evaluation has been carried out on temporal networks with the help of LUNES-temporal, a discreteevent simulator that allows to test algorithms running on a distributed environment. The experiments showthat some gossip protocols allow to either save a considerable number of messages or to provide betteranonymity guarantees, at the cost of a little lower coverage achieved and/or a little increase of the deliverytime.Keywords: temporal networks, simulation, P2P, gossip protocols.1 INTRODUCTIONNowadays, the ubiquitous cloud computing paradigm implies that most of the applications running on theInternet follow a centralized client-server approach. This means that all the resources of the applicationare situated in some servers, and the users of the system need to contact such servers in order to retrievethe information. An alternative to this scheme is to use a decentralized approach, where the data and com-putation resources are distributed among the various nodes and the central servers, if present, only have acoordination role. So, it is possible to design systems whose architecture is decentralized and where all thenodes, often referred as peers, share the workload without privileges, a hierarchy and central entities beinginvolved, i.e. Peer-to-Peer (P2P).Normally, P2P systems make use of an overlay network, meaning that an application level communicationnetwork is created, running on top of an already existing network infrastructure (i.e. the Internet), oftenregardless of the real geographical distribution of involved nodes. In such a case, this scheme might leadSerena, Zichichi, D’Angelo, and Ferrettito a lot of traffic overhead to keep the network up and running (Backx et al. 2002). However, it has beendemonstrated that it is possible to considerably reduce the network traffic by using a smart approach topropagate the information on the network (D’Angelo and Ferretti 2017). Often there has been little interestin traffic minimization in distributed environments, because it may not be crucial for the functioning of thesystem. Thus, usually peers relay the new data that they receive to all their neighbors (except the one fromwhich they received the message), and the only concern is to avoid infinite loops of messages. However,for certain applications, traffic minimization can be a relevant issue, and significant improvements can beachieved without compromising the efficiency of the communication. Several algorithms to spread the mes-sages among the peers exist and, depending on the features of the system, certain protocols (and certainprotocols’ parameters) may turn out to be more appropriate than others.Simulation is a useful methodology in order to investigate which protocols are more suitable for the variouspurposes. By generating a virtual environment where multiple nodes communicate through the use of mes-sages, it is possible to analyze the behaviour of the different algorithms and to evaluate the overall efficiencywith the help of some performance metrics. Generally, it is desirable to achieve a very high if not completecoverage (i.e. the percentage of peers that receive a message), and to minimize the network traffic and thedelivery time (i.e. the time between the creation of a message and its delivery). However, no algorithm canmaximize all these features simultaneously, so it is necessary to find a balanced trade off, taking into accountwhich trait is more important for the specific application. For example, blockchains usually do not requireall the nodes to suddenly receive all the blocks and transactions, because some information, if missing, canbe retrieved. Therefore, in this scenario a protocol that focuses on anonymity and traffic minimization mightbe convenient, but without totally neglecting the delivery time, that if too large could lead the network toreach an inconsistent state. The output of the metrics can be influenced by many factors other than the gossipprotocol, like the connectivity of the network or the topology of the overlay. Another factor to consider is",
        "file_name": "10!23919%annsim52504!2021!9552126.pdf",
        "file_path": ".\\PDFs\\10!23919%annsim52504!2021!9552126.pdf"
    },
    {
        "title": "LYNX: Towards a Legal Knowledge Graph for Multilingual Europe",
        "implementation_urls": [],
        "doi": "10.26826/law-in-context.v37i1.129",
        "arxiv": null,
        "abstract": "ABSTRACTLynx is an innovation project in Europe whose objective is to develop services for legal compliance. A legal knowledge graph is built over multilingual, multijurisdictional documents using semantic web technologies. A collection of ser-vices implementing natural language techniques enables better legal information retrieval, cross-lingual answering of questions and information discovery. Three use cases are discussed, as well as the overall impact of the project.Keywords – Lynx Legal Knowledge Graph, compliance services, European legislation, multilingualism  Acknowledgements: This work has been funded by the project Lynx, which has received funding from the European Union’s Horizon 2020 research and innovation program under grant agreement no. 780602. For more information: http://www.lynx-project.eu. Disclosure statement – No potential conflict of interest was reported by the authors.License – This work is under Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) https://creativecommons.org/licenses/by-nc-sa/4.0/ Suggested citation: Rodríguez-Doncel, V. and Montiel-Ponsoda, E. 2021. “Lynx:Towards a Legal Knowledge Graph for Multilingual Europe”, Law in Context, 37(1): 175-178, DOI: https://doi.org/10.26826/law-in-context.v37i1.129Summary1. Introduction2. Results of Lynx project2.1 A multilingual legal knowledge graph2.2 Software2.3 Demonstration in diverse scenarios3. Impact of the project4. Conclusions5. Referenceshttp://journals.latrobe.edu.au/index.php/law-in-context/indexhttps://doi.org/10.26826/law-in-context.v37i1.129\rhttps://orcid.org/0000https://orcid.org/0000http://www.lynx-project.euhttps://creativecommons.org/licenses/by-nc-sa/4.0/https://doi.org/10.26826/law-in-context.v37i1.129\rLaw in Context, Vol 37, Issue 1, 2020 176ISSN: 1839-41831. INTRODUCTION The European Union (EU), post-Brexit, is comprised of 27 member states populated by approximately 450 million people, who speak over 60 indigenous languages with different legal status. The EU has adopted 24 official languages, and every EU national has the right to use any of these 24 languages to contact the EU institutions, and institutions are obliged to reply in the same language. Although the European Commission favors three of these languages as ‘procedural languages’ (English, French and German), EU law and many other legislative documents are published in all official languages except Irish. How-ever, member states do not translate their legislation into foreign languages and finding the applicable norms in any location in one’s own language is not easy.The double fragmentation (of Law and languages) hampers the development of a unified market with fluid commercial exchanges in Europe. This has been recognized by the EU authorities, who set the completion of the Digital Single Market as one of their 10 political priorities for the ",
        "file_name": "200",
        "file_path": "PDFs\\200.pdf"
    },
    {
        "title": "OpenADR Ontology: Semantic Enrichment of Demand Response Strategies in Smart Grids",
        "implementation_urls": [
            {
                "identifier": "https://github.com/albaizq/OpenADRontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%sest48500!2020!9203093.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "SHACL shapes for the proposed OpenADR ontology can be found in our Github repository23."
                    }
                ]
            }
        ],
        "doi": "10.1109/sest48500.2020.9203093",
        "arxiv": null,
        "abstract": "Abstract—Demand Response (DR) gains increasing attention asa core building block of smart grids. Advanced ICT systems havebeen made available in the last decades and have been employedalready in commercial energy markets. As more and morehardware and software solutions are flooding the market, theneed for interoperability among systems has become a necessity.Building upon OpenADR, a well-known standard for DR, thiswork presents its semantic enrichment towards transforming itinto an ontology (publicly available), which ultimately enablessemantic interoperability among various DR stakeholders andsystems and other semantic-related features like data validation,reusing terms and integration with other standard ontologies.Following the Linked Open Terms methodology, a detaileddescription of the main OpenADR services is presented, encodedin OWL, along with needed extensions that derive from otherwell-known ontologies. By introducing an OpenADR ontology,the adoption and deployment of OpenADR in both researchand industrial implementations is expected to expand, ultimatelypromoting significantly semantic interoperability in DR systems.Keywords—OpenADR, ontology, demand response, smart grid,semantic interoperabilityI. INTRODUCTIONA. MotivationDemand Response (DR) is already part of the energy marketin multiple European countries and in many more there are fu-ture plans drafted to accommodate this relatively new businessmodel [1]. An increasing number of equipment, services, roles,as well as information exchange have made their appearancein every day energy and financial transactions. Thus, althoughenabling the participation of customers in DR schemes isbecoming a commodity, besides opportunities, it also givesbirth to new challenges, one of which is interoperability [2].The EU Energy Efficiency Directive establishes the require-ments and technical modalities for the respective stakeholdersamong member states towards facilitating the uptake of DRservices [3]. Nevertheless, actual deployment of DR has anincreasing number of variations in response to different energymarket products and regulations amongst member states. Thisvariety of DR programs, where each has its own requirements,highlights the need for clearly defined standards to communi-cate DR program and market information [4]. Thus, multiplestandards have been proposed to describe this domain.The Energy Market Information Exchange (EMIX1) is astandard for price and product definition. The Universal SmartEnergy Framework (USEF [5]) describes the market for flexi-bility, enabling commoditization and market trading of flexibleenergy use. The Smart Grid Architecture Model (SGAM [6])aims to present the design of smart grid use cases from anarchitectural viewpoint. It consists of five layers represent-ing business objectives and processes, functions, information",
        "file_name": "10!1109%sest48500!2020!9203093.pdf",
        "file_path": ".\\PDFs\\10!1109%sest48500!2020!9203093.pdf"
    },
    {
        "title": "Editorial: Special issue on Semantic eScience: Methods, tools and applications",
        "implementation_urls": [],
        "doi": "10.3233/sw-200380",
        "arxiv": null,
        "abstract": "Abstract. Openly shared, available, and accessible scientific resources facilitate tackling grand challenges that our society,organizations, communities and individuals are facing today. Pandemics, climate change, environmental modeling, genomics,or space exploration all create open research questions for which Artificial Intelligence – and Semantic Web in particular –have a unique potential to accelerate scientific discoveries. In this editorial, we introduce a special issue on Semantic eSciencemethods, tools and applications for the Semantic Web Journal and outline five challenges for Semantic eScience in the years tocome.1. IntroductionAn increasing number of datasets, software toolsand methods reported in scientific publications areshared today to support scientific results and ease theirreproducibility. Open initiatives such as Zenodo1 orthe European Open Science Cloud2 contribute signif-icantly to finding and accessing these scientific re-sources. However, reusing and understanding datasets,software and methods is still challenging given theeffort required to address interoperability issues; andcapture constraints, assumptions, limitations, exam-ples and internal variables associated with data andcode.Computational experiments are becoming morecomplex due to the heterogeneity of models involved,the amount and type of data required, computing re-sources needed, and interdisciplinary collaborations.1https://zenodo.org/2https://www.eosc-portal.eu/about/eoscArtificial Intelligence and Semantic Web technologiescan uniquely contribute to address these challengesby providing means to create meaningful and well-defined descriptions for scientific resources and en-abling the automation of tedious tasks that are cur-rently performed manually.Making scientific resources accessible, semanticallydescribed, and interlinked would create an invalu-able network for scientific research, increasing trans-parency of Science, enabling reproducibility and as-sisting researchers in communicating scientific out-comes to current and future generations. Researcherswould be able to build on previous scientific experi-ments and its results more efficiently; and focus theirefforts in tackling new challenges instead of manipu-lating data in the right format for their analysis. Edu-cators may leverage semantically annotated resourcesfor new students to easily access, understand and reuseexperiments openly available worldwide.The goal of this special issue is to emphasize thesebenefits by collecting the latest research solutions tobridge the gap between existing scientific communica-1570-0844/20/$35.00 © 2020 – IOS Press and the authors. All rights reservedmailto:dgarijo@isi.edumailto:nvillanuevarosales@utep.edu",
        "file_name": "10!3233%sw-200380.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-200380.pdf"
    },
    {
        "title": "FAIR Computational Workflows",
        "implementation_urls": [],
        "doi": "10.1162/dint_a_00033",
        "arxiv": null,
        "abstract": "ABSTRACTComputational workflows describe the complex multi-step methods that are used for data collection, data preparation, analytics, predictive modelling, and simulation that lead to new data products. They can inherently contribute to the FAIR data principles: by processing data according to established metadata; by creating metadata themselves during the processing of data; and by tracking and recording data provenance. These properties aid data quality assessment and contribute to secondary data usage. Moreover, workflows are digital objects in their own right. This paper argues that FAIR principles for workflows need to address their specific nature in terms of their composition of executable software steps, their provenance, and their development.† Corresponding author: Carole Goble (E-mail: carole.goble@manchester.ac.uk, ORCID: 0000-0003-1219-2137).© 2019 Chinese Academy of Sciences Published under a Creative Commons Attribution 4.0 International (CC BY 4.0) licenseDownloaded from http://direct.mit.edu/dint/article-pdf/2/1-2/108/1893377/dint_a_00033.pdf by École polytechnique user on 18 January 2024http://dx.doi.org/10.1162/dint_a_00051http://dx.doi.org/10.1162/dint_a_00030https://w3id.org/fair/principles/terms/F1https://w3id.org/fair/principles/terms/F2https://w3id.org/fair/principles/terms/F3https://w3id.org/fair/principles/terms/F4https://w3id.org/fair/principles/terms/A1https://w3id.org/fair/principles/terms/A1.1https://w3id.org/fair/principles/terms/A1.2https://w3id.org/fair/principles/terms/A2https://w3id.org/fair/principles/terms/I1https://w3id.org/fair/principles/terms/I2https://w3id.org/fair/principles/terms/I3https://w3id.org/fair/principles/terms/R1https://w3id.org/fair/principles/terms/R1.1https://w3id.org/fair/principles/terms/R1.2https://w3id.org/fair/principles/terms/R1.3http://crossmark.crossref.org/dialog/?doi=10.1162/dint_a_00033&domain=pdf&date_stamp=2020-01-31Data Intelligence 109FAIR Computational Workflows1. INTRODUCTIONIn data intensive science, e-infrastructures and software tool-chains are heavily used to help scientists manage, analyze, and share increasing volumes of complex data [1]. Data processing tasks like data cleansing, normalisation and knowledge extraction need to be automated stepwise in order to foster performance, standardisation and re-usability. Increasingly complex data computations and parameter-driven simulations need reliable e-infrastructures and consistent reporting to enable systematic comparisons of alternative setups [2, 3]. As a response to these needs, the practice of performing computational processes using workflows has taken hold in different domains such as the life sciences [4, 5, 6], biodiversity [7], astronomy [8], geosciences [9], and social sciences [10]. Workflows also support the adoption of novel computational approaches, notably machine learning methods [11], due to the ease with which single components in a processing pipeline can be exchanged or updated.Generally speaking, a workflow is a precise description of a procedure – a multi-step process to coordinate multiple tasks and their data dependencies. In computational workflows each task represents the execution of a computational process, such as: running a code, the invocation of a service, the calling of a command ",
        "file_name": "10!1162%dint_a_00033.pdf",
        "file_path": ".\\PDFs\\10!1162%dint_a_00033.pdf"
    },
    {
        "title": "A template-based approach for annotating long-tail datasets",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. An increasing amount of data is shared on the Web throughheterogeneous spreadsheets and CSV files. In order to homogenize andquery these data, the scientific community has developed Extract, Trans-form and Load (ETL) tools and services that help making these files ma-chine readable in Knowledge Graphs (KGs). However, tabular data maybe complex; and the level of expertise required by existing ETL toolsmakes it difficult for users to describe their own data. In this paper wepropose a simple annotation schema to guide users when transformingcomplex tables into KGs. We have implemented our approach by extend-ing T2WML, a table annotation tool designed to help users annotatetheir data and upload the results to a public KG. We have evaluated oureffort with six non-expert users, obtaining promising preliminary results.Keywords: Dataset annotation · Metadata · Knowledge Graph.1 IntroductionAn increasing amount of data is shared on the Web by multiple organizationsusing Excel and CSV formats. Content creators usually prefer to use tabulardata because it is simple to generate, manipulate and visualize by humans; andthere is a significant number of tools to help explore and edit the contents ofspreadsheets. These data need to be properly understood by others, and hencedocumentation (e.g., variables captured, provenance, usage notes, etc.) is usuallyincluded in auxiliary files or the spreadsheets themselves. As a result, many ofthese spreadsheets have comments, clarifications, notes and references to otherfiles explaining how to interpret the information contained in them.In order to convert tabular data to a machine readable format, the SemanticWeb community has created Extract, Transform and Load (ETL) tools (e.g.,[4]) and mapping languages (e.g., [1, 5]) that help translating spreadsheets intoKnowledge Graphs. However, these tools and languages require significant exper-tise when transforming heterogeneous tabular data with comments, incompletevalues or columns that are interrelated to each other, making it difficult fordomain experts to integrate their own datasets with existing KGs.? Copyright c© 2020 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 Garijo et al.In this paper we describe an approach to help non-experts transform theirdata into a structured representation through dataset annotations. Our contri-butions include 1) a dataset annotation schema that helps generating templatesfor translating datasets into KGs; 2) an extension of the T2WML dataset an-notation tool [6] to accommodate the proposed schema; and 3) an approach toupload annotated datasets to a registry once the dataset annotation is complete.In order to assess our approach, we conducted a preliminary evaluation with 6users unfamiliar with Knowledge Representation or Semantic Web technologies,who were able to describe and integrate their annotated datasets as a KG.2 Challenges in Long-Tail Dataset AnnotationWe focus on those datasets that are not straightforward to map into a structuredrepresentation. Consider for example Table 1, which depicts the food prices indifferent regions of Ethiopia at different points in time. The table has a time seriesfor the price value of different items at different dates, a repeated column withthe item being described (ignore), the item category and different informationabout the region where that item was produced. The dataset has also somemissing values and labels marked as ”unknown”, which we may want to skip.",
        "file_name": "profiles2020-paper-3.pdf",
        "file_path": "PDFs\\profiles2020-paper-3.pdf"
    },
    {
        "title": "Events Matter: Extraction of Events from Court Decisions",
        "implementation_urls": [],
        "doi": "10.3233/faia200847",
        "arxiv": null,
        "abstract": "Abstract. The analysis of court decisions and associated events is part of the dailylife of many legal practitioners. Unfortunately, since court decision texts can oftenbe long and complex, bringing all events relating to a case in order, to understandtheir connections and durations is a time-consuming task. Automated court decisiontimeline generation could provide a visual overview of what happened throughouta case by representing the main legal events, together with relevant temporal infor-mation. Tools and technologies to extract events from court decisions however arestill underdeveloped. To this end, in the current paper we compare the effectivenessof three different extraction mechanisms, namely deep learning, conditional randomfields, and rule-based method, to facilitate automated extraction of events and theircomponents (i.e., the event type, who was involved, and when it happened). In addi-tion, we provide a corpus of manually annotated decisions of the European Court ofHuman Rights, which shall serve as a gold standard not only for our own evaluation,but also for the research community for comparison and further experiments.Keywords. event extraction, named entity recognition, court decisions1. IntroductionCourt decisions are an important source of law information for legal practitioners: theyelaborate on the facts of a case, involved parties, interpretations of the circumstances,the applicable law and legal principles, and finally the legal assessment leading to thedecision. Legal professionals constantly extract, interpret and reason with and about priorcases whilst arguing for a decision in a current, undecided case. However, court decisionstexts can be long and complex and thus time-consuming to read. Therefore it would bebeneficial to find a means to provide a quick overview of a case, thereby helping to turndecisions into operational, consumable and actionable legal knowledge.In this work we focus specifically on using Natural Language Processing (NLP) tech-niques to automatically extract the essence of a court case. Besides extracting generallegal rules from individual cases, we aim at providing a quick overview of what hap-pened, who was involved and when the event took place. In the terminology of NLP, eventextraction can be treated as a text classification task aiming at assigning text fragments(typically, paragraphs, sentences or smaller parts of documents) to predefined (event)Legal Knowledge and Information SystemsS. Villata et al. (Eds.)© 2020 The Authors, Faculty of Law, Masaryk University and IOS Press.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA20084733classes [1]. Another, related NLP task is Named Entity Recognition (NER) which ex-tracts entities referred to in texts and classifies them into categories [2], for instance peo-ple, places and organizations; moreover, named entities can also be domain-specific, forinstance, courts or laws. Event extraction can benefit from NER, since it can be used toenrich events with relevant information, such as the parties involved. This paper focuseson the extraction of events and their components from court decisions of the EuropeanCourt of Human Rights (ECHR)1 based on a sample thereof.Summarizing our contributions, we: (i) provide a corpus of manually annotatedECHR decisions; (ii) perform a comparison of different approaches to automatically ex-tract events and their components – implementations as well as our evaluation results aremade available on GitHub; and (iii) introduce a prototypical web interface that can beused to display court decisions along with their extracted timelines.The remainder of this paper is structured as follows. We outline related works in Sec-",
        "file_name": "10!3233%faia200847.pdf",
        "file_path": ".\\PDFs\\10!3233%faia200847.pdf"
    },
    {
        "title": "On the Efficiency of Decentralized File Storage for Personal Information Management Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/miker83z/testingIPFS",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%iscc50000!2020!9219623.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://github.com/miker83z/testingIPFS"
                    }
                ]
            }
        ],
        "doi": "10.1109/iscc50000.2020.9219623",
        "arxiv": "2007.03505",
        "abstract": "Abstract—This paper presents an architecture, based on Dis-tributed Ledger Technologies (DLTs) and Decentralized File Stor-age (DFS) systems, to support the use of Personal InformationManagement Systems (PIMS). DLT and DFS are used to managedata sensed by mobile users equipped with devices with sensingcapability. DLTs guarantee the immutability, traceability andverifiability of references to personal data, that are stored inDFS. In fact, the inclusion of data digests in the DLT makes itpossible to obtain an unalterable reference and a tamper-prooflog, while remaining compliant with the regulations on personaldata, i.e. GDPR. We provide an experimental evaluation on thefeasibility of the use of DFS. Three different scenarios have beenstudied: i) a proprietary IPFS approach with a dedicated nodeinterfacing with the data producers, ii) a public IPFS service andiii) Sia Skynet. Results show that through proper configurationof the system infrastructure, it is viable to build a decentralizedPersonal Data Storage (PDS).Index Terms—Personal Information Management System, Dis-tributed Ledger Technologies, Decentralized File Storage, Sensingas a Service.I. INTRODUCTIONThe advent of social media and Web 2.0 favoured a processthat broke the boundaries between authorship and readership:users produce the data that is consumed by other users.This has increased the privacy threats of applications thatare shaped by user-generated content, as it often consists ofhighly personal data. In general, the economics of personalinformation is helped by the more pervasive nature of to-day’s digital world. This information enables organizations toprovide personalized or more useful services in digital andphysical spaces, but it could also have potentially harmfulconsequences for the privacy and autonomy of users andsociety at large. Current platform-centered data managementtechniques threaten the control that individuals exercise overtheir personal information and give to few companies thepower to necessarily rely on them to explore, filter and obtain∗This work has received funding from the European Union’s Horizon2020 research and innovation programme under the Marie Skłodowska-CurieITN EJD grant agreement No 814177 Law, Science and Technology JointDoctorate - Rights of Internet of Everythingdata of interest. Not mentioning the fact that some of thesecentral entities can operate without any transparency on theuse of users’ data.An individual digital counterpart can be depicted not onlyby using his own personal information, but also that of hissocial links (e.g. friends, family, colleagues) . Thus, it becomeseasier to understand users activity choice and lifestyle patterns[1] and to make more intrusive recommendations using thisdata [2], [3]. Lack of privacy control, for instance, leads anindividual being thrown into a “filter bubble” that can affect",
        "file_name": "10!1109%iscc50000!2020!9219623.pdf",
        "file_path": ".\\PDFs\\10!1109%iscc50000!2020!9219623.pdf"
    },
    {
        "title": "FunMap: Efficient Execution of Functional Mappings for Knowledge Graph Creation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/SDM-TIB/FunMap",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-62419-4_16.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "We aim to answer the following research questions: Q1) What is the impact of data duplication rate in the execution time of a knowledge graph creation approach? Q2) What is the impact of different types of complexity over transformation functions during a knowledge graph creation process? Q3) How does the repe-tition of a same function in different mappings affect the existing RML engines? Q4) What is the impact of relational data sources in the knowledge graph cre-ation process? All the resources used to perform this evaluation are available in our Github repository6."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-62419-4_16",
        "arxiv": "2008.13482",
        "abstract": "Abstract. Data has exponentially grown in the last years, and knowl-edge graphs constitute powerful formalisms to integrate a myriad of ex-isting data sources. Transformation functions – specified with function-based mapping languages like FunUL and RML+FnO – can be appliedto overcome interoperability issues across heterogeneous data sources.However, the absence of engines to efficiently execute these mappinglanguages hinders their global adoption. We propose FunMap, an inter-preter of function-based mapping languages; it relies on a set of loss-less rewriting rules to push down and materialize the execution of func-tions in initial steps of knowledge graph creation. Although applicable toany function-based mapping language that supports joins between map-ping rules, FunMap feasibility is shown on RML+FnO. FunMap reducesdata redundancy, e.g., duplicates and unused attributes, and convertsRML+FnO mappings into a set of equivalent rules executable on RML-compliant engines. We evaluate FunMap performance over real-worldtestbeds from the biomedical domain. The results indicate that FunMapreduces the execution time of RML-compliant engines by up to a factorof 18, furnishing, thus, a scalable solution for knowledge graph creation.Keywords: Knowledge Graph Creation · Mapping Rules · Functions1 IntroductionKnowledge graphs (KGs) have gained momentum due to the explosion of avail-able data and the demand for expressive formalisms to integrate factual knowl-edge spread across various data sources [14]. KG creation requires the descriptionof schema alignments among data sources and an ontology, as well as the specifi-cation of methods to curate and transform data collected from the input sourcesinto a unified format. A rich spectrum of mapping languages has been proposedto specify schema-ontology alignments across data sources implemented in avariety of semi-structured and structured formats; exemplar approaches includearXiv:2008.13482v2  [cs.DB]  5 Oct 20202 Jozashoori et al.",
        "file_name": "10!1007%978-3-030-62419-4_16.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-62419-4_16.pdf"
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "arxiv": null,
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activitieswhen browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead tounexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniquesthat uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extendingthose queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations onboth scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document Similarity, Information Search and Retrieval, Clustering, Topic Models, Hashing1. IntroductionHuge amounts of documents are publicly availableon the Web offering the possibility of extracting knowl-edge from them (e.g. scientific papers in digital jour-nals). Document similarity comparisons in many in-formation retrieval (IR) and natural language process-ing (NLP) areas are too costly to be performed in suchhuge collections of data and require more efficient ap-*Corresponding author. E-mail: cbadenes@fi.upm.es.proaches than having to calculate all pairwise similari-ties.In this paper we address the problem of programmat-ically generating annotations for each of the items in-side big collections of textual documents, in a way thatis computationally affordable and enables a semantic-aware exploration of the knowledge inside it that state-of-the-art methods relying on topic models are not ableto materialize.Most text mining algorithms represent documentsin a common feature space that abstracts the specific1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reserved2 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithmsequence of words used in each document and, withappropriate representations, facilitate the analysis ofrelationships between documents even when writtenusing different vocabularies. Although a sparse wordor n-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. LatentSemantic Indexing (LSI) [17], Probabilistic Latent Se-mantic Indexing (PLSI) [25] and more recently, LatentDirichlet Allocation (LDA) [11], which is the simplestprobabilistic topic model (PTM) [10], are algorithmsfocused on reducing feature space by annotating docu-ments with thematic information. PLSI and PTM alsoallow a better understanding of the corpus through thetopics discovered, since they use probability distribu-tions over the complete vocabulary to describe them.",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-200373.pdf"
    },
    {
        "title": "Astrea: Automatic Generation of SHACL Shapes from Ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/Astrea",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\978-3-030-49461-2_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Furthermore, the code of the tool is also available in GitHub6 under the Apache 2.0 licence7."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.3571009",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-49461-2_29",
        "arxiv": null,
        "abstract": "Abstract. Knowledge Graphs (KGs) that publish RDF data modelledusing ontologies in a wide range of domains have populated the Web. TheSHACL language is a W3C recommendation that has been endowed toencode a set of either value or model data restrictions that aim at validat-ing KG data, ensuring data quality. Developing shapes is a complex andtime consuming task that is not feasible to achieve manually. This arti-cle presents two resources that aim at generating automatically SHACLshapes for a set of ontologies: (1) Astrea-KG, a KG that publishes a setof mappings that encode the equivalent conceptual restrictions amongontology constraint patterns and SHACL constraint patterns, and (2)Astrea, a tool that automatically generates SHACL shapes from a setof ontologies by executing the mappings from the Astrea-KG. These tworesources are openly available at Zenodo, GitHub, and a web application.In contrast to other proposals, these resources cover a large number ofSHACL restrictions producing both value and model data restrictions,whereas other proposals consider only a limited number of restrictionsor focus only on value or model restrictions.Keywords: SHACL shapes · RDF validation · OntologyResource type: Dataset & SoftwareAstrea-KG: http://astrea.helio.linkeddata.es/Astrea-KG DOI: https://doi.org/10.5281/zenodo.3571009Astrea application: http://astrea.linkeddata.es/1 IntroductionKnowledge Graphs (KGs) are becoming pervasive on the Web [5]. Since 2014there is a growing number of KGs from different domains that publish a quitelarge amount of data using RDF and modelled with ontologies [19]. As a result,in the last decade a considerable effort has been put in developing ontologiesfor specific domains [21]. Due to the growth of these public available KGs, theW3C has promoted a recommendation called SHACL (Shapes Constraint Lan-guage) to validate the RDF graphs [2]. In the last years KGs validation by meansof SHACL shapes has gained momentum and has become a relevant researchtopic [14].c© Springer Nature Switzerland AG 2020A. Harth et al. (Eds.): ESWC 2020, LNCS 12123, pp. 497–513, 2020.https://doi.org/10.1007/978-3-030-49461-2_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-49461-2_29&domain=pdfhttp://orcid.org/0000-0002-1823-4484http://orcid.org/0000-0003-2011-3654http://orcid.org/0000-0002-0421-452Xhttp://astrea.helio.linkeddata.es/https://doi.org/10.5281/zenodo.3571009http://astrea.linkeddata.es/https://doi.org/10.1007/978-3-030-49461-2_29498 A. Cimmino et al.A shape defines a set of restrictions that data from a KG must fulfil. There aretwo kinds of restrictions [15], those that refer to the data model, e.g., cardinality,and those that apply to the data values, e.g., string patterns. Due to this reasondeveloping shapes has become the cornerstone solution to validate KG data.Nevertheless, developing data shapes is a complex task due to the potentialsize of the data and all the available restrictions that require a deep domain",
        "file_name": "978-3-030-49461-2_29.pdf",
        "file_path": "PDFs\\978-3-030-49461-2_29.pdf"
    },
    {
        "title": "A Framework Based on Distributed Ledger Technologies for Data Management and Services in Intelligent Transportation Systems",
        "implementation_urls": [],
        "doi": "10.1109/access.2020.2998012",
        "arxiv": null,
        "abstract": "ABSTRACT Data are becoming the cornerstone of many businesses and entire systems infrastructure.Intelligent Transportation Systems (ITS) are no different. The ability of intelligent vehicles and devices toacquire and share environmental measurements in the form of data is leading to the creation of smart servicesfor the benefit of individuals. In this paper, we present a system architecture to promote the developmentof ITS using distributed ledgers and related technologies. Thanks to these, it becomes possible to create,store and share data generated by users through the sensors on their devices or vehicles, while on themove. We propose an architecture based on Distributed Ledger Technologies (DLTs) to offer features suchas immutability, traceability and verifiability of data. IOTA, a promising DLT for IoT, is used togetherwith Decentralized File Storages (DFSes) to store and certify data (and their related metadata) comingfrom vehicles or by the users’ devices themselves (smartphones). Ethereum is then exploited as the smartcontract platform that coordinates the data sharing through access control mechanisms. Privacy guaranteesare provided by the usage of distributed key management systems and Zero Knowledge Proof. We provideexperimental results of a testbed based on real traces, in order to understand if DLT and DFS technologiesare ready to support complex services, such as those that pertain to ITS. Results clearly show that, whilethe viability of the proposal cannot be rejected, further work is needed on the responsiveness of DLTinfrastructures.INDEX TERMS Intelligent transportation systems, distributed ledger technologies, blockchain, smartcontracts, decentralized file storage, sensing as a service.I. INTRODUCTIONThe future of Intelligent Transportation Systems (ITS) will bebased on the ability of vehicles to sense, store and exchangebig data. Vehicles will be more and more equipped withsensors that track information about the vehicle internals,as well as information about the surrounding environment androad conditions. Moreover, ubiquitous connectivity allowsindividuals to post crowdsensed information through theirsmartphones and mobile devices. This makes them becomeThe associate editor coordinating the review of this manuscript andapproving it for publication was Yanli Xu .an active part of ITS. Such crowd-sensed information isessential for building sophisticated smart services that aimat improving traffic management, transportation efficiencyand safety, raising awareness about the environment, and thusimproving the liveability and health status of the communityof a given territory. We thus envisage that vehicles and theirusers will be able to record data, store them in some databoxes, and communicate with other vehicles or users as well.There are numerous examples of Vehicle-to-Vehicle (V2V)and Vehicle-to-Infrastructure (V2I) based applications, suchas notification services [1], [2], as well as standards forcommunication messages, e.g., ETSI Cooperative Awareness100384 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ VOLUME 8, 2020https://orcid.org/0000-0002-4159-4269https://orcid.org/0000-0002-1911-4708https://orcid.org/0000-0002-3690-6651https://orcid.org/0000-0001-8218-7195M. Zichichi et al.: Framework Based on DLTs for Data Management and Services in ITSMessages (CAM) [3]. In these contexts, one of the mainissues is the unreliability of the exchanged information. Thisproblem is typically due to the physical errors of the sensors,",
        "file_name": "10!1109%access!2020!2998012.pdf",
        "file_path": ".\\PDFs\\10!1109%access!2020!2998012.pdf"
    },
    {
        "title": "Towards a new generation of ontology based data access",
        "implementation_urls": [
            {
                "identifier": "https://github.com/tarql/tarql",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-190384.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: OBDA, Data Translation, Query Translation, Mapping Translation 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-190384",
        "arxiv": null,
        "abstract": "Abstract. Ontology Based Data Access (OBDA) refers to a range of techniques, algorithms and systems that can be used todeal with the heterogeneity of data that is common inside many organisations as well as in inter-organisational settings andmore openly on the Web. In OBDA, ontologies are used to provide a global view over multiple local datasets; and mappings arecommonly used to describe the relationships between such global and local schemas. Since its inception, this area has evolvedin several directions. Initially, the focus was on the translation of original sources into a global schema, and its materialisation,including non-OBDA approaches such as the use of Extract Transform Load (ETL) workflows in data warehouses and, morerecently, in data lakes. Then OBDA-based query translation techniques, relying on mappings, were proposed, with the aim ofremoving the need for materialisation, something especially useful for very dynamic data sources. We think that we are nowwitnessing the emergence of a new generation of OBDA approaches. It is driven by the fact that a new set of declarative mappinglanguages, most of which stem from the W3C Recommendation R2RML for Relational Databases (RDB), are being created.In this vision paper, we enumerate the reasons why new mapping languages are being introduced. We discuss why it may berelevant to work on translations among them, so as to benefit from the engines associated to each of them whenever one languageand/or engine is more suitable than another. We discuss the emerging concept of “mapping translation”, the basis for this newgeneration of OBDA, together with some of its desirable properties: information preservation and query result preservation. Weshow several scenarios where mapping translation can be or is being already applied, even though this term has not necessarilybeen used in existing literature.Keywords: OBDA, Data Translation, Query Translation, Mapping Translation1. IntroductionDatabase technologies play a vital role in the devel-opment of information systems for all sorts of organi-sations. So far, relational databases (RDB) are still thedominating type of structure and technology used fordata management inside organisations, although otherformats (e.g. JSON, spreadsheets, XML) and typesof databases (e.g. noSQL, graph databases) have alsoemerged as alternatives for data representation andmanagement in the last decades.*Corresponding author. E-mail: ocorcho@fi.upm.es.In the early days of information system devel-opment, it was natural for organisations to developtheir own data models, which were strongly alignedwith their activities. This led to a large heterogene-ity across organisations, and even across different de-partments inside the same organisation. Such hetero-geneity was especially evident in the case of organ-isational changes, merges, etc. Similarly, data ware-houses were also used in order to align and materialisedata from different sources, normally from the sameorganisation, so as to provide support for analyticalqueries and for the generation of reports. These situ-ations made researchers and professionals start work-1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:ocorcho@fi.upm.esmailto:fpriyatna@fi.upm.esmailto:dchaves@fi.upm.esmailto:ocorcho@fi.upm.es2 O. Corcho et al. / Towards a New Generation of Ontology Based Data Access1 12 23 3",
        "file_name": "10!3233%sw-190384.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-190384.pdf"
    },
    {
        "title": "Graph-based root cause analysis for service-oriented and microservice architectures",
        "implementation_urls": [
            {
                "identifier": "https://github.com/prometheus/node",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!jss!2019!110432.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: SOA, Microservices, Root Cause Analysis, Containers, Graphs 1."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.jss.2019.110432",
        "arxiv": null,
        "abstract": "AbstractService-oriented architectures and microservices define two ways of designing software with the aim ofdividing an application into loosely-coupled services that communicate among each other. This translatesinto rapid development, where each service is developed and deployed by small teams, enabling continuousshipping of new features and fast-evolving applications. However, the underlying complexity of this typeof architecture can hinder observability and maintenance by the user. In particular, identifying the rootcause of an anomaly detected in the application can be a difficult and time-consuming task, consideringthe numerous services and connections to be examined. In this work, we present a root cause analysisframework, based on graph representations of these architectures. The graphs can be used to compareany anomalous situation that happens in the system with a library of anomalous graphs that serves as aknowledge base for the user troubleshooting those anomalies. We use the Grid’5000 testbed to deploy threedifferent architectures and inject a set of anomalies. The results show how our graph-based approach is19.41% more effective than a machine learning method that does not take into account the relationshipbetween elements.Keywords: SOA, Microservices, Root Cause Analysis, Containers, Graphs1. IntroductionAs many industries increasingly rely on information systems to operate efficiently, software developmentand architectures have evolved in different directions. Virtualisation has gained momentum in the last decadethanks to the ability of cloud providers to offer Infrastructure as a Service (IaaS) to their clients. This enablescustomers to use processing power on demand through Virtual Machines (VMs) and it eliminates the burdenof maintaining the infrastructure. It also allows the provider to optimize the utilization of the datacenterby means of VM migration and consolidation. The next step in virtualization has been containerisation. Acontainer is an executable unit of packaged software that includes all the dependencies needed. It runs onthe host operating system, sharing the kernel and using its own user space, isolating it from other containers.This eliminates any boot time overhead in comparison with VM’s, empowering the migration and scalingadvantages of virtualisation. A machine can also host dozens of containers at the same time1 thanks to theirlightweight nature. Finally, from a software development point of view, they eliminate the “it works on mymachine” problem, since the whole running environment is included inside the container.Industry has acknowledged all the containers benefits [1], specially thanks to the adoption of technolo-gies like Docker [2], with DevOps [3] and microservice architectures becoming growing trends and commonEmail addresses: abrandon@fi.upm.com (Álvaro Brandón), marc.solesimo@ca.com (Marc Solé),alberto.huelamosegura@ca.com (Alberto Huélamo), david.solans@ca.com (David Solans), mperez@fi.upm.com (Maŕıa S.Pérez), victor.muntes@ca.com (Victor Muntés-Mulero)1https://www.datadoghq.com/docker-adoption/ (last accessed Dec 2018)Preprint submitted to Elsevier October 7, 2019practices. The philosophy behind microservices follows the same principles used in service-oriented archi-tectures [4]. It consists of breaking the system logic into different, small units where each one has a singletask or responsibility. The different units or microservices communicate and cooperate with each other toprovide the system functionality as a whole. Any of the aforementioned units of logic is executed inside acontainer. In comparison with a monolithic architecture, it allows the user to scale specific components ofthe application by increasing the number of containers responsible for that part. Besides, it also enablesnew paradigms like serverless computing, where certain events trigger the allocation of new containers, ab-stracting the infrastructure needed by the user [5]. Finally, it also facilitates the development process andsoftware reuse [6].Coordinating and scaling these containers require an orchestrating unit, specially if we want to deploythem in a distributed infrastructure with several machines. Tools like Kubernetes2 or DC/OS3 have filledthis gap. These platforms have self-healing features, where unhealthy or faulty containers can be relaunched,or containers migrated to a different machine in case one of their hosts dies. But establishing the root causeof these failures can be really complex, specially when we have a network of different services that depend oneach other. The troubleshooting process normally involves a tedious search through logs across the different",
        "file_name": "10!1016%j!jss!2019!110432.pdf",
        "file_path": ".\\PDFs\\10!1016%j!jss!2019!110432.pdf"
    },
    {
        "title": "An Ontology-Based Deep Learning Approach for Knowledge Graph Completion with Fresh Entities",
        "implementation_urls": [
            {
                "identifier": "https://github.com/Elviish/ntn-pytorch-ontological-info",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-23887-2_15.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Moreover, these provided datasets are hegemonic in the literature and widely used to 3The implementation used in this work, as well as the employed datasets are available at https://github.com/Elviish/ntn-pytorch-ontological-info."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-23887-2_15",
        "arxiv": null,
        "abstract": "Abstract. This paper introduces a new initialization method for knowledge graph(KG) embedding that can leverage ontological information in knowledge graphcompletion problems, such as link classification and link prediction. Althoughthe initialization method is general and applicable to different KG embeddingapproaches in the literature, such as TransE or RESCAL, this paper experimentswith deep learning and specifically with the neural tensor network (NTN) model.The experimental results show that the proposed method can improve link classi-fication for a given relation by up to 15%. In a second contribution, the proposedmethod allows for addressing a problem not studied in the literature and intro-duced here as “KG completion with fresh entities”. This is the use of KG em-beddings for KG completion when one or several of the entities in a triple (head,relation, tail) has not been observed in the training phase.Keywords: Statistical Relational Learning, Ontological Knowledge Base, Knowl-edge Graph Embedding, Latent Feature Model.1 IntroductionKnowledge representation has always been one of the main challenges of ArtificialIntelligence. Throughout time, several approaches have been proposed to model knowl-edge in a structured and comprehensive way. Ontologies [7] are an important approachto knowledge representation. Ontologies present a formal definition of types, propertiesand relationships between entities applied to a concrete domain. These representationsare fairly intuitive for humans, as well as being easily translated into machine languagesto allow computers to share concepts. One of the main characteristics of this represen-tation is that concepts or classes are organized in a hierarchical way. These classes tendto remain static, whereas individuals or instances are more dynamic.Knowledge graphs (KGs) are usually supported on this structured form of represen-tation. The approach considered in KGs is, however, different from the one existing inontologies. Unlike ontologies, KGs aim to gather all the concrete facts existing in thedomain instead of extracting general patterns. A fact follows usually the schema (head,†Authors listed in alphabetical order.2 Amador-Domı́nguez et al.relation, tail), representing the link between two certain entities by means of a specificrelation. These KGs have an increasing popularity in the literature, because they supportseveral tasks, such as question answering or recommendation systems [5].KGs usually present a high degree of incompleteness, hindering them to be usedin a number of domains. Even when completing KGs is mostly a handcrafted task, anumber of approaches allow this Knowledge Graph Completion (KGC) to be automaticor at least supported automatically [5]. Popular KGC approaches essentially calculatea Knowledge Graph Embedding (KGE) that is used for tasks such as: (1) link or tripleprediction, given two elements of a triple in the KG, ranking the most plausible entitiesto fulfill the missing element; and (2) link or triple classification, predicting if a triplebelongs to a KG [10].Figure 1 illustrates a simplistic KG about American politicians and their relatives.In this graph, Michelle Obama’s nationality is unknown. A KGC approach would allow:triple prediction, i.e., ranking the nationalities more likely to fulfill the triple (MichelleObama, nationality, ?); or, triple classification, i.e., calculating the confidence of thetriple (Michelle Obama, nationality, United States). KGE and KGC approaches couldlearn that American presidents’ wives are likely to be American, although this is notnecessarily true as is the case with Donal Trump’s wife.Fig. 1: An example of Knowledge Graph Completion.Two major shortcomings found in current KGC and KGE proposals are that: (1)",
        "file_name": "10!1007%978-3-030-23887-2_15.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-23887-2_15.pdf"
    },
    {
        "title": "Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2019.03.001",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1016%j!websem!2019!03!001.pdf",
        "file_path": ".\\PDFs\\10!1016%j!websem!2019!03!001.pdf"
    },
    {
        "title": "Scalable Cross-lingual Document Similarity through Language-specific Concept Hierarchies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/cbadenes/crosslingual-semantic-similarity",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3360901!3364444.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "There are 4,904 re-ciprocal hierarchical relationships (no polyhierarchy) and 6,992 reciprocal associative relationships."
                    }
                ]
            }
        ],
        "doi": "10.1145/3360901.3364444",
        "arxiv": "2101.03026",
        "abstract": "ABSTRACTWith the ongoing growth in number of digital articles in a wider setof languages and the expanding use of different languages, we needannotation methods that enable browsing multi-lingual corpora.Multilingual probabilistic topic models have recently emerged as agroup of semi-supervised machine learning models that can be usedto perform thematic explorations on collections of texts in multiplelanguages. However, these approaches require theme-aligned train-ing data to create a language-independent space. This constraintlimits the amount of scenarios that this technique can offer solu-tions to train and makes it difficult to scale up to situations wherea huge collection of multi-lingual documents are required duringthe training phase. This paper presents an unsupervised documentsimilarity algorithm that does not require parallel or comparablecorpora, or any other type of translation resource. The algorithmannotates topics automatically created from documents in a sin-gle language with cross-lingual labels and describes documents byhierarchies of multi-lingual concepts from independently-trainedmodels. Experiments performed on the English, Spanish and Frencheditions of JCR-Acquis corpora reveal promising results on classi-fying and sorting documents by similar content.CCS CONCEPTS• Information systems → Digital libraries and archives; In-formation retrieval.KEYWORDScross-lingual semantic similarity; large-scale text analysis; topicmodelsACM Reference Format:Carlos Badenes-Olmedo, José Luis Redondo-García, and Oscar Corcho. 2019.Scalable Cross-lingual Document Similarity through Language-specific Con-cept Hierarchies. In Proceedings of the 10th International Conference onKnowledge Capture (K-CAP ’19), November 19–21, 2019, Marina Del Rey, CA,USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3360901.3364444Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA© 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-7008-0/19/11. . . $15.00https://doi.org/10.1145/3360901.33644441 INTRODUCTIONCross-language information extraction deals with the retrieval ofdocuments written in languages different from the language of theuser’s query. At execution time, the query in the source language istypically translated into the target language of the documents with",
        "file_name": "10!1145%3360901!3364444.pdf",
        "file_path": ".\\PDFs\\10!1145%3360901!3364444.pdf"
    },
    {
        "title": "PaCTS 1.0: A Crowdsourced Reporting Standard for Paleoclimate Data",
        "implementation_urls": [],
        "doi": "10.1029/2019pa003632",
        "arxiv": null,
        "abstract": "Abstract The progress of science is tied to the standardization of measurements, instruments, and data.This is especially true in the Big Data age, where analyzing large data volumes critically hinges on the databeing standardized. Accordingly, the lack of community‐sanctioned data standards in paleoclimatologyhas largely precluded the benefits of Big Data advances in the field. Building upon recent efforts tostandardize the format and terminology of paleoclimate data, this article describes the PaleoclimateCommunity reporTing Standard (PaCTS), a crowdsourced reporting standard for such data. PaCTS captureswhich information should be included when reporting paleoclimate data, with the goal of maximizing thereuse value of paleoclimate data sets, particularly for synthesis work and comparison to climate modelsimulations. Initiated by the LinkedEarth project, the process to elicit a reporting standard involved aninternational workshop in 2016, various forms of digital community engagement over the next few years,and grassroots working groups. Participants in this process identified important properties acrosspaleoclimate archives, in addition to the reporting of uncertainties and chronologies; they also identifiedarchive‐specific properties and distinguished reporting standards for new versus legacy data sets. This workshows that at least 135 respondents overwhelmingly support a drastic increase in the amount of metadataaccompanying paleoclimate data sets. Since such goals are at odds with present practices, we discuss atransparent path toward implementing or revising these recommendations in the near future, using bothbottom‐up and top‐down approaches.Plain Language Summary Standardizing the way data are described and shared is key toaccelerating the progress of science. Building on recent advances in paleoceanography andpaleoclimatology, we present the first community‐led reporting standard for such datasets. The PaleoclimateCommunity reporTing Standard (PaCTS) provides guidelines as to which information should be includedwhen reporting data from various paleoclimate archives, as well as themes common to many fields, likeuncertainty and other site‐specific information. The ultimate goal of this effort is to (1) make these datasetsmore re‐usable over the long term, and (2) provide a roadmap for implementing and revising the standard,as the field of paleoclimatology and its practitioners both evolve. The requirements are driven by thediffering needs of data producers and the data consumers, who often have different goals in mind. Thus,agreeing on and writing up these requirements involves building consensus among the community to decideon their present and future goals.10.1029/2019PA003632Paleoceanography and PaleoclimatologyKHIDER ET AL. 21. IntroductionPaleoclimatology is a highly integrative discipline, often requiring the comparison of multiple data setsand model simulations to reach fundamental insights about the climate system. Currently, suchsyntheses are hampered by the time and effort required to transform the data into a usable formatfor each application. This task, called data wrangling, is estimated to consume up to 80% of researchertime in some scientific fields (Dasu & Johnson, 2003), an estimate commensurate with the experienceof many paleoclimatologists, particularly at the early‐career stage. Wrangling involves not onlyidentifying missing values or outliers in the time series but also searching multiple databases forthe scattered records, contacting the original investigators for the missing data and metadata, andorganizing the data into a machine‐readable format. Further, this wrangling requires an understandingof each data set's originating field and its unspoken practices and so cannot be easily automated or out-sourced to unskilled labor or software. There is therefore an acute need for standardizing paleoclimatedata sets.Indeed, standardization accelerates scientific progress, particularly in the era of Big Data, where data shouldbe Findable, Accessible, Interoperable, and Reusable (FAIR; Wilkinson et al., 2016). Standardization iscritical to many scientific endeavors: efficiently querying databases, analyzing the data and visualizingthe results; removing participation barriers for early‐careers scientists or people outside the field; redu-cing unintended errors in data management; and ensuring appropriate credit of the original authors.While the paleoclimate community has made great strides in this direction (e.g., Williams et al., 2018),much work remains. The recent adoption of the FAIR data principles (Wilkinson et al., 2016) by the",
        "file_name": "10!1029%2019pa003632.pdf",
        "file_path": ".\\PDFs\\10!1029%2019pa003632.pdf"
    },
    {
        "title": "Semantic Modelling of Plans and Execution Traces for Enhancing Transparency of IoT Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TrustLens/EP-PLAN",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1109%iotsms48152!2019!8939260.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available: https://www.w3.org/TR/2013/REC-prov-o-20130430/[4] D."
                    }
                ]
            }
        ],
        "doi": "10.1109/iotsms48152.2019.8939260",
        "arxiv": null,
        "abstract": "Abstract—Transparency of IoT systems is an essential require-ment for enhancing user’s trust towards such systems. Prove-nance mechanisms documenting the execution of IoT systemsare often cited as an enabler of such transparency. However,provenance records often lack detailed descriptions of a system’sexpected behaviour. Plan specifications describe the steps neededto achieve a certain goal by a human or an automated system.Once plans reach a certain level of complexity, they are typicallydecomposed in different levels of abstraction. However, thisdecomposition makes it difficult to relate high level abstract plansto their granular execution traces. This paper introduces EP-Plan, a vocabulary for linking the different levels of granularityof a plan with their respective provenance traces. EP-Plan alsoprovides the means to describe plan metadata such as constraints,policies, rationales, and expected participating agents associatedwith a plan.I. INTRODUCTIONAn increasing number of systems provide means to docu-ment provenance records of past executions [1] (i.e., executiontraces), in order to inspect and explain the creation process ofexisting results or behaviour of a system. In this paper, weargue that recording such provenance can be further enhancedby recording the set of planned steps that guided its execution,and we refer to such records as plans.Plans document intended system behaviour, which is ben-eficial at the point when no runtime provenance is available(e.g. to assess risks associated with a planned IoT deployment).Plans are also critical to understand errors, enabling a pointof reference for comparison when the execution deviates fromwhat was planned to happen.Gateway CloudPlanRelay data from sensorsPlanProcess and store dataPlanDeliver smart temperature monitoring at Alice’s homeSensorPlanProduce observationsIoT SystemExecution TraceA log of activities generating and publishing raw data values and timestampsExecution TraceA log of activities receiving and uploading sensor data  Execution Trace",
        "file_name": "10!1109%iotsms48152!2019!8939260.pdf",
        "file_path": ".\\PDFs\\10!1109%iotsms48152!2019!8939260.pdf"
    },
    {
        "title": "T2WML: Table To Wikidata Mapping Language",
        "implementation_urls": [
            {
                "identifier": "https://github.com/usc-isi-i2/t2wml",
                "type": "git",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1145%3360901!3364448.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The formal specification of the full language is available in https://github.com/usc-isi-i2/t2wml."
                    }
                ]
            }
        ],
        "doi": "10.1145/3360901.3364448",
        "arxiv": null,
        "abstract": "ABSTRACTThe web contains millions of useful spreadsheets and CSV files, butthese files are difficult to use in applications because they use awide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes iteasy to map and link arbitrary spreadsheets and CSV files to theWikidata data model. The output of T2WML consists of Wikidatastatements that can be loaded in the public Wikidata knowledgebase or in a Wikidata clone repository, creating an augmentedWikidata knowledge graph that application developers can queryusing SPARQL.1CCS CONCEPTS• Information systems→Extraction, transformation and load-ing.KEYWORDSKnowledge Graphs; RDF; Entity Linking; WikidataACM Reference Format:Pedro Szekely, Daniel Garijo, Divij Bhatia, JiashengWu, Yixiang Yao and JayPujara. 2019. T2WML: Table ToWikidata Mapping Language. In Proceedingsof the 10th International Conference on Knowledge Capture (K-CAP ’19),November 19–21, 2019, Marina Del Rey, CA, USA. ACM, New York, NY, USA,4 pages. https://doi.org/10.1145/3360901.33644481 INTRODUCTIONThe web contains millions of useful spreadsheets and CSV files,including data from many government and international organi-zations. Most institutions offer their data in web sites where userscan download the data in Excel and CSV formats. The downloadeddata is seldom directly usable because, unlike databases (which useone column per variable), spreadsheets often arrange the data indifferent layouts.Fig. 1 illustrates the problem using data downloaded from theUnited Nations web site2 about homicide rates in different countries.We truncated and colored the files for ease of presentation. Thecells with the homicide numbers are highlighted in green, the cells1This material is based upon work supported by United States Air Force under ContractNo. FA8650-17-C-7715.2https://dataunodc.un.org/crime/intentional-homicide-victimsPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.K-CAP ’19, November 19–21, 2019, Marina Del Rey, CA, USA© 2019 Association for Computing Machinery.ACM ISBN 978-1-4503-7008-0/19/11. . . $15.00https://doi.org/10.1145/3360901.3364448that provide contextual information for the value are highlightedin blue, and header cells are highlighted in dark blue. Fig. 1a shows",
        "file_name": "10!1145%3360901!3364448.pdf",
        "file_path": ".\\PDFs\\10!1145%3360901!3364448.pdf"
    },
    {
        "title": "Participation of women in doctorate, research, innovation, and management activities at Universidad Politécnica de Madrid: analysis of the decade 2006–2016",
        "implementation_urls": [],
        "doi": "10.1007/s11192-019-03179-9",
        "arxiv": null,
        "abstract": "AbstractThis article studies the participation of women in doctorate, lecturing and research, innova-tion, and management activities at Universidad Politécnica de Madrid (UPM), the most important and largest university in Spain devoted to engineering and architecture. The analyses revealed significant differences in the ratio of male (76%) and female (24%) lec-turing and research staff. This unequal ratio conducted to women underrepresentation in other actions such as coordination of international projects, decision-making designations, patenting and software licensing, collaboration with companies, and PhD supervision. PhD enrolment and PhD defence data, disaggregated by gender and by technological area, were also analysed as they are the starting point of the academic career, and showed a wide-spread male prevalence over women (ca. 70% men vs. 30% women). The aim of this paper is to present actual, accurate, objective, and gender-segregated information extracted from UPM databases, to carry out a qualitative study drawing on an opinion survey and a “gap” analysis, and to undertake a critical examination of the historic, political, sociocultural and personal factors affecting gender inequalities in academia. Policy recommendations to improve the situation of women and to achieve gender balance in the disciplines of engi-neering and architecture are also provided.Keywords Research · Innovation · Doctorate · Management · Women · STEAMMathematics Subject Classification 62-07 · 62-09JEL Classification I23 · I24 · J24 · J71 · N34 · O30Electronic supplementary material The online version of this article (https ://doi.org/10.1007/s1119 2-019-03179 -9) contains supplementary material, which is available to authorized users. * Asunción Gómez-Pérez  vicerrector.investigacion@upm.es1 Vice-Rectorate for Research, Innovation, and Doctoral Studies, Universidad Politécnica de Madrid, C/Ramiro de Maeztu 7, 28040 Madrid, Spainhttp://orcid.org/0000-0001-7869-6704http://orcid.org/0000-0001-9689-4798http://orcid.org/0000-0001-9299-1371http://orcid.org/0000-0002-3037-0331http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-019-03179-9&domain=pdfhttps://doi.org/10.1007/s11192-019-03179-9https://doi.org/10.1007/s11192-019-03179-91060 Scientometrics (2019) 120:1059–10891 3IntroductionThe participation of women in scientific fields has attracted significant attention in the past years. Different reports have analysed the equality between men and women in the EU, studying the situation of women in different scenarios, such as tertiary education and exec-utive jobs. Moreover, the adoption of anti-discriminatory measures, the discussion of equal treatment policies and women’s rights was also well-documented in these papers (Lipinsky 2013; EC 2015, 2017).In the particular case of Spain, former governments have published reports analysing the participation of the women in science and technology (MINECO 2011; Puy 2015). In the same line, the enrolment and the contribution of women in the engineering and archi-tecture areas have been also widely documented (Pérez-Artieda et al. 2014), showing the prevalence of men over women in all the scenarios studied: undergraduates, PhD gradu-ates, workforce, and decision-making positions. Nevertheless the gender gap in the field of engineering and architecture is especially significant and, although extensively literature has described the gender imbalance in science, has not been completely explored.",
        "file_name": "10!1007%s11192-019-03179-9.pdf",
        "file_path": ".\\PDFs\\10!1007%s11192-019-03179-9.pdf"
    },
    {
        "title": "Automating ontology engineering support activities with OnToology",
        "implementation_urls": [
            {
                "identifier": "https://github.com/GeorgFerdinandSchneider/bot",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!websem!2018!09!003.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Electronic copy available at: https://ssrn.com/abstract=3260516 Pr ep rin t n ot p ee r r ev ie w ed supported by the integration of WIDOCO [12], a standalone application for generating HTML documentation for an individual ontol-ogy."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.websem.2018.09.003",
        "arxiv": null,
        "abstract": "AbstractDue to the increasing uptake of semantic technologies, ontologies are now part of a good number of informa-tion systems. As a result, software development teams that have to combine ontology engineering activitieswith software development practices are facing several challenges, since these two areas have evolved, in gen-eral, separately. In this paper we present OnToology, an approach to manage ontology engineering supportactivities (i.e., documentation, evaluation, releasing and versioning). OnToology is a web-based applicationthat builds on top of Git-based environments and integrates existing semantic web technologies. We havevalidated OnToology against a set of representative requirements for ontology development support activitiesin distributed environments, and report on a survey of the system to assess its usefulness and usability.Keywords: ontology engineering, ontology evaluation, ontology documentation, ontology publication1. IntroductionSince the late 1990s, several ontology engineeringmethodologies have been proposed to transform theart of developing ontologies into an engineering ac-tivity. Methodologies such as METHONTOLOGY[1], On-To-Knowledge [2] and the NeOn Methodol-ogy [3] define clear guidelines, processes, activitiesand life cycles to guide ontology development.Now that ontologies are being increasinglyadopted in information systems, it is clear that on-tology development tasks may also benefit from theapplication of common software engineering prac-tices. Most of the ontology development supportactivities, such as documentation, visualization andevaluation, are usually performed individually, exe-cuting heterogeneous tools that make these activi-ties cumbersome and time consuming. In addition,maintaining and keeping track of the generated re-Email addresses: aalobaid@fi.upm.es (AhmadAlobaid), dgarijo@isi.edu (Daniel Garijo),mpoveda@fi.upm.es (Maŕıa Poveda-Villalón),isantana@fi.upm.es (Idafen Santana-Pérez),albafernandez@fi.upm.es (Alba Fernández-Izquierdo),ocorcho@fi.upm.es (Oscar Corcho)sources for each version of an ontology has becomea challenge for ontology developers.The ontology engineering community has alreadyshown progress towards adapting ontology develop-ment to agile software development methodologies[4, 5, 6]; as well as supporting collaborative ontol-ogy development throughout the use of common-practice software engineering tools [7, 8]. In fact,it is now common among ontology developers touse Git-based environments [9] such as GitHub1(usual in software development) for keeping trackof ontology revisions. However, existing approachespresent either partial solutions; require specializedskills that complicate their adoption (e.g., complexinstallation setup); or produce their outcome usingidiosyncratic formats that are difficult to integrate",
        "file_name": "10!1016%j!websem!2018!09!003.pdf",
        "file_path": ".\\PDFs\\10!1016%j!websem!2018!09!003.pdf"
    },
    {
        "title": "Ontology evolution for personalised and adaptive activity recognition",
        "implementation_urls": [],
        "doi": "10.1049/iet-wss.2018.5209",
        "arxiv": null,
        "abstract": "Abstract: Ontology-based knowledge driven Activity Recognition (AR) models play a vital role in realm of Internet of Things (IoTs). However, these models suffer the shortcomings of static nature, inability of self-evolution and lack of adaptivity. Also, AR models cannot be made comprehensive enough to cater all the activities and smart home inhabitants may not be restricted to only those activities contained in AR model. So, AR models may not rightly recognize or infer new activities. In this paper, a framework has been proposed for dynamically capturing the new knowledge from activity patterns to evolve behavioural changes in AR model (i.e. ontology based model). This ontology based framework adapts by learning the specialized and extended activities from existing user-performed activity patterns. Moreover, it can identify new activity patterns previously unknown in AR model, adapt the new properties in existing activity models and enrich ontology model by capturing change representation to enrich ontology model. The proposed framework has been evaluated comprehensively over the metrics of accuracy, statistical heuristics and Kappa Coefficient. A well-known dataset named DAMSH has been used for having an empirical insight to the effectiveness of proposed framework that shows a significant level of accuracy for AR models.  1. Introduction Human Activity Recognition (HAR) determines the activities that have been performed by humans based upon certain knowledge and context. Earlier, Activity Recognition (AR) was performed by observing and analyzing human activities through surveillance cameras. Such manual observation driven AR seemed cost-intensive and demanding around the clock e.g. personnel deployment in home care services was infeasible financially. However, automated HAR systems resolved the issues by providing efficient and cost-effective measures instead of human-centered observations and analysis. Continuous scientific and technical progress has directed the human expectation from HAR toward Personalized HAR [2] for personalized service provision. A rich growth of data-driven and knowledge-driven modeling techniques have been proposed in [3] [4] [5]. Limitations of data-driven problems are cold start problem and non-reusability [1] [3]. Whereas, knowledge-driven techniques are static in nature, incomplete, and non-adaptable [1] [4]. One recent contribution in knowledge-driven techniques is based upon ontologies [5] [6]. Compared with the rest of the approaches, ontology-based models provide a higher degree of automation, better reasoning ability and solid technological foundations but still lacking the self-evolution. In this paper, we extend our work described in [7] for ontology evolution. Proposed ontological model for Activity Recognition (AR) adopts hybrid activity modeling approach (knowledge-driven and data-driven) in which seed knowledge about activities is modeled in an ontology. Seed knowledge comprises of a set of actions necessary to perform an activity called Perceptible Activity Models (PAMs). Model described in [7] transfer the sensor stream into action properties. This sensor stream is used with different ontological contexts such as duration, location, object type, temporal dependencies among actions and feature-based semantic similarities [7] to recognize personalized activity patterns.  Practically, in activity modeling, it is not possible to ",
        "file_name": "10!1049%iet-wss!2018!5209.pdf",
        "file_path": ".\\PDFs\\10!1049%iet-wss!2018!5209.pdf"
    },
    {
        "title": "Why are ontologies not reused across the same domain?",
        "implementation_urls": [],
        "doi": "10.1016/j.websem.2018.12.010",
        "arxiv": null,
        "abstract": "to DOLCE, one of the main characteristics of an abstract entity is that it has neither spatial nor temporal qualities. Examples of such abstract entities are mathematical ones: a triangle, a cir­cumference, etc. Nevertheless, SUMO defines social role as a sub class of abstract. Let us note that a social role has both spatial and temporal qualities, since it may take place in a particular area along a particular time interval. Let us now suppose that ontology O1 imports SUMO and that O2 imports DOLCE. Let us also suppose that O1 imports O2. In this case, O1 includes contradictory definitions of the term abstract. Given that all the details of the natural language definitions are not formalized in the ontologies, it may be case that no inconsistency appears when a computer reasons with this ontology. However, there will be a contradiction in the intended meaning of the terms of O1. Something similar may happen if a redefinition of classes/prop­erties in the reusing ontology [19,20] takes place. Let us suppose, for example, the following axiom: reused : property rdf : subPropertyOf reusing : property http://www.sparontologies.net/ontologies/scoro/source.htmlhttp://data.semanticweb.org/ns/swc/swc_2009-05-09.htmlhttp://linkedscience.org/teach/ns/http://www.ontoware.org/index.htmlhttp://www.ontologydesignpatterns.org/ont/dul/DUL.owlhttp://www.ontologydesignpatterns.org/ont/d0.owlhttps://w3id.org/def/dul-dolce-zero-en-espannolIf reusing:property were declared as inverse functional, then reused:property would become inverse functional as well. If soft reuse had been carried out, then, formally, the only definition would be the one of the reusing ontology. However, if a third ontology imported both the reused and the reusing ones, there would be, in the third ontology, aformal definition from the reused ontology and another formal definition in the reusing ontology. Concerning the information that was obtained through the OTN use case, several heterogeneity problems were detected. First of all, there are ontologies, in the academic domain, focused on different aspects of such domain but out of the scope of OTN ontology, for example content management, social networking, etc.. Some ontologies represent the same domain as OTN, but from a different perspective. Others do not use Spanish, being the lack of localiza­tion [21]a key factor that hampers reuse. In fact, it has been proved that just 27.04% of the LOV ontologies use languages other than English. Other reasonstonot reuse LOV ontologies belonging tothe Academy category have been deficiencies in the documentation of some candidate ontologies and unreachable imported ontologies. 5. Related work One of the first analysis on ontology reuse was done by Sim-perl [22]. The author presents different use cases on ontology reuse as well as methods and tools. The main conclusion is that the reuse approach is a decision-making problem in which the developer ",
        "file_name": "10!1016%j!websem!2018!12!010.pdf",
        "file_path": ".\\PDFs\\10!1016%j!websem!2018!12!010.pdf"
    },
    {
        "title": "Taxi dispatching strategies with compensations",
        "implementation_urls": [],
        "doi": "10.1016/j.eswa.2019.01.001",
        "arxiv": "2401.11553",
        "abstract": "Abstract Urban mobility efficiency is of utmost importance in big cities. Taxi vehicles are key elements in daily traffic activity. The advance of ICT and geo-positioning systems has given rise to new opportunities for improving the efficiency of taxi fleets in terms of waiting times of passengers, cost and time for drivers, traffic density, CO2 emissions, etc., by using more informed, intelligent dispatching. Still, the explicit spatial and temporal components, as well as the scale and, in particular, the dynamicity of the problem of pairing passengers and taxis in big towns, render traditional approaches for solving standard assignment problem useless for this purpose, and call for intelligent approximation strategies based on domain-specific heuristics. Furthermore, taxi drivers are often autonomous actors and may not agree to participate in assignments that, though globally efficient, may not be sufficently beneficial for them individually. This paper presents a new heuristic algorithm for taxi assignment to customers that considers taxi reassignments if this may lead to globally better solutions. In addition, as such new assignments may reduce the expected revenues of individual drivers, we propose an economic compensation scheme to make individually rational drivers agree to proposed modifications in their assigned clients. We carried out a set of experiments, where several commonly used assignment strategies are compared to three different instantiations of our heuristic algorithm. The results indicate that our proposal has the potential to reduce customer waiting times in fleets of autonomous taxis, while being also beneficial from an economic point of view.  Keywords: Coordination, dynamic fleet management, dynamic optimization, multi-agent systems, open systems, taxi assignment.  1. Introduction Urban mobility is one of the main concerns that public managers face in big cities nowadays. Traffic congestions generate a high quantity of CO2 emissions and cause extra time spent by travelers. One of the main actors involved in the daily traffic activity in urban areas are taxi fleets. They consist of several thousands of vehicles in big cities (e.g. about 15,000 taxis in Madrid, Spain). They are usually affiliated to mediator services, which coordinate service calls and taxi dispatching. Lately, new mobility systems that benefit from the advances in information and communication technologies have emerged, such as Uber1, Lyft2 or Liftago3 among others. Two of the main goals of a taxi fleet are (i) to reduce the response time (e.g., the time between a customer call and the moment a taxi arrives at the customer’s location) and (ii) reduce costs of empty movements (e.g., movements taxis have to make in order to get to the location of customers). The provision of efficient methods for taxi assignment to customers is a challenge that can contribute to reducing distances of empty trips with the resulting decrease of traffic flow, pollution, time and so on. Typically, taxi fleet coordination companies apply the first-come first-serve strategy to assign taxis to customers. Once the taxi accepts the passenger, the dispatching is irreversible. This method is known to be inefficient (Egbelu & Tanchoco, 1984).                                                 1 http://www.uber.com 2 http://www.lyft.com 3 http://www.liftago.com  2 The aforementioned case falls into a specific class of assignment problems which is characterized by a dynamic demand in time and space. To efficiently solve such problems, dynamic algorithms are required instead of classical assignment optimization methods. For this purpose, techniques from the field of intelligent systems are promising, because they allow for developing heuristics-based algorithms that intelligently prune the search space, so as to reduce the computational complexity and to support a sufficient degree of scalability. Furthermore, taxi drivers are usually autonomous actors, i.e. they can freely choose whether to accept or to reject a recommendation proposed by the mediator service, which puts additional constraints on the set of feasible solutions to the assignment problem. As Ossowski and Omicini (2002) ",
        "file_name": "10!1016%j!eswa!2019!01!001.pdf",
        "file_path": ".\\PDFs\\10!1016%j!eswa!2019!01!001.pdf"
    },
    {
        "title": "Prediction and Decision-Making in Intelligent Environments Supported by Knowledge Graphs, A Systematic Review",
        "implementation_urls": [],
        "doi": "10.3390/s19081774",
        "arxiv": null,
        "abstract": "Abstract: Ambient Intelligence is currently a lively application domain of Artificial Intelligenceand has become the central subject of multiple initiatives worldwide. Several approaches insidethis domain make use of knowledge bases or knowledge graphs, both previously existing and adhoc. This form of representation allows heterogeneous data gathered from diverse sources to becontextualized and combined to create relevant information for intelligent systems, usually followinghigher level constraints defined by an ontology. In this work, we conduct a systematic review of theexisting usages of knowledge bases in intelligent environments, as well as an in-depth study of thepredictive and decision-making models employed. Finally, we present a use case for smart homesand illustrate the use and advantages of Knowledge Graph Embeddings in this context.Keywords: knowledge base; knowledge graph; intelligent environment; ambient intelligence;reasoning model; knowledge graph embedding1. IntroductionAccording to Augusto et al.: “An Intelligent Environment is one in which the actions of numerousnetworked controllers is orchestrated by self-programming pre-emptive processes in such a way asto create an interactive holistic functionality that enhances occupants experiences” [1]. Although inthis particular context the term “environment” is popularly associated with homes, it also encompassesbroader scenarios, such as buildings, streets or other areas. Intelligent environments are technologicallybased on the combination of several socio-technical innovations such as the Internet of Things (IoT),mobile Internet access, smartphones, data analytics, open data initiatives, and sharing economymodels [2]. These advances allow intelligent environments to manage assets and resources efficientlyby services enhanced with intelligence such as traffic management or healthcare systems.Developing responsive and smarter environments is one of the main present objectives, as shownby the number of research projects developed to pursue this goal, such as Km4City [3] or RoomPathy [4].Although there exists a considerable heterogeneity among the existing works in terms of objectives,methods, and areas of application, the use of knowledge bases (KBs) or knowledge graphs (KGs) is inthe core of a large number of these works.KBs play a key role in multiple ambient intelligence applications, as they are an essential part ofthe conversion of heterogeneous, numerical data provided by sensors into contextualized and semanticinformation. The transformation procedure is usually performed using ontologies, which enable theSensors 2019, 19, 1774; doi:10.3390/s19081774 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-6838-1266https://orcid.org/ORCID ID: 0000-0001-7587-0703https://orcid.org/0000-0002-0792-4156https://orcid.org/0000-0001-9461-7922http://www.mdpi.com/1424-8220/19/8/1774?type=check_update&version=1http://dx.doi.org/10.3390/s19081774http://www.mdpi.com/journal/sensorsSensors 2019, 19, 1774 2 of 21conversion of unstructured data into knowledge. The nature of this knowledge can be very diverse,depending on its source, its application domain, or its specific purpose. The use of KBs supports formalreasoning and the construction of machine learning models, both supervised and unsupervised.This paper contributes with a systematic review of the main existing applications of KB across thedifferent domains of application of ambient intelligence, studying which are the most common usagesper area. Furthermore, we extract general features of the works existing within a particular area, aswell as its main issues and shortcomings. In addition, we emphasize the different decision-makingand prediction methods identified in the literature, evaluating them according to some of themost relevant and desirable features of them: scalability, interpretability, predictive capability andresource consumption.",
        "file_name": "pdf?version=1555141210",
        "file_path": "PDFs\\pdfversion1555141210.pdf"
    },
    {
        "title": "An intelligent interface for integrating climate, hydrology, agriculture, and socioeconomic models",
        "implementation_urls": [],
        "doi": "10.1145/3308557.3308711",
        "arxiv": null,
        "abstract": "ABSTRACT Understanding the interactions between natural processes and human activities poses major challenges as it requires the integration of models and data across disparate disciplines.  It typically takes many months and even years to create valid end-to-end simulations as different models need to be configured in consistent ways and generate data that is usable by other models. MINT is a novel framework for model integration that captures extensive knowledge about models and data and aims to automatically compose them together. MINT guides a user to pose a well-formed modeling question, select and configure appropriate models, find and prepare appropriate datasets, compose data and models into end-to-end workflows, run the simulations, and visualize the results. MINT currently includes hydrology, agriculture, and socioeconomic models.  CCS CONCEPTS H.5.m. Information interfaces and presentation (e.g., HCI): Miscellaneous. KEYWORDS Intelligent workflow systems; model integration; environmental modeling, scientific discovery.  ACM Reference format: Daniel Garijo, Deborah Khider, Varun Ratnakar, Yolanda Gil, Kelly Cobourn, Ewa Deelman, Chris Duffy, Rafael Ferreira da Silva, Armen Kemanian, Craig Knoblock, Vipin Kumar, Scott Peckham, Yao-Yi Chiang, Ankush Khandelwal, Minh Pham, Jay Pujara, Maria Stoica, Kshitij Tayal, Binh Vu, Dan Feldman, Lele Shu, Rajiv Mayani, Anna Dabrowski, Daniel Hardesty-Lewis, Suzanne Pierce. 2018. An Intelligent Interface for Integrating Climate, Hydrology, Agriculture, and Socioeconomic Models. In Proceedings of IUI '19 Companion, March 17–20, 2019, Marina del Rey, CA, USA. https://doi.org/10.1145/3308557.3308711 1 Introduction Understanding how human activities affect natural resources, and how natural processes affect societies requires complex computational simulations that cut across disciplinary boundaries. For example, in order to understand how weather predictions and agriculture practices affect water availability or how flooding affects planting strategies and population migration, modelers integrate physics-based climate and hydrology models with biologically-informed agriculture models and market distribution economic models. While the questions are short fused in order to prepare for natural disasters or to make near-term policy decisions, integrating these diverse models may take many months or even years.  Major challenges include finding relevant models to address a question and datasets with the necessary granularity and quality to run the models, developing sophisticated data transformations to set up and execute a model, and checking for models compatibility. Existing infrastructure supports some aspects of the modeling (e.g., [1]) but there are no comprehensive frameworks that tackle ",
        "file_name": "10!1145%3308557!3308711.pdf",
        "file_path": ".\\PDFs\\10!1145%3308557!3308711.pdf"
    },
    {
        "title": "A sustainable process and toolbox for geographical linked data generation and publication: a case study with BTN100",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/btn100",
                "type": "git",
                "paper_frequency": 10,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\s40965-019-0060-4.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Original and transformed files, plugin and scripts are available at https://github.com/oeg-upm/btn100."
                    }
                ]
            }
        ],
        "doi": "10.1186/s40965-019-0060-4",
        "arxiv": null,
        "abstract": "AbstractWe describe the process and tools that we have used to generate and publish the BTN100 Linked Dataset, based onthe original data from the Spanish Topographic Base (1:100.000 scale) from the Spanish Instituto Geográfico Nacional.We have taken into account the limitations and lessons learned from our initial experience on the generation andpublication of Linked Data from a range of geographical sources in Spain, in 2010, and we have now refined theprocess in order to facilitate: declarative mappings for the transformations from existing open data (shapefiles),automation of transformations whenever there are changes in the original data sources, version control, andalignment with INSPIRE URIs. As a result of this transformation and publication process we have also updated thereference ontology for geographical features and aligned with general ontologies such as GeoSPARQL.Keywords: Geospatial data, Linked data, Linked dataset, OntologyIntroductionOne of the activities of the Spanish Instituto GeográficoNacional1 (IGN) is to produce geographical informationfor all the territorial entities in Spain. IGN is responsi-ble for maintaining and making accessible cartographicand topographic databases for the representation of theSpanish territory. Their catalogs publish data related totransport networks, geodetic information, administrativeunits, etc. making it possible for everyone to downloadthem from their data portal2 under an open data licensecompatible with CC By 4.03.Governments, via their many agencies and organiza-tions, are constantly producing data that may be highlyinterrelated, but in practice become isolated data dueto lack of interoperability. Cartographic and topographicinformation from IGN may easily enrich informationfrom other government entities data, e.g. data fromthe National Institute of Statistics, Institute of CulturalHeritage, General Direction of Cadastre, Geological andMining Institute, etc.*Correspondence: pespinoza@fi.upm.es1Ontology Engineering Group, Universidad Politécnica de Madrid, Boadilla delMonte 28660, SpainFull list of author information is available at the end of the articleHowever, the generalized lack of use of semantics stan-dards in the descriptions of the data elements within thedata sources make it difficult to reuse them. Althoughprogress in data availability, there are still plenty issuesrelated to semantic interoperability; this is the ability ofinformation systems to exchange data with unambiguous,shared meaning.There are several initiatives around the world that havefocused on generating and publishing Linked Data from arange of geospatial data sources, and a W3C/OCGWork-ing Group was running between 2015 and 2017 with thetitle “Spatial Data on the Web” producing recommenda-tions on how to publish different types of geospatial, sen-sor, and temporal data on the web in a principled manner.The LinkedGeoData initiative4 aimed to make availablethe information collected by Open Street Map5 as RDF",
        "file_name": "s40965-019-0060-4",
        "file_path": "PDFs\\s40965-019-0060-4.pdf"
    },
    {
        "title": "Toward proactive social inclusion powered by machine learning",
        "implementation_urls": [],
        "doi": "10.1007/s10115-018-1230-x",
        "arxiv": null,
        "abstract": "Abstract The ñght against social exclusion is at the heart of the Europe 2020 strategy: 120 million people are at risk of suffering this condition in the EU. Risk prediction models are widely used in insurance companies and health services. However, the use of these models to allow an early detection of social exclusion by social workers is not a c o m m o n practice. This paper describes a data analysis of over 16 K cases with over 60 predictors from the Spanish region of Castilla y León. The use of machine learning paradigms such as logistic regression and random forest makes possible a high precision in predicting chronic social exclusion: around 9 0 % in the most conservative predictions. This prediction models offer a quick rule of thumb that can detect citizens who are in danger of been excluded from the society beyond a temporary situation, allowing social workers to further study these cases. Keywords Social exclusion - Social services - Data analysis - Machine learning - Data mining 1 Introduction S b c W gxcZwjWM is a complex and multidimensional process involving the lack of resources, rights, goods and services, and the inability to participate in the normal relationships and activities, available to most people in a society, whether in economic, social, cultural or political scopes [14]. Social exclusion affects not only the quality of life of individuals, but also the equity and cohesion of society as a whole. El Emilio Serrano emilioserra @ fi.upm.es Mari Carmen Suárez-Figueroa mcsuarez @ ñ.upm. es Jacinto González-Pachón jgpachon@fi.upm.es Asunción Gómez-Pérez asun@fi.upm.es t Ontology Engineering Group, Artificial Intelligence Department, Universidad Politécnica de Madrid, Madrid, Spain http://fi.upm.esmailto:jgpachon@fi.upm.esmailto:asun@fi.upm.esThe economic crisis is undermining the sustainability of social protection systems in the E U [6]: 2 4 % of all the E U population (over 120 million people) are at risk of poverty or social exclusion [6]. The fight against poverty and social exclusion is at the heart of the Europe 2020 strategy for smart, sustainable and inclusive growth. In chronic medical diseases, there is strong evidence supporting that early detection results in less severe outcomes. This paper intends to provide social workers with methods and tools to bring this early detection, which is so beneficial in the medical field, to the challenging problem of chronic social exclusion. Note that although poverty has a significant effect on some dimensions of social exclusion, there are other important causes such as age, ethnicity, disability, gender, and employment status. Therefore, it is considerably more challenging to analyze, detect, treat, and predict social exclusion than poverty. This paper contributes with an (1) analysis of the social services data of Castilla y León (CyL), which is the largest region in Spain and counts with around two and a half million inhabitants. This analysis allows getting insights into why social exclusion can become chronic. Furthermore, a(2) machine learning model capable ofquantifying the risk ofchronic social exclusion is build. Finally, a (3) responsive web application is deployed to allow queries by social workers through a number of devices such as smartphones, tablets, or laptops. A RESTful web service is also provided to integrate the predictive capabilities into other ",
        "file_name": "10!1007%s10115-018-1230-x.pdf",
        "file_path": ".\\PDFs\\10!1007%s10115-018-1230-x.pdf"
    },
    {
        "title": "Enhancing energy management at district and building levels via an EM-KPI ontology",
        "implementation_urls": [],
        "doi": "10.1016/j.autcon.2018.12.010",
        "arxiv": null,
        "abstract": "Abstract 1 The use of information and communication technologies facilitates energy management (EM) at 2 both district and building levels but also generates a considerable amount of data. To gain insights 3 into such data, it is essential to resolve the cross-domain data interoperability problem and 4 determine an approach to exchange performance information and insightful data amongst various 5 stakeholders. This paper developed an EM-KPI (key performance indicator) ontology to exchange 6 key performance information and data for districts and buildings. The ontology contains two 7 components: namely KPIs and EM master data; these, respectively, represent multi-level 8 performance information for energy performance tracking and the key data for data exploitation. 9 Through a demonstration, a sample linked dataset generated using the data correlation predefined 10 in the ontology is presented. The linked data analysis proves the feasibility of the ontology for 11 exchanging data among different stakeholders and for exploring insights in relation to 12 performance improvements.  13 Key words:  14 District; building; energy management; stakeholders; ontology; linked data. 15 1. Introduction 16 Buildings account for approximately 40% of the total final energy use in EU countries [1]. 17 However, a large portion of existing buildings are either designed or operated inefficiently [2]. 18 Energy management (EM) is a measure adopted to improve energy efficiency in buildings. 19 Furthermore, there is an increasing need to manage energy not only in a single building, but also 20 on a district scale [3]. Since the implementation of smart cities involves increasing distributed 21 electricity generation such as solar panels in energy distribution networks, EM at a district level, 22 for the purpose of combining the electricity supply and demand of buildings, is pivotal [4,5]. The 23 use of information and communication technologies (ICTs) facilitates the realisation of joint EM 24 that integrates the energy supply and demand sides.  25 Meanwhile, the use of ICTs also generates a massive amount of data and information, which could 26 provide new analysis possibilities for data-driven decision support and offer insights in relation 27 to potential performance improvement [6]. According to the National Institute of Standards and 28 Technology (NIST) in the United States, it could save up to $2 trillion in energy costs by 2030, 29 through exploiting the data from smart grids [7]. Although the expansion of data presents great 30 opportunities for energy performance improvement, there are still challenges faced in the effort 31 to make sense of this data. The problem is twofold. Firstly, there is an interoperability problem 32 between the cross-domain heterogeneous data. Secondly, the solution requires the extraction of 33 insightful data in order to avoid unnecessary analysis.  34 The extraction of core, insightful data is the primary challenge encountered when seeking to 35 access a large amount of data. Data exploitation is valuable only if they address the issues related 36 to the stakeholders. It is important to focus on data that is worth collecting, analyses which are 37 worth sharing and problems which are worth solving [8]. Master data offers a way to represent 38 key data that provides the most valuable information in an organisation [9]. In this case, master 39 data refers to the critical data objects that need to be shared across or beyond an organisation 40 which support decision-making. Master data was initially used for enterprise data management 41 due to the large volumes of data generated during business processes [10]. In the context of energy 42 management, a similar situation is encountered. Introducing the concept of master data into the 43 energy field can help make the large amount of energy-related data actionable, thus bringing 44 additional insight and value through improved decision-making.  45 The master data involved in EM should be shared among different stakeholders; therefore, it is 46 essential to support their performance concerns. In our previous study, we defined stakeholders 47 as those who have an interest in, who have influence in and who are impacted by the actions of 48 energy management; a detailed methodology was developed for selecting the KPIs (key 49 performance indicators) that underpin stakeholders’ performance goals; additionally, the use of 50 ",
        "file_name": "10!1016%j!autcon!2018!12!010.pdf",
        "file_path": ".\\PDFs\\10!1016%j!autcon!2018!12!010.pdf"
    },
    {
        "title": "Enhancing the maintainability of the Bio2RDF project using declarative mappings",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Bio2RDF is one of the most popular projects that integratesand publishes biomedical datasets as Linked Data. The community hasactively contributed to the generation of these datasets using ad-hoc pro-grammed scripts. In the context of the Semantic Web, Ontology-BasedData Access (OBDA) approaches have been proposed to provide dataaccess and transformation in a more standardized way, using declara-tive mapping languages. In this paper, we propose the use of an OBDAapproach to provide an alternative to the way in which transformationsinto RDF are currently done in the Bio2RDF project, with the aim of en-hancing its methodology in terms of understandability, reusability andmaintainability. We describe the proposed methodology together withthe declarative mappings creation process aiming to improve the afore-mentioned features. We compare the RDF dataset generated using ourproposal with the latest release of Bio2RDF for a subset of the datasources that we have dealt with. Finally, we discuss the set of challengesthat we face with this approach.Keywords: Bio2RDF · OBDA · RML1 IntroductionIn the last decades, the amount of databases that have been created to store andshare biological knowledge has heavily increased [1,2]. According to [3], there aremore than 1600 biological databases that are publicly accessible online, includingwell-known examples, such as PubMed1, UniProt2 or KEGG3. Nowadays, theseresources have become essential for researchers, as they rely on them to conductmuch of their work.Each biological data source contains information specific to its domain. Thismeans that the knowledge of a concept (e.g. enzyme, transcription factor) isdistributed in multiple data sources that are created by different institutions,usually represented in different formats and terminologies. A relevant challengein this domain is how to integrate these data sources in order to provide a1 https://www.ncbi.nlm.nih.gov/pubmed/2 https://www.uniprot.org/3 https://www.genome.jp/kegg/https://www.ncbi.nlm.nih.gov/pubmed/https://www.uniprot.org/https://www.genome.jp/kegg/2 Iglesias-Molina et al.Table 1: Comparison of the methodology of Bio2RDF in its differentreleases and the proposed approach with declarative mappings. Thefeatures compared are the type of tool, how many can be used, and if it allowsmaterialisation or virtualization.FeatureBio2RDFRelease 1Bio2RDFReleases 2 & 3DeclarativeMappingsTool Type Ad-hoc solution Ad-hoc solution General Purpose# Tools 1 (myBio2RDF app) 1 (PHP scripts) ManyMaterialization Yes Yes Yes",
        "file_name": "paper-01.pdf",
        "file_path": "PDFs\\paper-01.pdf"
    },
    {
        "title": "Ontological Representation of Smart City Data: From Devices to Cities",
        "implementation_urls": [],
        "doi": "10.3390/app9010032",
        "arxiv": null,
        "abstract": "Abstract: Existing smart city ontologies allow representing different types of city-related data fromcities. They have been developed according to different ontological commitments and hence do notshare a minimum core model that would facilitate interoperability among smart city informationsystems. In this work, a survey has been carried out in order to study available smart city ontologiesand to identify the domains they are representing. Taking into account the findings of the survey anda set of ontological requirements for smart city data, a list of ontology design patterns is proposed.These patterns aim to be easily replicated and provide a minimum set of core concepts in order toguide the development of smart city ontologies.Keywords: ontology; smart cities; ontology design patterns1. IntroductionThe term smart city refers to a city that manages, in an intelligent way, all its associated resourceswith the aim to enhance the quality of the services provided to citizens and to improve their qualityof life [1,2]. The smart city domain has been a topic of interest in many sectors around the world.Standardization bodies, for example, the International Telecommunications Union (https://www.itu.int) and the International Standards Organization (https://www.iso.org), have been working ondefining standards and recommendations in order to provide a unified way to refer to and managethis particular field. In addition, several initiatives and projects in the smart city field have emerged,which denote the efforts and investments that industries, countries, and regions are making in orderto manage the city resources in a better manner. In the case of city coalitions or research groups(e.g., Smart Cities-European Medium-Sized Cities (http://smart-cities.eu), Open and Agile Smart Cities(http://www.oascities.org), Smart Cities Council (http://smartcitiescouncil.com), etc.), their studiesand business reports mention more than 300 smart cities involved [3] with an increasing need for andinterest in exploring solutions in order to improve their city processes. In the case of projects (e.g.,ESPRESSO [4], CityPulse [5], SmartSantander [6], etc.), which are financed by public, private, or a mixof both funds, they aim, in most cases, to provide technological tools to solve requirements in severalcity challenges.In this respect, there is wide agreement about the fact that smart cities are characterized by apervasive use of information and communication technologies [7–9], which, in various urban domains,may help cities make better use of their resources [10]. Some of these technologies include open datainfrastructures, mobile applications, public participation tools, Internet of Things (IoT) platforms, etc.The data handled or produced by all these technologies is very heterogeneous in terms of formats,structure, and delivery mechanisms, both inside the same city and across different cities. Hence,this opens up the opportunity to create common models to allow interoperability inside cities. In thiscontext, ontologies, understood as formal specifications of shared conceptualizations [11], can be usedAppl. Sci. 2019, 9, 32; doi:10.3390/app9010032 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttps://orcid.org/0000-0002-9260-0753https://www.itu.inthttps://www.itu.inthttps://www.iso.orghttp://smart-cities.euhttp://www.oascities.orghttp://smartcitiescouncil.comhttp://dx.doi.org/10.3390/app9010032http://www.mdpi.com/journal/applscihttp://www.mdpi.com/2076-3417/9/1/32?type=check_update&version=2",
        "file_name": "pdf?version=1545833153",
        "file_path": "PDFs\\pdfversion1545833153.pdf"
    },
    {
        "title": "Semantic technologies and interoperability in the built environment",
        "implementation_urls": [],
        "doi": "10.3233/sw-180321",
        "arxiv": null,
        "abstract": "Abstract. The built environment consists of plenty of physical assets with which we interact on a daily basis. In order toimprove not only our built environment, but also our interaction with that environment, we would benefit a lot from semanticrepresentations of this environment. This not only includes buildings, but also large infrastructure (bridges, tunnels, waterways,underground systems), and geospatial data. With this special issue, an insight is given into the current state of the art in termsof semantic technologies and interoperability in this built environment. This editorial not only summarizes the content of theSpecial Issue on Semantic Technologies and interoperability in the Built Environment, it also provides a brief overview of thecurrent state of the art in general in terms of standardisation and community efforts.Keywords: Semantics, built environment, building information model, linked data, architecture, construction1. Semantics in the built environmentThe built environment encompasses a variety of ar-tifacts ranging from buildings to infrastructures. Theseartifacts are linked at different scales and in variousways. Namely, relations can be found between ele-ments in a singular building, furniture, street lights,and so forth to elements and concepts covering theentire city, such as infrastructure, traffic, and peopleflows. Apart from this multiplicity of artifacts, the builtenvironment also gathers multiple stakeholders whocollaborate in various ways. Collaboration and inter-action not only happens in the built environment as itexists on a daily basis, but even more so throughout*Corresponding author. E-mail: pipauwel.pauwels@ugent.be.all the design, construction and operation phases tak-ing place within the built environment. This includesspecialists (architects, engineers, and contractors), butalso local administrators, facility managers, and citi-zens.A large part of this environment is governed by theArchitecture, Engineering, and Construction (AEC)industry. An effective collaboration and interoperabil-ity between these different actors throughout the life-cycle of the built environment has always been akey challenge to this industry. Data from stakehold-ers is modeled and published in various languagesand scales, in particular using Building InformationModelling (BIM) tools [2], and data evolves consid-erably over time. Hence, maintaining data consistencythroughout the whole life-cycle of a building, espe-1570-0844/18/$35.00 © 2018 – IOS Press and the authors. All rights reservedmailto:pipauwel.pauwels@ugent.bemailto:mpoveda@fi.upm.esmailto:alvaro.sicilia@salle.url.edumailto:Jerome.Euzenat@inria.frmailto:pipauwel.pauwels@ugent.be732 P. Pauwels et al. / Semantic technologies and interoperability in the built environmentcially during the design and construction phases, is afundamental challenge to this industry.Beyond the design and construction phases, impor-tant other amounts of data are present as well in thebuilt environment in general. This data is used to make",
        "file_name": "10!3233%sw-180321.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-180321.pdf"
    },
    {
        "title": "Towards Blockchain and Semantic Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-36691-9_19",
        "arxiv": null,
        "abstract": "Abstract. Blockchain has become a pervasive technology in a widenumber of sectors like industry, research, and academy. In the last decadea large number of tailored-domain problems have been solved thanks tothe blockchain. Due to this reason, researchers expressed their interestin combining the blockchain with other well-known technologies, likeSemantic Web. Unfortunately, as far as we known, in the literature noone has presented the different scenarios in which Semantic Web andblockchain can be combined, and the further benefits for both. In thispaper, we aim a providing an in-depth view of the beneficial symbi-otic relation that these technologies may reach together and report thedifferent scenarios that we have identified in the literature to combineSemantic Web and blockchain.Keywords: Blockchain · Semantic Web · Semantic blockchain1 IntroductionIn the last decade the blockchain technologies have become a pervasive in ourworld [1]. Sectors like finance, security, IoT, or public services have benefited fromthe quantum leap that block chain has brought [2]. The wide range of domainsin which this technology has been used has led researchers to elicit and analysethe problems and challenges related to the use of blockchain technologies [3].One of the interests that researchers have shown lately is to combine theSemantic Web and blockchain technologies [4,4,5]. The reason of this interestrelies on the symbiotic relationship that enhances both technologies, and thepotential that can be reached combining them [6]. As far as we known currentliterature focuses mainly in applications that rely on blockchain and SemanticWeb, with the exception of English et al. who presented the only article thatanalyses the benefits of combining these technologies [7]. The work of Englishet al. provides an overview of what the Semantic Web can do for the blockchainand vice-versa, nevertheless their work focuses on covering a large number oftopics, and not only the benefits, and thus, they lack of an in-depth analysis ofthe benefits and the scenarios in which both technologies are combined.In this paper we aim to extend the work of English et al. [7], by providingan in-depth analysis of the benefits that blockchain may find by relying onSemantic Web and vice-versa. In addition, our goal is to provide an overviewc© Springer Nature Switzerland AG 2019W. Abramowicz and R. Corchuelo (Eds.): BIS 2019 Workshops, LNBIP 373, pp. 220–231, 2019.https://doi.org/10.1007/978-3-030-36691-9_19http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-36691-9_19&domain=pdfhttps://doi.org/10.1007/978-3-030-36691-9_19Towards Blockchain and Semantic Web 221of the different scenarios and approaches to combine blockchain with SemanticData by analysing the advantages and disadvantages of the different scenarios.The rest of this article is organised as follows: Sect. 2 introduces the keyconcepts of the blockchain and the Semantic Web. Thirdly, Sect. 3 presents thedifferent benefits that both technologies may offer to the other. After that, Sect. 4introduces the different scenarios that we have identified to combine blockchainand Semantic Web. Finally, Sect. 5 recaps our conclusions.2 PreliminariesIn this section we aim at introducing the key-concepts of the blockchain andthe Semantic Web, as well as, the main characteristics of both. Our goal is notto provide an in-depth description, instead we aim at describing only the key-",
        "file_name": "10!1007%978-3-030-36691-9_19.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-36691-9_19.pdf"
    },
    {
        "title": "Ontological representation of smart city data: From devices to cities",
        "implementation_urls": [],
        "doi": "10.3390/app9010032",
        "arxiv": null,
        "abstract": "Abstract: Existing smart city ontologies allow representing different types of city-related data fromcities. They have been developed according to different ontological commitments and hence do notshare a minimum core model that would facilitate interoperability among smart city informationsystems. In this work, a survey has been carried out in order to study available smart city ontologiesand to identify the domains they are representing. Taking into account the findings of the survey anda set of ontological requirements for smart city data, a list of ontology design patterns is proposed.These patterns aim to be easily replicated and provide a minimum set of core concepts in order toguide the development of smart city ontologies.Keywords: ontology; smart cities; ontology design patterns1. IntroductionThe term smart city refers to a city that manages, in an intelligent way, all its associated resourceswith the aim to enhance the quality of the services provided to citizens and to improve their qualityof life [1,2]. The smart city domain has been a topic of interest in many sectors around the world.Standardization bodies, for example, the International Telecommunications Union (https://www.itu.int) and the International Standards Organization (https://www.iso.org), have been working ondefining standards and recommendations in order to provide a unified way to refer to and managethis particular field. In addition, several initiatives and projects in the smart city field have emerged,which denote the efforts and investments that industries, countries, and regions are making in orderto manage the city resources in a better manner. In the case of city coalitions or research groups(e.g., Smart Cities-European Medium-Sized Cities (http://smart-cities.eu), Open and Agile Smart Cities(http://www.oascities.org), Smart Cities Council (http://smartcitiescouncil.com), etc.), their studiesand business reports mention more than 300 smart cities involved [3] with an increasing need for andinterest in exploring solutions in order to improve their city processes. In the case of projects (e.g.,ESPRESSO [4], CityPulse [5], SmartSantander [6], etc.), which are financed by public, private, or a mixof both funds, they aim, in most cases, to provide technological tools to solve requirements in severalcity challenges.In this respect, there is wide agreement about the fact that smart cities are characterized by apervasive use of information and communication technologies [7–9], which, in various urban domains,may help cities make better use of their resources [10]. Some of these technologies include open datainfrastructures, mobile applications, public participation tools, Internet of Things (IoT) platforms, etc.The data handled or produced by all these technologies is very heterogeneous in terms of formats,structure, and delivery mechanisms, both inside the same city and across different cities. Hence,this opens up the opportunity to create common models to allow interoperability inside cities. In thiscontext, ontologies, understood as formal specifications of shared conceptualizations [11], can be usedAppl. Sci. 2019, 9, 32; doi:10.3390/app9010032 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-0421-452Xhttps://orcid.org/0000-0002-9260-0753https://www.itu.inthttps://www.itu.inthttps://www.iso.orghttp://smart-cities.euhttp://www.oascities.orghttp://smartcitiescouncil.comhttp://dx.doi.org/10.3390/app9010032http://www.mdpi.com/journal/applscihttp://www.mdpi.com/2076-3417/9/1/32?type=check_update&version=2",
        "file_name": "pdf?version=1545833153",
        "file_path": "PDFs\\pdfversion1545833153.pdf"
    },
    {
        "title": "CORAL: A Corpus of Ontological Requirements Annotated with Lexico-Syntactic Patterns",
        "implementation_urls": [
            {
                "identifier": "https://github.com/CQ2SPARQLOWL/",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-21348-0_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "the dataset presented by Wisniewski et al.8 provides a corpus of 234 compe-tency questions related to 5 different ontologies9."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.1967306",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-21348-0_29",
        "arxiv": null,
        "abstract": "Abstract. Ontological requirements play a key role in ontology devel-opment as they determine the knowledge that needs to be modelled. Inaddition, the analysis of such requirements can be used (a) to improveontology testing by easing the automation of requirements into tests; (b)to improve the requirements specification activity; or (c) to ease ontol-ogy reuse by facilitating the identification of patterns. However, thereis a lack of openly available ontological requirements published togetherwith their associated ontologies, which hinders such analysis. Therefore,in this work we present CORAL (Corpus of Ontological RequirementsAnnotated with Lexico-syntactic patterns), an openly available corpus of834 ontological requirements annotated and 29 lexico-syntactic patterns,from which 12 are proposed in this work. CORAL is openly availablein three different open formats, namely, HTML, CSV and RDF under“Creative Commons Attribution 4.0 International” license.Keywords: Corpus · Linked data · Ontological requirements ·Lexico-syntactic patternsResource type: DatasetDOI: https://doi.org/10.5281/zenodo.1967306URL: http://coralcorpus.linkeddata.es1 IntroductionIn recent years, the definition of functional ontological requirements [3,17,18],which represent the needs that the ontology to be built should cover, and theirautomatic formalization into axioms or tests (e.g., [5,13,19]) have been studied.This work is partially supported by the H2020 project VICINITY: Open virtual neigh-bourhood network to connect intelligent buildings and smart objects (H2020-688467),by ETSI Specialist Task Force 534, and by a Predoctoral grant from the I+D+i pro-gram of the Universidad Politécnica de Madrid. The authors want to thank Agnieszka�Lawrynowicz and her team for helping in the collection of ontological requirements.c© Springer Nature Switzerland AG 2019P. Hitzler et al. (Eds.): ESWC 2019, LNCS 11503, pp. 443–458, 2019.https://doi.org/10.1007/978-3-030-21348-0_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-21348-0_29&domain=pdfhttps://doi.org/10.5281/zenodo.1967306http://coralcorpus.linkeddata.eshttps://doi.org/10.1007/978-3-030-21348-0_29444 A. Fernández-Izquierdo et al.The aim of these studies is to reduce the time consumed by ontology develop-ers during the ontology verification activity, in which the ontology is comparedagainst the ontological requirements, thus ensuring that the ontology is builtcorrectly [16]. Functional ontological requirements can be written in the form ofcompetency questions, which are natural language questions that the ontologyto be modelled should be able to answer, or as statements, which are sentencesthat determine what should be built in the ontology.However, to accurately define ontological requirements is not a trivial taskand, therefore, neither is their automatic translation into a formal language. Dueto the fact that some requirements are ambiguous [9] or vague, their transfor-mation into axioms or tests is not usually direct and, consequently, it is verydifficult to automate such translation. The analysis of these functional onto-logical requirements can be used in several domains, e.g., to improve ontologytesting by easing the automation of requirements into tests, to improve the",
        "file_name": "10!1007%978-3-030-21348-0_29.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-21348-0_29.pdf"
    },
    {
        "title": "Large-scale semantic exploration of scientific literature using topic-based hashing algorithms",
        "implementation_urls": [],
        "doi": "10.3233/sw-200373",
        "arxiv": null,
        "abstract": "Abstract. Searching for similar documents and exploring major themes covered across groups of documents are common activitieswhen browsing collections of scientific papers. This manual knowledge-intensive task can become less tedious and even lead tounexpected relevant findings if unsupervised algorithms are applied to help researchers. Most text mining algorithms representdocuments in a common feature space that abstract them away from the specific sequence of words used in them. ProbabilisticTopic Models reduce that feature space by annotating documents with thematic information. Over this low-dimensional latentspace some locality-sensitive hashing algorithms have been proposed to perform document similarity search. However, thematicinformation gets hidden behind hash codes, preventing thematic exploration and limiting the explanatory capability of topics tojustify content-based similarities. This paper presents a novel hashing algorithm based on approximate nearest-neighbor techniquesthat uses hierarchical sets of topics as hash codes. It not only performs efficient similarity searches, but also allows extendingthose queries with thematic restrictions explaining the similarity score from the most relevant topics. Extensive evaluations onboth scientific and industrial text datasets validate the proposed algorithm in terms of accuracy and efficiency.Keywords: Document Similarity, Information Search and Retrieval, Clustering, Topic Models, Hashing1. IntroductionHuge amounts of documents are publicly availableon the Web offering the possibility of extracting knowl-edge from them (e.g. scientific papers in digital jour-nals). Document similarity comparisons in many in-formation retrieval (IR) and natural language process-ing (NLP) areas are too costly to be performed in suchhuge collections of data and require more efficient ap-*Corresponding author. E-mail: cbadenes@fi.upm.es.proaches than having to calculate all pairwise similari-ties.In this paper we address the problem of programmat-ically generating annotations for each of the items in-side big collections of textual documents, in a way thatis computationally affordable and enables a semantic-aware exploration of the knowledge inside it that state-of-the-art methods relying on topic models are not ableto materialize.Most text mining algorithms represent documentsin a common feature space that abstracts the specific1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reserved2 C. Badenes-Olmedo et al. / Topic-based Hashing Algorithmsequence of words used in each document and, withappropriate representations, facilitate the analysis ofrelationships between documents even when writtenusing different vocabularies. Although a sparse wordor n-gram vectors are popular representational choices,some researchers have explored other representationsto manage these vast amounts of information. LatentSemantic Indexing (LSI) [17], Probabilistic Latent Se-mantic Indexing (PLSI) [25] and more recently, LatentDirichlet Allocation (LDA) [11], which is the simplestprobabilistic topic model (PTM) [10], are algorithmsfocused on reducing feature space by annotating docu-ments with thematic information. PLSI and PTM alsoallow a better understanding of the corpus through thetopics discovered, since they use probability distribu-tions over the complete vocabulary to describe them.",
        "file_name": "10!3233%sw-200373.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-200373.pdf"
    },
    {
        "title": "Towards human-guided machine learning",
        "implementation_urls": [],
        "doi": "10.1145/3301275.3302324",
        "arxiv": null,
        "abstract": "ABSTRACT Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user’s knowledge about the data available.  We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements. CCS CONCEPTS • Human-centered computing KEYWORDS Human-guided machine learning; Automated machine learning (AutoML); Task analysis; Scientific workflows. ACM Reference format: Yolanda Gil, James Honaker, Shikhar Gupta, Yibo Ma, Vito D’Orazio, Daniel Garijo, Shruti Gadewar, Qifan Yang and Neda Jahanshad. 2019. Towards Human-Guided Machine Learning. In 24th International Confer- ence on Intelligent User Interfaces (IUI ’19), March 17–20, 2019, Marina del Rey, CA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3301275.3302324 1 Introduction In recent years, Automated Machine Learning (AutoML) approaches are making great strides to automatically search for machine learning solutions from a large space of possible kinds of models.  Typically, a solution is created by choosing a model (e.g., random forest, SVM, etc.) and then configuring those models by assigning (hyper)parameter values [1]–[4]. A series of challenges and workshops have led to steadfast improvements [5]. Commercial products are now becoming available that automate machine learning, notably for image classification and select natural language processing tasks [6].  While fully automated model learning is appropriate for many applications, there are many contexts where full automation is not desirable or possible.  This is the case when users have knowledge that supplements the available data, particularly in a scientific research context. An AutoML system would look at a set of instances or images the same whether they are about tumor tissues or ad placements.  However, biologists would bring to bear extensive knowledge about human disease in the development of a machine learning model. Without this knowledge, machine learning ",
        "file_name": "10!1145%3301275!3302324.pdf",
        "file_path": ".\\PDFs\\10!1145%3301275!3302324.pdf"
    },
    {
        "title": "T2WML: A cell-based language to map tables into wikidata records",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. The web contains millions of useful spreadsheets and CSVfiles, but these files are difficult to use in applications because they usea wide variety of data layouts and terminology. We present Table ToWikidata Mapping Language (T2WML), a language that makes it easyto map and link arbitrary spreadsheets and CSV files to the Wikidatadata model. The output of T2WML consists of Wikidata statementsthat can be loaded in the public Wikidata, or loaded in a Wikidataclone, creating an augmented Wikidata knowledge graph that applicationdevelopers can query using SPARQL.1Keywords: Knowledge Graphs, RDF, Entity Linking, Wikidata1 IntroductionThe web contains millions of useful spreadsheets and CSV files, including datafrom many government and international organizations. Organizations that offerdata often have web sites where users can search, browse and download dataon a large number of topics. Most institutions offer their data in Excel andCSV formats. The downloaded data is seldom directly usable because, unlikedatabases, which use one column per variable, spreadsheets often arrange thedata in different layouts.Fig. 1 illustrates the problem using data about homicide rates in differentcountries, downloaded from the United Nations web site2. We truncated andcolored the files for ease of presentation. The cells with the homicide numbersare highlighted in green, the cells that provide contextual information for thevalue are highlighted in blue, and header cells are highlighted in dark blue.Fig. 1a shows the layout of the data provided in the UN website; Fig. 1b showsa more compact representation using multi-level headers; Fig. 1c shows a layout1 Copyright (c) 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0). This material is basedupon work supported by United States Air Force under Contract No. FA8650-17-C-7715.2 https://dataunodc.un.org/crime/intentional-homicide-victimsFig. 1. Intentional Homicide Data (Excel file downloaded from dataunodc.un.org)that could be used to store the data in a database, and that can be used directlyin tools such as Pandas; Fig. 1d illustrates a common convention for arrangingdata by topic, by creating stacked tables that share common headings. All tablespresent the same homicide data. The interpretation of each value is defined byfour cells (country, year, population and source) that identify the context for avalue. In each table, the context cells are located in different parts of the data.Only in Fig. 1c (Database) the context cells are in the same row as the value; inthe other tables, context cells appear in different rows, in header rows (examplesa and b), or in visually distinct rows within the table (example d).Existing languages for mapping structured data to RDF, including R2RML3,RML [1], Karma [3] and CSV2RDF [2] process tabular data row by row, requiringtabular data to be in database format (Fig. 1c). RML supports non-tabularformats (JSON and XML) and Karma provides folding and unfolding operatorsto rearrange data for row-based processing. None support complex layouts suchas those in examples b or d.T2WML is a mapping language designed to meet three objectives: 1) Identifyand map data and their context qualifiers in arbitrary data layouts found in Exceland CSV files without the need of complex preprocessing steps to transformtables into a canonical \"Database\" representation; 2) Enable users who are not",
        "file_name": "paper12.pdf",
        "file_path": "PDFs\\paper12.pdf"
    },
    {
        "title": "WDPlus: Leveraging wikidata to link and extend tabular data",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "ABSTRACTScientific observations and other open data are usually made avail-able online in a tabular manner as CSVs and spreadsheets. However,users of these data face three main challenges when attempting touse these products: finding which datasets are related to a topic ofinterest; determining which existing information can be used toextend a given dataset; and how to share their integrated datasetresults with the rest of the community. In this paper we presentWDPlus, a framework designed to address these challenges byleveraging Wikidata. WDPlus allows searching for heterogeneousdatasets, facilitates completing tabular data usingWikidata and pro-poses a mechanism to extend Wikidata in a decentralized manner.KEYWORDSKnowledge Graphs, Entity Linking, Wikidata, RDF1 INTRODUCTIONToday, data about any domain can be found on the web in datarepositories, web APIs and millions of spreadsheets and CSV files.These data comes in a myriad of formats, layouts, terminology andcleanliness that make them difficult to integrate together.Users of these data face three main challenges. The first one isfinding datasets related to a feature or topic of interest. For example,climate scientists often look for years of observational data fromauthoritative sources when estimating the climate of a region. Thesecond challenge is how to complete a given dataset with existingknowledge: machine learning applications are data hungry andrequire asmany data points and features as possible to improve theirpredictions, which often requires integrating data from differentsources. The final challenge is sharing integrated results: onceseveral datasets have been merged together, how to make themavailable to the rest of the community?Knowledge graphs have become the preferred technology to ad-dress these challenges. Large organizations, including search engineproviders, shopping giants and finance institutions are investing inlarge knowledge graphs to integrate and retrieve heterogeneousdata. However, data integration pipelines are usually created man-ually, require significant expertise, and are seldom available to thegeneral public. Similarly, linking to existing datasets in the theLinked Open Data Cloud1usually requires the expertise of a knowl-edge engineer to properly identify the appropriate target instancesto link to in other datasets.1https://lod-cloud.net/Copyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).Recent initiatives such as Data.world,2Google data search [2]and DataCommons",
        "file_name": "short4.pdf",
        "file_path": "PDFs\\short4.pdf"
    },
    {
        "title": "SAD Generator: Eating Our Own Dog Food to Generate KGs and Websites for Academic Events",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kgb-workshop/sad-generator",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-32327-1_19.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "4 Available online at: https://github.com/kgb-workshop/sad-generator 5 http://kgb-workshop.org/2 Fig."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-32327-1_19",
        "arxiv": null,
        "abstract": "Abstract. Nowadays, a website is used to disseminate information aboutan event (e.g., location, dates, time). In the academic world, it is com-mon to develop a website for an event, such as workshops or conferences.Aligning with the �Web of data�, its dissemination should also happen bypublishing the information of the event as a knowledge graph, e.g., viaRDF that is available through a SPARQL endpoint or a Triple PatternsFragment server. However, the RDF generation and website developmentis not always straightforward and can be time-consuming. In this demo,we present the Semantic Academic-event Dissemination (SAD) Genera-tor for generating RDF and websites for academic events. The generatorallows to (i) annotate CSV �les that contain academic event data anduse the annotations to generate a knowledge graph and (ii) generate awebsite with the information for the event querying the knowledge graph.We used our generator to generate the RDF and website of a real work-shop, the KGB workshop. It can be easily reused by organizers of otheracademic events by simply providing the event's information in CSV �les.Keywords: RML · RDF · GraphQL · SPARQL · YARRRML1 IntroductionIn many domains, a website is one of the most frequent dissemination tools usedto reach a wide range of audiences. Typically, a website is generated when anevent is organized, containing useful information for potential attendees, such asthe location, the dates or its main description. In the academic world, di�erentkinds of event are usually organized, such as conferences, workshops, researchschools or seminars where a corresponding website may be provided.In the context of workshops and conferences, the information showed bytheir websites has usually the same structure. The website may show generalinformation about the event (name, dates, location), call for papers togetherwith corresponding topics, organizers, important dates and program committee.This data can be easily de�ned using semi-structured data formats, such as CSV�les or Excel �les.The resources of the event should also be published as a knowledge graphto be aligned with the �Web of data�, following the Semantic Web/Linked Dataapproach: instances are identi�ed by URLs and their types and properties areproperly annotated. However, this step is usually seen as an additional featurefor the dissemination, so it is avoided and, even if done, it remains unrelatedto the website generation process. In fact, many of these websites obtain thedata from isolated data sources where the used format is selected by the Webdeveloper (e.g, databases, JSON or XML) and the data model is speci�c for eachcase. Recommendations, like RDFa, provide mechanisms to solve some of theseissues, but its uptake is low, as it needs a manual intensive e�ort.In this demo paper, we propose the SAD Generator4, a Semantic Academic-event Dissemination Generator that uses Semantic Web technologies to generatea knowledge graph and website for academic events. This knowledge graph pro-vides the data for generating the website. Furthermore, the knowledge graph canbe made available via, e.g, a SPARQL endpoint or a Triple Pattern Fragments(TPF) server. We use this generator to build the knowledge graph and websiteof Knowledge Graph Building Workshop 20195 (co-located with ESWC2019).2 The SAD GeneratorWhen using our generator two main steps are executed: (i) the knowledgegraph is built based on the raw data and, then, (ii) the website is generated",
        "file_name": "10!1007%978-3-030-32327-1_19.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-32327-1_19.pdf"
    },
    {
        "title": "Towards Blockchain and Semantic Web",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-36691-9_19",
        "arxiv": null,
        "abstract": "Abstract. Blockchain has become a pervasive technology in a widenumber of sectors like industry, research, and academy. In the last decadea large number of tailored-domain problems have been solved thanks tothe blockchain. Due to this reason, researchers expressed their interestin combining the blockchain with other well-known technologies, likeSemantic Web. Unfortunately, as far as we known, in the literature noone has presented the different scenarios in which Semantic Web andblockchain can be combined, and the further benefits for both. In thispaper, we aim a providing an in-depth view of the beneficial symbi-otic relation that these technologies may reach together and report thedifferent scenarios that we have identified in the literature to combineSemantic Web and blockchain.Keywords: Blockchain · Semantic Web · Semantic blockchain1 IntroductionIn the last decade the blockchain technologies have become a pervasive in ourworld [1]. Sectors like finance, security, IoT, or public services have benefited fromthe quantum leap that block chain has brought [2]. The wide range of domainsin which this technology has been used has led researchers to elicit and analysethe problems and challenges related to the use of blockchain technologies [3].One of the interests that researchers have shown lately is to combine theSemantic Web and blockchain technologies [4,4,5]. The reason of this interestrelies on the symbiotic relationship that enhances both technologies, and thepotential that can be reached combining them [6]. As far as we known currentliterature focuses mainly in applications that rely on blockchain and SemanticWeb, with the exception of English et al. who presented the only article thatanalyses the benefits of combining these technologies [7]. The work of Englishet al. provides an overview of what the Semantic Web can do for the blockchainand vice-versa, nevertheless their work focuses on covering a large number oftopics, and not only the benefits, and thus, they lack of an in-depth analysis ofthe benefits and the scenarios in which both technologies are combined.In this paper we aim to extend the work of English et al. [7], by providingan in-depth analysis of the benefits that blockchain may find by relying onSemantic Web and vice-versa. In addition, our goal is to provide an overviewc© Springer Nature Switzerland AG 2019W. Abramowicz and R. Corchuelo (Eds.): BIS 2019 Workshops, LNBIP 373, pp. 220–231, 2019.https://doi.org/10.1007/978-3-030-36691-9_19http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-36691-9_19&domain=pdfhttps://doi.org/10.1007/978-3-030-36691-9_19Towards Blockchain and Semantic Web 221of the different scenarios and approaches to combine blockchain with SemanticData by analysing the advantages and disadvantages of the different scenarios.The rest of this article is organised as follows: Sect. 2 introduces the keyconcepts of the blockchain and the Semantic Web. Thirdly, Sect. 3 presents thedifferent benefits that both technologies may offer to the other. After that, Sect. 4introduces the different scenarios that we have identified to combine blockchainand Semantic Web. Finally, Sect. 5 recaps our conclusions.2 PreliminariesIn this section we aim at introducing the key-concepts of the blockchain andthe Semantic Web, as well as, the main characteristics of both. Our goal is notto provide an in-depth description, instead we aim at describing only the key-",
        "file_name": "10!1007%978-3-030-36691-9_19.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-36691-9_19.pdf"
    },
    {
        "title": "Towards an Ontology for Public Procurement Based on the Open Contracting Data Standard",
        "implementation_urls": [
            {
                "identifier": "https://github.com/TBFY/ocds-ontology",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-29374-1_19.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "This edition of the OCDS ontology is available online in GitHub in two versions15: one version only with core OCDS terms and a second version with extensions (e.g., enquiries, lots, etc.)."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-29374-1_19",
        "arxiv": null,
        "abstract": "Abstract. The release of a growing amount of open procurement dataled to various initiatives for harmonising the data being provided. Amongothers, the Open Contracting Data Standard (OCDS) is highly relevantdue to its high practical value and increasing traction. OCDS defines acommon data model for publishing structured data throughout most of thestages of a contracting process. OCDS is document-oriented and focuseson packaging and delivering relevant data in an iterative and event-drivenmanner through a series of releases. Ontologies, beyond providing uniformaccess to heterogeneous procurement data, could enable integration withrelated data sets such as with supplier data for advanced analytics andinsight extraction. Therefore, we developed an ontology, the “OCDSontology”, by using OCDS’ main domain perspective and vocabulary,since it is an essential source of domain knowledge. In this paper, weprovide an overview of the developed ontology.Keywords: Procurement · OCDS · Ontology.1 IntroductionPublic entities worldwide are increasingly required to publish information abouttheir procurement processes (e.g., in Europe, with EU directives 2003/98/ECand 2014/24/EU8) in order to improve effectiveness, efficiency, transparency, andaccountability of public services [8]. As a result, the release of a growing amountof open procurement data led to various initiatives (e.g., OpenPEPPOL4, CENBII5, TED eSenders6, CODICE7, Open Contracting Data Standard (OCDS)8) forharmonising the data being provided. XML formats and file templates are definedwithin these standards to make it possible to structure the messages exchanged bythe various agents involved in electronic procurement. These standards are mostly4 https://peppol.eu5 http://cenbii.eu6 https://simap.ted.europa.eu/web/simap/sending-electronic-notices7 https://contrataciondelestado.es/wps/portal/codice8 http://standard.open-contracting.orghttps://peppol.euhttp://cenbii.euhttps://simap.ted.europa.eu/web/simap/sending-electronic-noticeshttps://contrataciondelestado.es/wps/portal/codicehttp://standard.open-contracting.org2 A. Soylu et al.oriented to achieve interoperability, addressing communication between systems,and hence they usually focus on the type of information that is transmittedbetween the various organizations involved in the process. The structure of theinformation is commonly provided by the content of the documents that areexchanged. Furthermore, there are no generalised standardised practices to referto third parties, companies participating in the process, or even the main objectof contracts. In sum, this still generates a lot of heterogeneity. Ontologies havebeen proposed to alleviate this problem [1,10]. Several ontologies (e.g., PPROC[5], LOTED2 [3], MOLDEAS [7], PCO [6]) have recently emerged, with differentlevels of detail and focus (e.g., legal, process-oriented, pragmatic). However, noneof them has had a wide adoption so far.In this context, OCDS is highly relevant due to its high practical value andincreasing traction. It defines a common data model for publishing structured datathroughout all the stages of a contracting process. It is document-oriented and",
        "file_name": "10!1007%978-3-030-29374-1_19.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-29374-1_19.pdf"
    },
    {
        "title": "An overview of the TBFY knowledge graph for public procurement",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. A growing amount of public procurement data is being madeavailable in the EU for the purpose of improving the effectiveness, effi-ciency, transparency, and accountability of government spending. However,there is a large heterogeneity, due to the lack of common data formats andmodels. To this end, we developed an ontology network for representingand linking tender and company data and ingested relevant data fromtwo prominent data providers into a knowledge graph, called TBFY. Inthis poster paper, we present an overview of our knowledge graph.Keywords: Public procurement · Knowledge graph · Ontology.1 IntroductionIn the EU, public authorities spend around 14% of GDP on the purchase ofservices, works, and supplies every year7. Therefore, a growing amount of publicprocurement data is being made available in the EU through public portals for thepurpose of improving the effectiveness, efficiency, transparency, and accountabilityof government spending. However, there is a large heterogeneity, due to the lackof common data formats and models for exposing such data.There are various standardization initiatives for electronic procurement, suchas Open Contracting Data Standard (OCDS)8 and TED eSenders 9. However,these are mostly oriented to achieve interoperability, document-oriented, andprovide no standardised practices to refer to third parties, companies participatingin the process, etc. This again generates a lot of heterogeneity. The Semantic Web? Copyright c© 2019 for this paper by its authors. Use permitted under CreativeCommons License Attribution 4.0 International (CC BY 4.0).7 https://ec.europa.eu/growth/single-market/public-procurement_en8 http://standard.open-contracting.org/latest/en/9 http://simap.ted.europa.eu/https://ec.europa.eu/growth/single-market/public-procurement_enhttp://standard.open-contracting.org/latest/en/http://simap.ted.europa.eu/approach has been proposed as a response [1]. For example, several ontologies havebeen developed, such as PPROC ontology [3] for describing public processes andcontracts, LOTED2 ontology [2] for public procurement notices, PCO ontology [4]for contracts in public domain, and MOLDEAS ontology [5] for announcementsabout public tenders. Each of these was developed with different concerns inmind (legal, process-oriented, etc.) without significant adoption so far.To this end, we developed an ontology network for representing and linkingtender and company data and ingested relevant data from two prominent dataproviders into a knowledge graph, called TBFY. In this poster paper, we presentan overview of our knowledge graph for public procurement.2 Knowledge GraphWe integrated two datasets according to an ontology network: tender dataprovided by OpenOpps10 in the OCDS format and company data provided byOpenCorporates11. OpenOpps has gathered over 2M tender documents frommore than 300 publishers through Web scrapping and by using open APIs, whileOpenCorporates currently has 140M entities collected from national registers.2.1 Ontology NetworkWe are currently using two main ontologies. First, an ontology for tender data(see Figure 1) that we developed using the OCDS’ data model12.Fig. 1. A fragment of OCDS ontology depicting some of the key classes.Second, we reused the euBG ontology for company data13. Both ontologies",
        "file_name": "paper14.pdf",
        "file_path": "PDFs\\paper14.pdf"
    },
    {
        "title": "Towards the definition of a language-independent mapping template for knowledge graph creation",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "ABSTRACTThe use of knowledge graphs is spreading in the scientific commu-nity across different domains, from social sciences to biomedicine.The creation of knowledge graphs usually needs the integrationof multiple heterogeneous data sources in different formats andschemas. One common way to achieve this process is using declara-tive mappings, which establish the relationships between the sourcedata and the ontology, improving relevant aspects such as main-tainability, readability and understandability. Learning how to useand create mappings is not an easy task, hindering the use of thistechnology to anyone outside the area. As a result, this task is usu-ally carried out by experts. To ease the mapping creation, severalmapping editors have been developed, but their success is limited.In this paper, we devise the use of a well-known tool commonlyused in the scientific community, the spreadsheets, to specify themapping rules in a language-independent way. Our aim is to easethe mapping creation and make it more accessible for the commu-nity. We also show a real use case, in which using spreadsheetshelps in the mapping creation process and enables a handy way forediting and visualizing mapping rules.CCS CONCEPTS•Computingmethodologies→Artificial intelligence; Knowl-edge representation and reasoning.KEYWORDSKnowledge graph, spreadsheet, declarative mapping1 INTRODUCTIONThe expansion of the Semantic Web technologies has reached usersacross several domains, such as legal and biomedical. An increasingnumber of knowledge graphs from these areas are being created,restructuring knowledge in a machine-readable way [4]. For theirconstruction it is necessary to integrate different data sources; thenthey allow search optimization and the possibility of applying ma-chine learning techniques to obtain new knowledge, among otherpossibilities. Some examples are DBpedia [1] and Wikidata [18].There are multiple approaches to create knowledge graphs, fromusing ad-hoc tools to declarative mappings. The later defines rulesCopyright ©2019 for this paper by its authors. Use permitted under Creative CommonsLicense Attribution 4.0 International (CC BY 4.0).to establish relationships between the global schema and the datasources. Examples of mappings languages are the W3C recommen-dation R2RML [7] and its extension RML [9].The use of declarative mappings for semantic web non-expertsis often complicated. That is one of the reasons why the mappingcreation is usually carried out by knowledge engineers. This posesa barrier for potential users from other domains. To face this issue,several mapping editors have been proposed. They aim at makingthe mapping creation and editing easier and more intuitive [11, 16].Despite these efforts, users prefer to use tools like OpenRefine1,which is non-declarative, thus hindering the reproducibility andmaintainability of the transformations performed.",
        "file_name": "short3.pdf",
        "file_path": "PDFs\\short3.pdf"
    },
    {
        "title": "An Abstract Framework for Non-Cooperative Multi-Agent Planning",
        "implementation_urls": [],
        "doi": "10.3390/app9235180",
        "arxiv": null,
        "abstract": null,
        "file_name": "pdf?version=1575008421",
        "file_path": "PDFs\\pdfversion1575008421.pdf"
    },
    {
        "title": "Towards a Linked Democracy Model",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_4",
        "arxiv": null,
        "abstract": "Abstract In this chapter we lay out the properties of participatory ecosystems aslinked democracy ecosystems. The goal is to provide a conceptual roadmap thathelps us to ground the theoretical foundations for a meso-level, institutional theoryof democracy. The identification of the basic properties of a linked democracyeco-system draws from different empirical examples that, to some extent, exhibitsome of these properties. We then correlate these properties with Ostrom’s designprinciples for the management of common-pool resources (as generalised to groupscooperating and coordinating to achieve shared goals) to open up the question ofhow linked democracy ecosystems can be governed.Keywords Linked democracy � Common-Pool resources4.1 IntroductionIn previous chapters we have suggested that our model of linked democracy can berepresented as a three-layered, overlapping structure of Linked Open Data (LOD),linked platforms, and linked ecosystems. A linked democracy model represents thedistributed interplay between people, digital technologies, and data (see Fig. 4.1).We have also provided examples of digital platforms and ecosystems that exhibit acertain degree of connectedness by tapping on LOD, on open data, or on crowd-sourced data produced elsewhere.Breaking silos down is a common, distinctive feature of the examples we havereviewed. But are there any other properties than we can distill from these exam-ples? Moreover, is it possible to turn those properties into design principles thathelp to orchestrate a linked democracy model? Design principles should guide theimplementation of a linked democracy model; they should also capture the insti-tutional arrangements needed to produce aligned decision making in a givendomain, either local or global. As we have seen with the Icelandic or Mexico Cityexamples, a lack of institutional endorsement of carefully designed participatory© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_475http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_4&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_4outcomes can bring crowdsourced constitutional processes to a deadlock. Linkeddemocracy is about finding ways out of locked democracy.We are fully aware that generalizing specific design principles for the efficientfunctioning of a linked democracy would require an exhaustive, large-scale surveyof case studies. We have examined some illustrative examples in the previouschapters, but this falls short of providing a comprehensive panorama. Therefore, inthis chapter we will first identify some distinctive properties of a linked democracymodel based on our previous examples. Second, we will map these properties ontothe well-established set of design principles that Elinor Ostrom identified asenabling effective management of ‘common-pool resources’ (CPR) groups (Ostrom1990, 90–102). Recently, David Wilson et al. reviewed Ostrom’s principles from anevolutionary perspective to argue that they ‘have a wider range of application thanCPR groups and are relevant to nearly any situation where people must cooperateand coordinate to achieve shared goals’ (Wilson et al. 2013, 522). We considerlinked democracy ecosystems to be one of those situations involving cooperation—in performing a wide range of tasks—and coordination—of large groups of indi-",
        "file_name": "10.1007%2F978-3-030-13363-4_4.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_4.pdf"
    },
    {
        "title": "Deliberative and Epistemic Approaches to Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_2",
        "arxiv": null,
        "abstract": "Abstract Deliberative and epistemic approaches to democracy are two importantdimensions of contemporary democratic theory. This chapter studies thesedimensions in the emerging ecosystem of civic and political participation tools, andappraises their collective value in a new distinct concept: linked democracy. Linkeddemocracy is the distributed, technology-supported collective decision-makingprocess, where data, information and knowledge are connected and shared bycitizens online. Innovation and learning are two key elements of Atheniandemocracies which can be facilitated by the new digital technologies, and across-disciplinary research involving computational scientists and democratic the-orists can lead to new theoretical insights of democracy.Keywords Deliberative democracy � Epistemic democracy � Semantic web �Institutions � Participatory ecosystems2.1 IntroductionSemantic Web engineers have often complained that building ontologies is hard. Tobuild an ontology for a given domain—for example, tort law—one needs to recruitexperts in that domain, elicit their legal knowledge, and then reach a shared, explicitconsensus of how such legal knowledge will be represented and formalised so thatcomputers can ‘understand it’. It is not an easy task, indeed, especially if ontologieshave to be designed from scratch and the subject matter is complex.If it is hard to build ontologies, mapping the conceptual domain of deliberativeand epistemic theories of democracy is not less harder. In fact, it is quite theopposite. In the last thirty years, political philosophers and scientists have producedan oceanic body of literature on the justification, mechanisms, and outcomes ofdemocracy based on a number of procedural and cognitive arguments. They havedone so at different levels: normative (discussing the foundational values), theo-retical (formulating hypothesis), and empirical (developing case studies and testingnew institutional arrangements). Successive generations of scholars have expanded,© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_227http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_2&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_2refined, or remixed their different approaches with extraordinary sophistication. Asa result, any attempt to represent the domain of contemporary models of democracywill necessarily be limited and selective. Like the making of the 19th centuryOxford English Dictionary, or the 21st century Wikipedia, the effort would requirethe involvement of hundreds if not thousands of dedicated volunteers.This chapter will take an oblique route by briefly considering the debates indemocracy theory over the last decades that have explored the meaning and practiceof democratic participation. The discussions about the role of citizen participationare sometimes structured into a binary between ‘procedural’ and ‘epistemic’accounts of democratic practice, or, with a different terminology, between ‘ma-joritarian’ and ‘populist’ approaches. Hélène Landemore has proposed a moreexpressive dichotomy: the ‘talkers’ and the ‘counters’ (Landemore 2013, 53).1 The‘talkers’ walk the path of ‘deliberation followed by majority rule as a fallible butoverall reliable way to make collective decisions’; the ‘counters’ explore ‘theepistemic properties of judgement aggregation when large groups of people are",
        "file_name": "10.1007%2F978-3-030-13363-4_2.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_2.pdf"
    },
    {
        "title": "Linked Democracy Foundations, Tools, and Applications Introduction to Linked Data",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_1",
        "arxiv": null,
        "abstract": "Abstract This chapter presents Linked Data, a new form of distributed data on theweb which is especially suitable to be manipulated by machines and to shareknowledge. By adopting the linked data publication paradigm, anybody can publishdata on the web, relate it to data resources published by others and run artificialintelligence algorithms in a smooth manner. Open linked data resources maydemocratize the future access to knowledge by the mass of internet users, eitherdirectly or mediated through algorithms. Governments have enthusiasticallyadopted these ideas, which is in harmony with the broader open data movement.Keywords Linked data � Semantic web � Democracy � Ontologies � Knowledgerepresentation � eDemocracy1.1 IntroductionMore than half of the world’s population has access to the Internet. Vast amounts ofknowledge accumulated in roughly 2 billion websites are available to anyone whois able to read and can afford an internet connection.Entertainment habits, interpersonal human relations and almost any conceivableaspect of human life have been profoundly transformed with the arrival of theinternet. Yet modern democracies have remained relatively unaffected. It is true thatpropaganda techniques have undergone changes, political parties organize theircampaign strategies differently and the idea of eDemocracy is perhaps about tohatch; but the public institutions, the habits of citizens and the overall political gameare all apparently the same.We have to indulge—Internet is a new thing. But a careful observation of theevolution of technologies and the new organizational forms they enable revealdiscrete signs of change, now with little effect but potentially of much impact.This chapter introduces some new technologies and ideas which may seemirrelevant today, but which will probably exert a powerful influence on the forth-coming transformations of the concept of democracy.© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_11http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_1&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_11.2 The World Wide Web as a Source of Dataand Knowledge1.2.1 Data, Information and KnowledgeMarshall McLuhan described technology as extensions of man (McLuhan 1964),whereby our bodies and our senses are extended beyond their natural limits.Certainly, a shovel is an improvement of our hands when we dig a trench andtelescopes are augmented eyes when we look at the stars. In top level chess tour-naments, chess players prepare their games and study their opponents with a jointteam of humans and machine—machines also extend human’s capabilities forthinking.In order to make a value judgement, we need data—this is a truism. But todaywe also need machines which need data. Whenever we take an important decision,we usually google for some related information. Our decisions are mediated byinformation provided by a company, or a handful of companies, whose interestsmay not match our interests. Maybe in the future we will have a wider range of",
        "file_name": "10.1007%2F978-3-030-13363-4_1.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_1.pdf"
    },
    {
        "title": "Legal Linked Data Ecosystems and the Rule of Law",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_5",
        "arxiv": null,
        "abstract": "Abstract This chapter introduces the notions of meta-rule of law and socio-legalecosystems to both foster and regulate linked democracy. It explores the way ofstimulating innovative regulations and building a regulatory quadrant for the rule oflaw. The chapter summarises briefly (i) the notions of responsive, better and smartregulation; (ii) requirements for legal interchange languages (legal interoperability);(iii) and cognitive ecology approaches. It shows how the protections of the sub-stantive rule of law can be embedded into the semantic languages of the web of dataand reflects on the conditions that make possible their enactment and implemen-tation as a socio-legal ecosystem. The chapter suggests in the end a reusablemulti-levelled meta-model and four notions of legal validity: positive, composite,formal, and ecological.Keywords Web of data � Socio-legal ecosystem � Rule of law �Meta-rule of law �Semantic languages � Governance � Linked democracy � Semantic web regulatorymodels � Regulatory quadrant � Legal validity5.1 Introduction: The Rule of Law in a New Brave WorldWewill expand in this chapter someways of implementing linked democracy on legaland political bases. Linked democracy is not only a theoretical approach incorpo-rating open linked data to theories of democracy. It consists of practices and the realbehaviour of people exercising their political rights on everyday bases. Thus, it alsopossesses a personal and cultural dimension that should be valued and protected. Lawis an obvious element. Behaviour on the web should be ‘fair’ and ‘legal’. What does itmean? Different states have different jurisdictions, and despite the international trendsof the global market, law has been, and still is, dependent on national states.How could we incorporate regulatory forms of empowering people on the web?How could algorithmic governance, data analytics, and semantics be used tofoster the principles of linked democracy that we have just presented at the end ofChap. 4?© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_587http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_5&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_5We will contend that there are two ways to reach such objectives: (i) embeddingthe principles of the substantive rule of law into the web of linked data (what wewill call the meta-rule of law), and (ii) incentivising the creation of socio-legalecosystems, i.e. the social conditions that are required to implement the meta-rule oflaw online and outline them among all stakeholders and users.We admit that this can be easier said than done. These two objectives might havean idealistic flavour. A few corporations have a dominant position on the web, theycan trade and invade privacy, and they usually do. As Shadbolt and Hampson(2018) have nicely put it, we live in a hyper-complex environment, shaped by ourown tools. This is a good breeding ground for elites to thrive. They also point outthat “what has changed is human potential, thanks to our transformative new tools.[…] The point is not that machines might wrest control from the elites. The problemis that most of us might never be able to wrest control of the machines from thepeople that occupy the command posts” (Shadbolt and Hampson 2018, 63).Power is certainly a problem. In our hyper-connected world, we barely know in",
        "file_name": "10.1007%2F978-3-030-13363-4_5.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_5.pdf"
    },
    {
        "title": "Multilayered Linked Democracy",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_3",
        "arxiv": null,
        "abstract": "Abstract Although confidence in democracy to tackle societal problems is falling,new civic participation tools are appearing supported by modern ICT technologies.These tools implicitly assume different views on democracy and citizenship whichhave not been fully analysed, but their main fault is their isolated operation innon-communicated silos. We can conceive public knowledge, like in Karl Popper’sWorld 3, as distributed and connected in different layers and by different connec-tors, much as it happens with the information in the web or the data in the linkeddata cloud. The interaction between people, technology and data is still to bedefined before alternative institutions are founded, but the so called linkeddemocracy should rest on different layers of interaction: linked data, linked plat-forms and linked ecosystems; a robust connectivity between democratic institutionsis fundamental in order to enhance the way knowledge circulates and collectivedecisions are made.Keywords Linked democracy � Multilayered linked democracy � Linked data �Linked platforms � Linked ecosystems � World 3 � Institutions3.1 IntroductionContemporary democracies face growing scepticism about their capacity to managecomplex societal problems. Financial crises, inequality and poverty, climate changeand armed conflicts routinely test the resilience of our democratic systems.Researchers are predominantly expressing concern about the developments of thelast decade. Larry Diamond draws from Freedom House data to argue that we are ina ‘mild but protracted democratic recession’ since 2006 (Diamond 2015, 144).Roberto Foa and Yascha Mounk analyse World Values Surveys to conclude thatcitizens in Western democracies have ‘become more cynical about the value of© The Author(s) 2019M. Poblet et al., Linked Democracy, SpringerBriefs in Law,https://doi.org/10.1007/978-3-030-13363-4_351http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttp://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-13363-4_3&amp;domain=pdfhttps://doi.org/10.1007/978-3-030-13363-4_3democracy as a political system, less hopeful that anything they do might influencepublic policy, and more willing to express support for authoritarian alternatives’(Foa and Mounk 2016, 7). John Boik et al. warn that traditional democratic insti-tutions are failing and that ‘the versions of democracy attempted by newlydemocratizing nations have been even less effective’ (Boik et al. 2015). Globally,voter turnout—a standard proxy to measure citizens’ satisfaction with democraticinstitutions—has been steadily but consistently declining since the 1960s (IDEAInternational 2016).This sceptical outlook coexists with some unprecedented technology trends: by2020, about 1.7 megabytes of new information will be created every second, forevery human being (Forbes 2015); there will be more mobile phone subscriptionsthan people on the planet and more than 6 billion of these devices will be smart-phones (ITU 2015). Digital technologies not only disrupt business models, theynow shape the way we access information, knowledge, and increasingly, the waywe exercise our rights. In doing so, they also transform civic action and enable newforms of citizenship.Political science, media and culture studies, and ICT disciplines have alreadyproduced a vast literature on civic participation online (e.g., see meta-analysis by",
        "file_name": "10.1007%2F978-3-030-13363-4_3.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_3.pdf"
    },
    {
        "title": "Linked Democracy Foundations, Tools, and Applications Conclusion",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-13363-4_6",
        "arxiv": null,
        "abstract": null,
        "file_name": "10.1007%2F978-3-030-13363-4_6.pdf",
        "file_path": "PDFs\\10.1007%2F978-3-030-13363-4_6.pdf"
    },
    {
        "title": "Introduction to the Special Issue Artificial Intelligence Knowledge Representation",
        "implementation_urls": [],
        "doi": "10.3390/systems7030035",
        "arxiv": null,
        "abstract": null,
        "file_name": "pdf?version=1563965423",
        "file_path": "PDFs\\pdfversion1563965423.pdf"
    },
    {
        "title": "Onett: Systematic knowledge graph generation for national access points",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. The Spanish DBpedia is a data source used initially to sup-port the Spanish community. However, our logs show that the Spanishlanguage goes beyond Spanish speakers and many non-Spanish speakersuse the Spanish DBpedia on a daily basis. In the last months we havemade two important enhancements to the Spanish DBpedia: (1) we pub-lish a nonstandard dataset containing the type of resources that in thestandard distribution have no type, and (2) we update automatically ourdata every week by using the DBpedia databus. In this way, we satisfya frequent request made by companies and we foster the usage of theSpanish language, the second mother language by the number of speak-ers (after Chinese), and the second in scientific papers (after English).Keywords: Spanish DBpedia · Resource type · DBpedia data bus.1 Introduction1.1 The rising of the Spanish languageThe data published by the Cervantes Institute in its 2020 report [4] are over-whelming: Spanish speakers have increased by 30% in the last decade, and thenumber of foreigners who study it has grown by 60%. More than 585 millionpeople speak Spanish. Of these, almost 489 million are native Spanish speakers.Furthermore, Spanish is the second mother tongue by number of speakers afterMandarin Chinese, and the third language in the global count of users after En-glish and Mandarin Chinese. On the Internet, it is the third most used and isthe second language, behind English, publishing scientific texts.The DBpedia project has long generated semantic information from EnglishWikipedia. Since June 2011, the information generation process has extractedinformation from Wikipedia in 111 of its languages, but only 18 languages have aDBpedia chapter with a website. One of them is Spanish. The DBpedia Interna-tionalization Committee has assigned a website and a SPARQL [7] endpoint for? Partially supported by HcommonK (RTC2019-007134-7) and Datos4.0 (TIN2013-46238-C4-3-R) projects.Copyright ©2021 for this paper by its authors. Use permitted under Creative Com-mons License Attribution 4.0 International (CC BY 4.0)2 S. Sanz-Lucio et al.each of these languages1. In the case of Spanish (with website es.dbpedia.org),the extraction process produces more than 100 million RDF triples from theSpanish Wikipedia. All these triples are available on the SPARQL endpointes.dbpedia.org/sparql using Semantic Web [2] and Linked Data [1] technolo-gies.1.2 The DBpedia datasetsAs we have mentioned previously, DBpedia extracts data from 111 differentlanguage editions of Wikipedia. Then, for each language we have a knowledgebase (a “Knowledge Graph” in modern terminology, abbreviated as KG). Thelargest DBpedia KG is extracted from the English edition of Wikipedia, witharound 400 million facts (triples) that describe 3.7 million resources (Wikipediaentries). The DBpedia knowledge graphs that are extracted from the other 110Wikipedia editions together consist of 1.46 billion facts and describe 10 millionadditional resources. Therefore, two-thirds of the information in DBpedia comesfrom non-English Wikipedias.From a technical perspective, the DBpedia project maps Wikipedia infoboxes [12]from 27 different language editions into the DBpedia ontology, a single shared on-tology consisting of 320 classes and 1,650 properties. The mappings are created",
        "file_name": "paper8.pdf",
        "file_path": "PDFs\\paper8.pdf"
    },
    {
        "title": "Themis: A tool for validating ontologies through requirements",
        "implementation_urls": [],
        "doi": "10.18293/SEKE2019-117",
        "arxiv": null,
        "abstract": "Abstract— The validation of ontologies, whose aim is to checkwhether an ontology matches the conceptualization it is meantto specify, is a key activity for guaranteeing the quality ofontologies. This work is focused on the validation throughrequirements, with the aim of assuring, both the domainexperts and ontology developers, that the ontologies they arebuilding or using are complete regarding their needs. Inspiredby software engineering testing processes, this work proposesa web-based tool called Themis, independent of any ontologydevelopment environment, for validating ontologies by meansof the application of test expressions which, following lexico-syntactic patterns, represent the desired behaviour that willpresent an ontology if a requirement is satisfied.I. INTRODUCTIONIn software engineering it is inconceivable to deliver asoftware product without its pertinent tests which guaranteethat it fulfills all its requirements. Besides, there are severalapproaches integrated into the software development processwhose aim is to test the software. Unit testing [1], whichvalidates that each unit of the software performs as designed,and behaviour-driven development [2], which focuses onthe behaviour the software product is implementing, areexamples of these approaches. Moreover, there are specificsyntaxes, such as Gherkin,1 which generate unambiguousspecifications of software to automate the testing process.However, in ontology engineering there is a lack of clearlydefined testing processes in order to be able to ascertainwhether an ontology satisfies its functional requirements [3],which state the particular knowledge that should be repre-sented. Such ontological requirements used to be writtenin form of competency questions [4] or natural languagesentences. The main issue when performing testing processesin the ontology engineering field is the ambiguity of theontological requirements, which sometimes are difficult toformalize into tests and to translate into axioms. Therefore,inspired by software engineering and its specific syntax forthe definition of tests, we propose Themis,2 a tool whichprovides a set of test expressions based on lexico-syntacticpatterns (LSPs) related to ontological requirements. TheseLSPs allows to relate different types of requirements withthe axioms needed to implement them in an ontology, andsuch implementations are used by Themis to identify whethera requirement is satisfied.DOI reference number: 10.18293/SEKE2019-1711https://docs.cucumber.io/gherkin2http://themis.linkeddata.esThemis can be used by both domain experts and ontologydevelopers to validate ontologies regarding their functionalrequirements. Other type of requirements, such as non-functional ones (e.g., “the ontology URIs must be in En-",
        "file_name": "seke2019-117",
        "file_path": "PDFs\\seke2019-117.pdf"
    },
    {
        "title": "Morph-GraphQL: GraphQL servers generation from R2RML mappings (SESE)",
        "implementation_urls": [
            {
                "identifier": "https://github.com/oeg-upm/morph-graphql",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\seke2019-055.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Currently, it 4https://github.com/oeg-upm/morph-graphql, deployed at http://graphql."
                    }
                ]
            }
        ],
        "doi": "10.18293/SEKE2019-055",
        "arxiv": null,
        "abstract": "Abstract—REST has become in the last decade the mostcommon manner to provide web services, yet it was not originallydesigned to handle typical modern applications (e.g., mobileapps). GraphQL was released publicly in 2015 and since thenhas gained momentum as an alternative approach to REST.However, generating and maintaining GraphQL resolvers is noteasy. First, a domain expert has to analyse a dataset, designthe corresponding GraphQL schema and map the dataset to theschema. Then, a software engineer (e.g., GraphQL developer)implements the corresponding GraphQL resolvers in a specificprogramming language. In this paper we present an approachthat generates GraphQL resolvers from declarative mappingsspecification in the W3C Recommendation R2RML, hence, canbe used both by a domain expert as without the need toinvolve software developers to implement the resolvers, and bysoftware developers as the initial version of the resolvers to beimplemented. Our approach is implemented in morph-GraphQL.Index Terms—GraphQL, R2RML, OBDAI. INTRODUCTIONIntroduced in 2000, Representational State Transfer (REST)has become the most common manner to provide web servicesin the last few years. Those web services that conform tothe REST principles, known as RESTful web services, useHTTP/S and its operations to make requests to the underlyingserver, such as GET to retrieve objects, POST to add objects,PUT to modify objects and DELETE to remove objects,among others.Over the years, the complexity of modern software concepthas evolved since the inception of REST. For example, typicalmobile applications have to take into account aspects thatreceive little attention in traditional applications, such as thesize of data being exchanged/transmitted and the number ofAPI calls being made. These aspects are relevant to the prob-lem known as over-fetching and under-fetching. Over-fetchingrefers to the situation in which a REST endpoint returns moreDOI reference number: 10.18293/SEKE2019-055data than what is required by the developer. For example, adeveloper may need some information about the name of a userso she hits the corresponding endpoint (/user). However,the endpoint may return information that is not needed by theclient, such as birth date and address. The opposite also raises aproblem, which is having the REST endpoint provide less datathan required. Such a case is called under-fetching. It refers tothe situation in which a single REST endpoint does not providesufficient information requested by the client. For example, inorder to obtain the names of all friends of a particular user,typically two endpoints may be needed: the first is the endpointthat returns the identifiers of all the friends (/friends), andthe second is the one that returns the details of each of thefriends based on the identifier (/user).",
        "file_name": "seke2019-055",
        "file_path": "PDFs\\seke2019-055.pdf"
    },
    {
        "title": "Optimizing automated term extraction for terminological saturation measurement",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Assessing the completeness of a document collection, within a domain of interest, is a complicated task that requires substantial effort. Even if an auto-mated technique is used, for example, terminology saturation measurement based on automated term extraction, run times grow quite quickly with the size of the input text. In this paper, we address this issue and propose an optimized approach based on partitioning the collection of documents in disjoint constituents and computing the required term candidate ranks (using the c-value method) inde-pendently with subsequent merge of the partial bags of extracted terms. It is proven in the paper that such an approach is formally correct – the total c-values can be represented as the sums of the partial c-values. The approach is also vali-dated experimentally and yields encouraging results in terms of the decrease  of the necessary run time and straightforward parallelization without any loss  in quality.  Keywords: Automated term extraction, terminological saturation, partial  c-value, merged-partial c-value, optimization 1 Introduction Ontology learning from texts is a developing research field that aims to extract domain description theories from text corpora. It is increasingly acknowledged as a plausible alternative to ontology development based on the interviews of domain knowledge stakeholders. One shortcoming of learning an ontology from texts is that the input cor-pus has to be quite big for being representative for the subject domain. Another short-coming is that learning ontologies from text is expensive, in terms of taken time, as it involves the use of several algorithms, in a pipeline [1], that are computationally hard.   https://orcid.org/0000-0001-6157-8111https://orcid.org/0000-0002-5159-254Xmailto:gen.dobr@gmail.commailto:vadim@ermolayev.commailto:dchaves@fi.upm.esmailto:egorfedorencko@gmail.comAutomated term extraction (ATE) is an essential step at the beginning of the pipeline for ontology learning [1, 2], that is known to be bulky in terms of the increase of the run time with the growth of the input text corpus. Therefore, finding a way to reduce: (i) either the size of the processed text; or (ii) the time spent for term extraction; or (iii) both is of importance.  In our prior work [2, 3, 4, 5], we developed the ATE-based approach (OntoElect) that helps circumscribe the minimal possible representative part of a documents collec-tion, which forms the corpus for further ontology learning. This technique is based on measuring terminological saturation in the collection of documents, which is computa-tionally quite expensive in the terms of the run time.  In this paper, we present the approach, based on the partitioning of a document col-lection, which allows substantially reducing ATE run time in the OntoElect processing pipeline.  The remainder of the paper is structured as follows. In Sect. 2, we outline our Onto-Elect approach to detect terminological saturation in document collections describing a subject domain. In Sect. 3, we review the related work in ATE and argue for the choice of the c-value method as the best appropriate for measuring terminological saturation. In Sect. 4, we explain our motives to optimize the c-value method based on partitioning a document collection and present a formal framework for that. Section 5 reports on the setup and results of our experimental evaluation of the proposed optimization approach. Finally, we draw the conclusions and outline our plans for the future work in Sect. 6.  ",
        "file_name": "20190001.pdf",
        "file_path": "PDFs\\20190001.pdf"
    },
    {
        "title": "Supervising Attention in an E-Learning System",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-01746-0_46",
        "arxiv": null,
        "abstract": "Abstract. Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. First, the worst aspect about this approach is that it only points out a potential decrease of attention after a productivity loss. An approach that could point out, in advance, upcoming breaks in attention could allow active/preventive interventions rather than reactive ones. In this paper we present a distributed system for monitoring attention in teams (of people). It is especially suit-ed for people working with computers and it can be interesting for domains such as the workplace or the classroom. It constantly analyz-es the behavior of the user while interacting with the computer and together with knowledge about the task, is able to temporally classify attention. Keywords: Distributed Intelligent System, Attention, Behavioral Biometrics. 1 Introduction In the field of computer science, an intelligent environment is a digitally augmented physical world where sensor-enabled and networked devices work continuously and collaboratively to make the lives of the inhabitants more comfortable. With this techno-logical evolution, job offers have changed, bringing along many significant and broad changes. Some of the most notorious ones can be pointed out by the emergence of indicators such as attentiveness which, in extreme cases, can compromise the life and well-being of the workers. In more moderate cases it will impair attention, general cognitive skills and productivity. In addition to these factors, many of these jobs are the so-called desk-jobs, in which people frequently sit for more than 8 hours [1]. Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. While the true nature of this relationship is yet to be thoroughly studied (properly contextualized in each work domain), there are other issues that need to be addressed. First, the worst aspect about this approach is that it only points out a potential decrease of attention after a productivity loss. This means that the “damage” is already done and that it is most  likely too late for the worker to cope with whatever caused the attention loss. An approach that could point out, in advance, upcoming breaks in attention (e.g. through the observation of behavioral patterns) could allow active/preventive interventions rather than reactive ones [2]. In this paper we present a distributed system for monitoring attention in teams (of people), in line with the vision of intelligent environments [3]. It is especially suited for people working with computers and it can be interesting for domains such as the workplace or the classroom. It constantly analyzes the behavior of the user while interacting with the computer and, together with knowledge about the task, is able to temporally classify attention. This work may be very interesting for team managers to assess the level of attention of their teams, identifying potentially dis-tracting events, hours or individuals. Moreover, distraction often appears when the individual is fatigued, bored or not motivated. This tool can thus be an important indicator of the team, allowing the manager to act accordingly at an individual or group level. In the overall, this tool will support the implementation of better human resources management strategies. 1.1 Previous Work Part of this framework was implemented in previous work. The first version focused on the analysis of the individuals’ interaction patterns with the computer, including features such as mouse velocity or acceleration, click duration, typing speed or rhythm, among others. For a complete list of features as well as the process of their acquisition and extraction, please see [4]. However, a limitation was also identified in this previous work. In fact, a user that opened a no work-related application and did not interact anymore with the computer until the end of the task had 0% of attention. On the other hand, if the user opens a work-related appli-cation and does not interact with the computer after that, the user's attention will be classified as 100% when he is most likely not even at the computer. The present work adds a new feature to this previously existing framework, by providing a precise measure of attention based not on the tasks work-related patterns but also on the key typing or mouse movement pattern. It thus constitutes a much more precise and reliable mechanism for attention monitoring, while maintaining all the advantages of the existing system: nonintru-sive, lightweight and transparent. mailto:d.alves@alumnos.upm.esmailto:pjon@di.uminho.pt2 2 Architecture From the architecture of the developed environment described in Figure 1 it is possible to collect data that describe the interaction with both the mouse and the keyboard in the devices in which students work. Three parts compose the module’s architecture.  In the devices operating by the students, it’s installed software that generates raw data, which store locally until it is synchro-nized with the web server in the cloud. In this step it’s encodes each event with the corresponding necessary information (e.g. ",
        "file_name": "10!1007%978-3-030-01746-0_46.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-01746-0_46.pdf"
    },
    {
        "title": "Towards a Knowledge Graph Based Platform for Public Procurement",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-14401-2_29",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1007%978-3-030-14401-2_29.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-14401-2_29.pdf"
    },
    {
        "title": "Conformance Test Cases for the RDF Mapping Language (RML)",
        "implementation_urls": [
            {
                "identifier": "https://github.com/rmlio/rml-test-cases",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-21395-4_12.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The test cases are described at http://rml.io/test-cases/and the corresponding files are available at https://github.com/rmlio/rml-test-cases."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-21395-4_12",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often generated using rules that applysemantic annotations to data sources. Software tools then execute theserules and generate or virtualize the corresponding RDF-based knowl-edge graph. RML is an extension of the W3C-recommended R2RMLlanguage, extending support from relational databases to other datasources, such as data in CSV, XML, and JSON format. As part of theR2RML standardization process, a set of test cases was created to assesstool conformance the specification. In this work, we generated an initialset of reusable test cases to assess RML conformance. These test casesare based on R2RML test cases and can be used by any tool, regard-less of the programming language. We tested the conformance of twoRML processors: the RMLMapper and CARML. The results show thatthe RMLMapper passes all CSV, XML, and JSON test cases, and mosttest cases for relational databases. CARML passes most CSV, XML,and JSON test cases regarding. Developers can determine the degree ofconformance of their tools, and users determine based on conformanceresults to determine the most suitable tool for their use cases.Keywords: RML · R2RML · test case1 IntroductionKnowledge graphs are often generated based on rules that apply semantic an-notations to raw or semi-structured data. For example, the DBpedia knowledgegraph is generated by applying classes and predicates of the DBpedia ontologyto Wikipedia [1]. Software tools execute these rules and generate correspondingRDF triples and quads [2], which materialize knowledge graphs. In the past,custom scripts prevailed, but lately, rule-driven tools emerged. Such tools distin-guish the rules that define how RDF terms and triples are generated from thetool that executes those rules. R2RML [3] is the W3C-recommended languageto define such rules for generating knowledge graphs from data in relationaldatabases (RDBs). An R2RML processor is a tool that, given a set of R2RMLrules and a relational database, generates an RDF dataset. Examples of R2RMLprocessors include Ultrawrap [4], Morph-RDB [5], Ontop [6], and XSPARQL [7].A subset of them was included in the RDB2RDF Implementation Report [8]which lists their conformance to the R2RML specification. Conformance is as-sessed based on whether the correct knowledge graph is generated for a set ofrules and certain relational database.Given that R2RML is focused on relational databases only, extensions andadaptations were applied to account for other types of data sources. These in-clude RML [9], XSPARQL [7], and xR2RML [10]. RML provides an extension ofR2RML to support heterogeneous data sources, including different formats suchas CSV, XML, JSON, and access interfaces, such as files and Web APIs. VariousRML processors emerged, such as the RMLMapper3, CARML4, GeoTriples [11],and Ontario5. Unlike R2RML, there are no test cases available to determine theconformance to the RML specification. As a result, processors are either nottested or only tested with custom test cases, which do not necessarily assess everyaspect of the specification. Consequently, no implementation report is availablethat allows comparing the different processors that generate knowledge graphsfrom heterogeneous data sources based on the conformance to the specification.This way, it is hard to determine the most suitable processor for a certain usecase.In this work, we introduce an initial set of RML test cases, which contains 297",
        "file_name": "10!1007%978-3-030-21395-4_12.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-21395-4_12.pdf"
    },
    {
        "title": "CORAL: A Corpus of Ontological Requirements Annotated with Lexico-Syntactic Patterns",
        "implementation_urls": [
            {
                "identifier": "https://github.com/CQ2SPARQLOWL/",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1007%978-3-030-21348-0_29.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "the dataset presented by Wisniewski et al.8 provides a corpus of 234 compe-tency questions related to 5 different ontologies9."
                    }
                ]
            },
            {
                "identifier": "https://doi.org/10.5281/zenodo.1967306",
                "type": "zenodo",
                "paper_frequency": 4,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-030-21348-0_29",
        "arxiv": null,
        "abstract": "Abstract. Ontological requirements play a key role in ontology devel-opment as they determine the knowledge that needs to be modelled. Inaddition, the analysis of such requirements can be used (a) to improveontology testing by easing the automation of requirements into tests; (b)to improve the requirements specification activity; or (c) to ease ontol-ogy reuse by facilitating the identification of patterns. However, thereis a lack of openly available ontological requirements published togetherwith their associated ontologies, which hinders such analysis. Therefore,in this work we present CORAL (Corpus of Ontological RequirementsAnnotated with Lexico-syntactic patterns), an openly available corpus of834 ontological requirements annotated and 29 lexico-syntactic patterns,from which 12 are proposed in this work. CORAL is openly availablein three different open formats, namely, HTML, CSV and RDF under“Creative Commons Attribution 4.0 International” license.Keywords: Corpus · Linked data · Ontological requirements ·Lexico-syntactic patternsResource type: DatasetDOI: https://doi.org/10.5281/zenodo.1967306URL: http://coralcorpus.linkeddata.es1 IntroductionIn recent years, the definition of functional ontological requirements [3,17,18],which represent the needs that the ontology to be built should cover, and theirautomatic formalization into axioms or tests (e.g., [5,13,19]) have been studied.This work is partially supported by the H2020 project VICINITY: Open virtual neigh-bourhood network to connect intelligent buildings and smart objects (H2020-688467),by ETSI Specialist Task Force 534, and by a Predoctoral grant from the I+D+i pro-gram of the Universidad Politécnica de Madrid. The authors want to thank Agnieszka�Lawrynowicz and her team for helping in the collection of ontological requirements.c© Springer Nature Switzerland AG 2019P. Hitzler et al. (Eds.): ESWC 2019, LNCS 11503, pp. 443–458, 2019.https://doi.org/10.1007/978-3-030-21348-0_29http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-030-21348-0_29&domain=pdfhttps://doi.org/10.5281/zenodo.1967306http://coralcorpus.linkeddata.eshttps://doi.org/10.1007/978-3-030-21348-0_29444 A. Fernández-Izquierdo et al.The aim of these studies is to reduce the time consumed by ontology develop-ers during the ontology verification activity, in which the ontology is comparedagainst the ontological requirements, thus ensuring that the ontology is builtcorrectly [16]. Functional ontological requirements can be written in the form ofcompetency questions, which are natural language questions that the ontologyto be modelled should be able to answer, or as statements, which are sentencesthat determine what should be built in the ontology.However, to accurately define ontological requirements is not a trivial taskand, therefore, neither is their automatic translation into a formal language. Dueto the fact that some requirements are ambiguous [9] or vague, their transfor-mation into axioms or tests is not usually direct and, consequently, it is verydifficult to automate such translation. The analysis of these functional onto-logical requirements can be used in several domains, e.g., to improve ontologytesting by easing the automation of requirements into tests, to improve the",
        "file_name": "10!1007%978-3-030-21348-0_29.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-21348-0_29.pdf"
    },
    {
        "title": "Best practices for publishing, retrieving, and using spatial data on the web",
        "implementation_urls": [
            {
                "identifier": "https://github.com/geo4web-testbed/general",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-180305.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "van den Brink et al./Best Practices for Publishing, Retrieving, and Using Spatial Data on the Web 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-180305",
        "arxiv": null,
        "abstract": "Abstract. Data owners are creating an ever richer set of information resources online, and these are being used for more and moreapplications. Spatial data on the Web is becoming ubiquitous and voluminous with the rapid growth of location-based services,spatial technologies, dynamic location-based data and services published by different organizations. However, the heterogeneityand the peculiarities of spatial data, such as the use of different coordinate reference systems, make it difficult for data users,Web applications, and services to discover, interpret and use the information in the large and distributed system that is the Web.To make spatial data more effectively available, this paper summarizes the work of the joint W3C/OGC Working Group onSpatial Data on the Web that identifies 14 best practices for publishing spatial data on the Web. The paper extends that work bypresenting the identified challenges and rationale for selection of the recommended best practices, framed by the set of principlesthat guided the selection. It describes best practices that are employed to enable publishing, discovery and retrieving (querying)spatial data on the Web, and identifies some areas where a best practice has not yet emerged.Keywords: Geographic information systems, Spatial data, Web technologies, World Wide Web, W3C, Open GeospatialConsortium, OGC*Corresponding author. E-mail: l.vandenbrink@geonovum.nl.**The views expressed are purely those of the author and may notin any circumstances be regarded as stating an official position of theEuropean Commission.1570-0844/0-1900/$35.00 c© 0 – IOS Press and the authors. All rights reservedmailto:l.vandenbrink@geonovum.nl2 L. van den Brink et al. / Best Practices for Publishing, Retrieving, and Using Spatial Data on the Web1. IntroductionSpatial data is important. Firstly, because it has be-come ubiquitous with the explosive growth in position-ing technologies attached to mobile vehicles, portabledevices, and autonomous systems. Secondly, becauseit is fundamentally useful for countless convenientconsumer services like transport planning, or for solv-ing the biggest global challenges like climate changeadaptation [1]. Historically, sourcing, managing andusing high-quality spatial data has largely been thepreserve of military, government and scientific enter-prises. These groups have long recognized the im-portance and value that can be obtained by sharingtheir own specialized data with others to achieve cross-theme interoperability, increased usability and betterspatial awareness, but they have struggled to achievethe cross-community uptake they would like. SpatialData Infrastructures (SDIs) [2], which commonly em-ploy the mature representation and access standards ofthe Open Geospatial Consortium (OGC), are now welldeveloped, but have become a part of the “deep Web”that is hidden for most Web search engines and humaninformation-seekers. Even geospatial experts still donot know where to start looking for what they need orhow to use it when they find it. The integration of spa-tial data from different sources offers possibilities toinfer and gain new information; however, spatial dataon the Web is published in various structures, formatsand with different granularities. This makes publish-ing, discovering, retrieving, and interpreting the spatialdata on the Web a challenging task. By contrast, the",
        "file_name": "10!3233%sw-180305.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-180305.pdf"
    },
    {
        "title": "The modular SSN ontology: A joint W3C and OGC standard specifying the semantics of sensors, observations, sampling, and actuation",
        "implementation_urls": [
            {
                "identifier": "https://github.com/w3c/sdw",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-180320.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The SSN ontology is available at http://www.w3.org/ns/ssn/."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-180320",
        "arxiv": null,
        "abstract": "Abstract. The joint W3C (World Wide Web Consortium) and OGC (Open Geospatial Consortium) Spatial Data on the Web(SDW) Working Group developed a set of ontologies to describe sensors, actuators, samplers as well as their observations,actuation, and sampling activities. The ontologies have been published both as a W3C recommendation and as an OGC im-plementation standard. The set includes a lightweight core module called SOSA (Sensor, Observation, Sampler, and Actuator)available at: http://www.w3.org/ns/sosa/, and a more expressive extension module called SSN (Semantic Sensor Network) avail-able at: http://www.w3.org/ns/ssn/. Together they describe systems of sensors and actuators, observations, the used procedures,the subjects and their properties being observed or acted upon, samples and the process of sampling, and so forth. The set ofontologies adopts a modular architecture with SOSA as a self-contained core that is extended by SSN and other modules toadd expressivity and breadth. The SOSA/SSN ontologies are able to support a wide range of applications and use cases, includ-ing satellite imagery, large-scale scientific monitoring, industrial and household infrastructures, social sensing, citizen science,observation-driven ontology engineering, and the Internet of Things. In this paper we give an overview of the ontologies anddiscuss the rationale behind key design decisions, reporting on the differences between the new SSN ontology presented here andits predecessor [9] developed by the W3C Semantic Sensor Network Incubator group (the SSN-XG). We present usage examplesand describe alignment modules that foster interoperability with other ontologies.Keywords: Ontology, Sensor, Actuator, Observation, Actuation, Sampling, Linked Data, Web of Things, Internet of Things1570-0844/18/$35.00 c© 2018 – IOS Press and the authors. All rights reservedmailto:armin.haller@anu.edu.aumailto:kerry.taylor@anu.edu.aumailto:jano@geog.ucsb.edumailto:simon.cox@csiro.aumailto:maxime.lefrancois@emse.frmailto:danh.lephuoc@tu-berlin.demailto:jlieberman@fas.harvard.edumailto:rgarcia@fi.upm.esmailto:rob@metalinkage.com.aumailto:cstadler@informatik.uni-leipzig.dehttp://www.w3.org/ns/sosa/http://www.w3.org/ns/ssn/A. Haller et al. / The Modular SSN Ontology: A Joint W3C and OGC Standard 11 12 23 34 45 56 67 78 89 910 1011 1112 1213 1314 1415 1516 1617 1718 1819 1920 2021 21",
        "file_name": "10!3233%sw-180320.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-180320.pdf"
    },
    {
        "title": "Models to represent linguistic linked data",
        "implementation_urls": [],
        "doi": "10.1017/s1351324918000347",
        "arxiv": null,
        "abstract": "AbstractAs the interest of the Semantic Web and computational linguistics communities in linguisticlinked data (LLD) keeps increasing and the number of contributions that dwell on LLDrapidly grows, scholars (and linguists in particular) interested in the development of LLDresources sometimes find it difficult to determine which mechanism is suitable for their needsand which challenges have already been addressed. This review seeks to present the state ofthe art on the models, ontologies and their extensions to represent language resources as LLDby focusing on the nature of the linguistic content they aim to encode. Four basic groupsof models are distinguished in this work: models to represent the main elements of lexicalresources (group 1), vocabularies developed as extensions to models in group 1 and ontologiesthat provide more granularity on specific levels of linguistic analysis (group 2), cataloguesof linguistic data categories (group 3) and other models such as corpora models or service-oriented ones (group 4). Contributions encompassed in these four groups are described, high-lighting their reuse by the community and the modelling challenges that are still to be faced.1 Introduction1.1 Background and motivationLanguage resources (dictionaries, terminologies, corpora, etc.) developed in the fieldsof corpus linguistics, computational linguistics and natural language processing(NLP) are often encoded in heterogeneous formats and developed in isolation fromone another. This makes their discovery, reuse and integration for both the develop-ment of NLP tools and daily linguistic research a difficult and cumbersome task. In∗We are very grateful to the anonymous reviewers for their meticulous reading of the surveyand for providing us with numerous insightful and constructive suggestions to improve it.We would also like to thank Dr Guadalupe Aguado-de-Cea for her help in proofreadingthis manuscript. This work is supported by the Spanish Ministry of Education, Cultureand Sports through the Formación del Profesorado Universitario (FPU) program, and bythe Spanish Ministry of Economy and Competitiveness through the project 4V (TIN2013-46238-C4-2-R) within the FEDER funding scheme, the Juan de la Cierva program, andthe Excellence Network ReTeLe (TIN2015-68955-REDT).https://www.cambridge.org/core/terms. https://doi.org/10.1017/S1351324918000347Downloaded from https://www.cambridge.org/core. University of New England, on 06 Oct 2018 at 15:39:38, subject to the Cambridge Core terms of use, available athttp://orcid.org/0000-0001-6433-4649https://www.cambridge.org/core/termshttps://doi.org/10.1017/S1351324918000347https://www.cambridge.org/core2 J. Bosque-Gil et al.order to alleviate such an issue and to enhance interoperability of language resourceson the Web, a community of language technologies experts and practitioners hasstarted adopting techniques coming from the area of study of linked data (LD).The LD paradigm emerges as a series of best practices and principles for ‘exposing,sharing and connecting data on the Web’ (Bizer, Heath and Berners-Lee 2011),independently of the domain. These principles state that unique resource identifiersshould be used to name things in a way that allows people to look them up, finduseful information represented with standard formalisms and discover more thingsthat are linked to those resources. LD emerged in the context of the SemanticWeb, an extension of the Web ‘in which information is given well-defined meaning,better enabling computers and people to work in cooperation’ (Berners-Lee et al.2001). General and domain-specific on-line ontologies (such as the ones we revisitin this survey) provide this well-defined meaning on the Semantic Web and areused to represent the data that will be linked to other data. Following this line,",
        "file_name": "10!1017%s1351324918000347.pdf",
        "file_path": ".\\PDFs\\10!1017%s1351324918000347.pdf"
    },
    {
        "title": "Modelling a smart environment for nonintrusive analysis of attention in the workplace",
        "implementation_urls": [],
        "doi": "10.1111/exsy.12275",
        "arxiv": null,
        "abstract": "Abstract Nowadays, the world is getting increasingly competitive and the quality and the amount of the work presented is one of the decisive factors when choosing an employee. It is no longer necessary to only perform but, to achieve a product with quality, on time, at the lowest possible cost and with the minimum resources. For this reason, the employee must have a high score of attention when performing a task and the factors that influence attention negatively must be reduced. This is true in many different domains, from the workplace to the classroom. In this paper we present a nonintrusive smart environment for monitoring people’s attention when working in teams. The presented system provides real-time information about each individual, as well as information about the team. It can be very useful for team managers to identify potentially distracting events or individuals since when the attention of an individual is not at its best when performing the proposed task her/his performance will be negatively affected, with consequences for the individual as well as for the organization. Keywords Smart Environment, Attention Behavior, Non-intrusive, Distributed Computing.  1. INTRODUCTION The rapid progress of wireless communication and sensing technologies enabled the development of smart learning environments, which are able to detect the environmental context as well as quantifying the attention of a worker in his workplace. For this reason, making intelligent learning systems has been the objective of many researchers in the field of computer science. In the field of computer science, a smart environment is a digitally augmented physical world where sensor-enabled and networked devices work continuously and collaboratively to make the lives of the inhabitants more comfortable. Indeed, significant advances in smart devices, wireless mobile communications, sensor networks, pervasive computing, machine learning, robotics, middleware and agent technologies, and human computer interfaces have made the dream of smart environments a reality. In this concept, the word “smart” means the ability to autonomously acquire and apply knowledge, and the word “environment” means our surroundings (Cook and Das, 2005). With this technological evolution, job offers have changed, bringing along many significant and broad changes. Some of the most notorious ones can be pointed out by the emergence of indicators such as attentiveness which, in extreme cases, can compromise the life and well-being of the workers. In more moderate cases it will impair attention, general cognitive skills and productivity. In addition to these factors, many of these jobs are the so-called desk-jobs, in which people frequently sit for more than 8 hours (Liao and Drury, 2000). Until now, the level of attention of a worker has been evaluated through his/her productivity: the more one produces, the better his/her attention at work. While the true nature of this relationship is yet to be thoroughly studied (properly contextualized in each work domain), there are other ",
        "file_name": "10!1111%exsy!12275.pdf",
        "file_path": ".\\PDFs\\10!1111%exsy!12275.pdf"
    },
    {
        "title": "Using machine learning to optimize parallelism in big data applications",
        "implementation_urls": [
            {
                "identifier": "https://github.com/hammerlab/slim",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!1016%j!future!2017!07!003.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Introduction Big data technology and services market is esti-mated to grow at a CAGR1 of 22.6% from 2015 to 2020 and reach $58.9 billion in 2020 [1]."
                    }
                ]
            }
        ],
        "doi": "10.1016/j.future.2017.07.003",
        "arxiv": null,
        "abstract": "AbstractIn-memory cluster computing platforms have gained momentum in the last years, due to their ability toanalyse big amounts of data in parallel. These platforms are complex and difficult-to-manage environments.In addition, there is a lack of tools to better understand and optimize such platforms that consequentlyform backbone of big data infrastructure and technologies. This directly leads to underutilization of avail-able resources and application failures in such environment. One of the key aspects that can address thisproblem is optimization of the task parallelism of application in such environments. In this paper, wepropose a machine learning based method that recommends optimal parameters for task parallelization inbig data workloads. By monitoring and gathering metrics at system and application level, we are able tofind statistical correlations that allow us to characterize and predict the effect of different parallelism set-tings on performance. These predictions are used to recommend an optimal configuration to users beforelaunching their workloads in the cluster, avoiding possible failures, performance degradation and wastageof resources. We evaluate our method with a benchmark of 15 Spark applications on the Grid5000 testbed.We observe up to a 51% gain on performance when using the recommended parallelism settings. The modelis also interpretable and can give insights to the user into how different metrics and parameters affect theperformance.Keywords: machine learning, spark, parallelism, big data1. IntroductionBig data technology and services market is esti-mated to grow at a CAGR1 of 22.6% from 2015 to2020 and reach $58.9 billion in 2020 [1]. Highly vis-ible early adopters such as Yahoo, eBay and Face-book have demonstrated the value of mining com-plex information sets, and now many companies areeager to unlock the value in their own data. In orderto address big data challenges, many different par-allel programming frameworks, like Map Reduce,Apache Spark or Flink have been developed [2, 3, 4].Planning big data processes effectively on theseplatforms can become problematic. They involvecomplex ecosystems where developers need to dis-cover the main causes of performance degradationin terms of time, cost or energy. However, process-ing collected logs and metrics can be a tedious andEmail address: abrandon@fi.upm.es (Álvaro BrandónHernández)1Compound Annual Growth Ratedifficult task. In addition, there are several param-eters that can be adjusted and have an importantimpact on application performance.While users have to deal with the challenge ofcontrolling this complex environment, there is afundamental lack of tools to simplify big data in-frastructure and platform management. Some toolslike YARN or Mesos [5, 6] help in decoupling theprogramming platform from the resource manage-ment. Still, they don’t tackle the problem of opti-mizing application and cluster performance.One of the most important challenges is findingthe best parallelization strategy for a particular ap-",
        "file_name": "10!1016%j!future!2017!07!003.pdf",
        "file_path": ".\\PDFs\\10!1016%j!future!2017!07!003.pdf"
    },
    {
        "title": "Keeping up with storage: Decentralized, write-enabled dynamic geo-replication",
        "implementation_urls": [],
        "doi": "10.1016/j.future.2017.06.009",
        "arxiv": null,
        "abstract": "AbstractLarge-scale applications are ever-increasingly geo-distributed. Maintaining the highest possible data locality is crucialto ensure high performance of such applications. Dynamic replication addresses this problem by dynamically creatingreplicas of frequently accessed data close to the clients. This data is often stored in decentralized storage systems suchas Dynamo or Voldemort, which offer support for mutable data. However, existing approaches to dynamic replicationfor such mutable data remain centralized, thus incompatible with these systems. In this paper we introduce a write-enabled dynamic replication scheme that leverages the decentralized architecture of such storage systems. We proposean algorithm enabling clients to locate tentatively the closest data replica without prior request to any metadata node.Large-scale experiments on various workloads show a read latency decrease of up to 42% compared to other state-of-the-art, caching-based solutions.Keywords: cloud, replication, geo-replication, storage, fault-tolerance, consistency, database, key-value store1. IntroductionLarge-scale applications such as social networks arebeing increasingly deployed over multiple, geograph-ically distributed datacenters (or sites). Such geo-distribution provides fast data access for end-usersworldwide while improving fault-tolerance, disaster-recovery and minimizing bandwidth costs. Today’scloud computing services [1, 2] allow a wider range ofapplications to benefit from these advantages as well.However, designing geo-distributed applications is dif-ficult due to the high and often unpredictable latencybetween sites [3].A key factor impacting application performance isdata locality, i.e. the location of the data relativelyto the application. Accessing remote data is orders ofmagnitude slower than using local data. Although suchremote accesses may be acceptable for rarely-accesseddata (cold data), they hinder application performanceEmail addresses: pmatri@fi.upm.es (Pierre Matri),mperez@fi.upm.es (Marı́a S. Pérez),alexandru.costan@irisa.fr (Alexandru Costan),luc.bouge@ens.ens-rennes.fr (Luc Bougé),gabriel.antoniu@inria.fr (Gabriel Antoniu)for frequently-used data (hot data). For instance, in asocial network application, popular profiles should bereplicated at all sites whereas others can remain locatedat fewer locations. Finding the right balance betweenreplication and storage is critical: replicating too manyprofiles wastes memory, while failing to replicate popu-lar ones results in degraded application performance.Dynamic replication [4] proposes to solve this issueby dynamically replicating hot data as close as possibleto the applications that access it. This technique is lever-aged in Content Delivery Networks (CDN) to cache im-mutable data close to the final user [5, 6]. Similarily, itis used in storage systems such as GFS [7] or HDFS [8]to replicate mutable data, by relying on the centralizedmetadata management of these systems [9, 10]. Yet,such an approach contradicts the design principles of",
        "file_name": "10!1016%j!future!2017!06!009.pdf",
        "file_path": ".\\PDFs\\10!1016%j!future!2017!06!009.pdf"
    },
    {
        "title": "KerA: Scalable Data Ingestion for Stream Processing",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00152",
        "arxiv": null,
        "abstract": "Abstract—Big Data applications are increasingly moving frombatch-oriented execution models to stream-based models thatenable them to extract valuable insights close to real-time. Tosupport this model, an essential part of the streaming processingpipeline is data ingestion, i.e., the collection of data from vari-ous sources (sensors, NoSQL stores, filesystems, etc.) and theirdelivery for processing. Data ingestion needs to support highthroughput, low latency and must scale to a large number of bothdata producers and consumers. Since the overall performanceof the whole stream processing pipeline is limited by that ofthe ingestion phase, it is critical to satisfy these performancegoals. However, state-of-art data ingestion systems such as ApacheKafka build on static stream partitioning and offset-based recordaccess, trading performance for design simplicity. In this paperwe propose KerA, a data ingestion framework that alleviate thelimitations of state-of-art thanks to a dynamic partitioning schemeand to lightweight indexing, thereby improving throughput,latency and scalability. Experimental evaluations show that KerAoutperforms Kafka up to 4x for ingestion throughput and up to5x for the overall stream processing throughput. Furthermore,they show that KerA is capable of delivering data fast enough tosaturate the big data engine acting as the consumer.Keywords—Stream processing, dynamic partitioning, ingestion.I. INTRODUCTIONBig Data real-time stream processing typically relies onmessage broker solutions that decouple data sources from ap-plications. This translates into a three-stage pipeline describedin Figure 1. First, in the production phase, event sources (e.g.,smart devices, sensors, etc.) continuously generate streams ofrecords. Second, in the ingestion phase, these records are ac-quired, partitioned and pre-processed to facilitate consumption.Finally, in the processing phase, Big Data engines consume thestream records using a pull-based model.Since users are interested in obtaining results as soon aspossible, there is a need to minimize the end-to-end latencyof the three stage pipeline. This is a non-trivial challengewhen records arrive at a fast rate and create the need tosupport a high throughput at the same time. To this purpose,Big Data engines are typically designed to scale to a largenumber of simultaneous consumers, which enables processingfor millions of records per second [1], [2]. Thus, the weaklink of the three stage pipeline is the ingestion phase: it needsto acquire records with a high throughput from the producers,serve the consumers with a high throughput, scale to a largenumber of producers and consumers, and minimize the writelatency of the producers and, respectively, the read latency ofthe consumers to facilitate low end-to-end latency.Fig. 1. Stream processing pipeline: records are collected at event timeand made available to consumers earliest at ingestion time, after the eventsare acknowledged by producers; processing engines continuously pull these",
        "file_name": "10!1109%icdcs!2018!00152.pdf",
        "file_path": ".\\PDFs\\10!1109%icdcs!2018!00152.pdf"
    },
    {
        "title": "SLoG: Large-Scale Logging Middleware for HPC and Big Data Convergence",
        "implementation_urls": [],
        "doi": "10.1109/icdcs.2018.00156",
        "arxiv": null,
        "abstract": "Abstract—Cloud developers traditionally rely on purpose-specific services to provide the storage model they need for anapplication. In contrast, HPC developers have a much morelimited choice, typically restricted to a centralized parallel filesystem for persistent storage. Unfortunately, these systems oftenoffer very low performance when subject to highly-concurrent,conflicting I/O patterns. This makes difficult the implementationof inherently concurrent data structures such as distributedshared logs. Yet, this data structure is key to applications suchas computational steering, data collection from physical sensorgrids or discrete event generators. In this paper we tackle thisissue. We present SLoG, a shared log middleware providing ashared log abstraction over a parallel file system, designed tocircumvent the aforementioned limitations. We evaluate SLoGdesign on up to 100,000 cores of the Theta supercomputer: itdemonstrates high append velocity at scale while also providingsubstantial benefits for other persistent backend storage systems.I. INTRODUCTIONTraditionally, cloud system designers have used purpose-specific storage services for their data storage needs. Theseinclude key-value stores [1]–[3], wide-column databases [4]–[6] or streaming message brokers [7], [8]. On the contrary,High-Performance Computing (HPC) platforms rest upon ona much more constrained set of storage primitives, typicallylimited to parallel file systems [9], [10] or transient burstbuffers [11], [12]. The low availability of local storage on thecompute nodes of most supercomputers today and the lack ofadministrative access give little opportunity for users to deploythe storage system they need.As the boundaries between HPC and Big Data Analytics(BDA) continue to blur [13], new challenges arise. A criticalobjective set in this convergence context is to foster applicationportability across platforms [14]. Let us consider an appli-cation running in a cloud context, which uses a specializedstorage service such as a distributed shared log [15]. Portingthis application to HPC (e.g., to leverage specific hardwarecapabilities) is challenging. Indeed, deploying cloud-orientedshared log services on HPC is often not possible because ofthe unique specificities of these platforms. While one couldconsider a shared log abstraction over the available parallelfile system, providing the illusion of a storage paradigm atopanother is extremely difficult considering conflicting set ofconstraints and APIs between the two models [16], [17].Shared log storage is indeed one of these storage modelsthat are both unavailable and very difficult to implement onHPC platforms using the available storage primitives. Yet,in scientific applications, distributed logs could play manyroles, e.g. for in-situ visualization of large data streams,collection of telemetry events for computational steering, ordata aggregation from arrays of physical sensors. A shared",
        "file_name": "10!1109%icdcs!2018!00156.pdf",
        "file_path": ".\\PDFs\\10!1109%icdcs!2018!00156.pdf"
    },
    {
        "title": "TýrFS: Increasing Small Files Access Performance with Dynamic Metadata Replication",
        "implementation_urls": [],
        "doi": "10.1109/ccgrid.2018.00072",
        "arxiv": null,
        "abstract": "Abstract—Small files are known to pose major performancechallenges for file systems. Yet, such workloads are increasinglycommon in a number of Big Data Analytics workflows or large-scale HPC simulations. These challenges are mainly caused bythe common architecture of most state-of-the-art file systemsneeding one or multiple metadata requests before being able toread from a file. Small input file size causes the overhead ofthis metadata management to gain relative importance as thesize of each file decreases. In this paper we propose a set oftechniques leveraging consistent hashing and dynamic metadatareplication to significantly reduce this metadata overhead. Weimplement such techniques inside a new file system named TýrFS,built as a thin layer above the Týr object store. We prove thatTýrFS increases small file access performance up to one order ofmagnitude compared to other state-of-the-art file systems, whileonly causing a minimal impact on file write throughput.I. INTRODUCTIONA large portion of research in data storage, managementand retrieval focuses on optimizing access performance forlarge files [1]–[5]. Yet, handling a large number of small filesraises other difficult challenges, that are partly related to thevery architecture of current file systems. Such small files,with a size inferior to a few megabytes, are very commonin large-scale facilities, as shown by multiple studies [6], [7].They can be generated by data-intensive applications suchas CM1 [8] or HACC [9], Internet of Things or StreamProcessing applications, as well as large scale workflows suchas Montage [10], CyberShake [11] or LIGO [12]. Improvingfile access performance for these applications is critical forscalability in order to handle ever-growing data sets on large-scale systems [13].As the amount of data to be transferred for storage op-erations on any single small file is intuitively small, the keyto optimizing access performance for such files lies in im-proving the efficiency of the associated metadata management.Actually, as the data size for each file decreases, the relativeoverhead of opening a file is increasingly significant. In ourexperiments, with small enough files, opening a file may takeup to an order of magnitude more time than reading the datait contains. One key cause of this behavior is the separationof data and metadata inherent to the architecture of currentfile systems. Indeed, to read a file, a client must first retrievethe metadata for all folders in its access path, that may belocated on one or more metadata servers, to check that the userhas the correct access rights or to pinpoint the location of thedata in the system. The high cost of network communicationsignificantly exceeds the cost of reading the data itself.We advocate that a different file system architecture isnecessary to reduce the cost of metadata management forsuch workloads involving many small files. While one could",
        "file_name": "10!1109%ccgrid!2018!00072.pdf",
        "file_path": ".\\PDFs\\10!1109%ccgrid!2018!00072.pdf"
    },
    {
        "title": "Smart Waste Collection System with Low Consumption LoRaWAN Nodes and Route Optimization",
        "implementation_urls": [
            {
                "identifier": "https://github.com/zaragoza-sedeelectronica/zaragoza-sedeelectronica.github.io",
                "type": "git",
                "paper_frequency": 3,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdf.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "The source code of this visualization example is available on the Zaragoza’s GitHub repository (https://github.com/zaragoza-sedeelectronica/zaragoza-sedeelectronica.github.io/tree/master/sparql/ejemplos/monumentos.html)."
                    }
                ]
            }
        ],
        "doi": "10.3390/s18051465",
        "arxiv": null,
        "abstract": "Abstract: Public administrations handle large amounts of data in relation to their internal processesas well as to the services that they offer. Following public-sector information reuse regulations andworldwide open data publication trends, these administrations are increasingly publishing their dataas open data. However, open data are often released without agreed data models and in non-reusableformats, reducing interoperability and efficiency in data reuse. These aspects hinder interoperabilitywith other administrations and do not allow taking advantage of the associated knowledge in anefficient manner. This paper presents the continued work performed by the Zaragoza city councilover more than 15 years in order to generate its knowledge graph, which constitutes the key piece oftheir data management system, whose main strengthen is the open-data-by-default policy. The mainfunctionalities that have been developed for the internal and external exploitation of the city’s opendata are also presented. Finally, some city council experiences and lessons learned during this processare also explained.Keywords: public administration; linked open data; knowledge graph; interoperability; ontologies;open government data; vocabularies; API1. IntroductionPublic administrations are large data producers and collectors due to the need to provide supportfor their internal processes, as well as for services offered to citizens. These data are traditionallyavailable in several formats, according to their own information models, and are managed in different,sometimes isolated, data sources. From the point of view of the organization, the importance of datais undeniable. A proper organizational data management improves decision-making, operationalefficiency and service provision. From the side of citizens, the public administration’s data are anessential asset that must be made available in order to enhance transparency and the accountability [1].From the point of view of re-users, having these data available enables developing value-addedservices and applications, consequently stimulating innovation and economy [2]. Having all thesebenefits in mind, public administrations are increasingly publishing their data as Open Data, definedas data that can be freely used, re-used and redistributed by anyone—subject only, at most, to therequirement to attribute and sharealike [3]. An Open Government Data strategy ensures that dataare made available across city departments and to third parties, contributes to citizen engagement,increases democracy and serves to drive economic growth and social improvement [4,5].However, these data are mostly represented with information models defined without previousagreements or consensus processes with other institutions, which hinders the possibility of using andInformation 2020, 11, 129; doi:10.3390/info11030129 www.mdpi.com/journal/informationhttp://www.mdpi.com/journal/informationhttp://www.mdpi.comhttps://orcid.org/0000-0002-3938-2064https://orcid.org/0000-0002-9260-0753http://dx.doi.org/10.3390/info11030129http://www.mdpi.com/journal/informationhttps://www.mdpi.com/2078-2489/11/3/129?type=check_update&version=2Information 2020, 11, 129 2 of 18sharing them within the same organization or with third parties. This lack of adoption of shared modelsis one of the major causes of inefficiency of information management within large organizations [6].In this regard, ontologies, defined as formal specifications of shared conceptualizations [7], have beenused to describe data without ambiguities. Hence, these may be good artifacts to facilitate reusability,interoperability and data quality assurance.When all these data are organized according to ontologies (also known as vocabularies in thecontext of Open Government Data) and represented in some graph-like format, we can talk abouta Knowledge Graph. Despite there being no consensus about a single definition for a KnowledgeGraph [8–11], in this work it is understood as a graph of data with the intent to compose knowledge [12].Knowledge Graphs have become a powerful resource in big industries, for example, Google’s",
        "file_name": "pdf",
        "file_path": "PDFs\\pdf.pdf"
    },
    {
        "title": "Predicting incorrect mappings",
        "implementation_urls": [],
        "doi": "10.1145/3167132.3167164",
        "arxiv": null,
        "abstract": "ABSTRACTDBpedia releases consist of more than 70multilingual datasets thatcover data extracted from different language-specific Wikipediainstances. The data extracted from those Wikipedia instances aretransformed into RDF usingmappings created by theDBpedia com-munity. Nevertheless, not all the mappings are correct and con-sistent across all the distinct language-specific DBpedia datasets.As these incorrect mappings are spread in a large number of map-pings, it is not feasible to inspect all such mappings manually toensure their correctness. Thus, the goal of this work is to pro-pose a data-driven method to detect incorrect mappings automati-cally by analyzing the information from both instance data as wellas ontological axioms. We propose a machine learning based ap-proach to building a predictive model which can detect incorrectmappings. We have evaluated different supervised classification al-gorithms for this task and our best model achieves 93% accuracy.These results help us to detect incorrect mappings and achieve ahigh-quality DBpedia.CCS CONCEPTS• Information systems→ Resource Description Framework(RDF); • Computing methodologies → Cross-validation; Se-mantic networks; • Social and professional topics→ Quality as-surance;KEYWORDSLinked Data, Data Quality, Mappings, DBpedia, Machine LearningPermission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full cita-tion on the first page. Copyrights for components of this work owned by others thanACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or re-publish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.SAC 2018, April 9–13, 2018, Pau, France© 2018 Association for Computing Machinery.ACM ISBN 978-1-4503-5191-1/18/04. . . $15.00https://doi.org/10.1145/3167132.3167164ACM Reference format:Mariano Rico, Nandana Mihindukulasooriya, Dimitris Kontokostas, HeikoPaulheim, Sebastian Hellmann, and Asunción Gómez-Pérez. 2018. Predict-ing Incorrect Mappings: A Data-Driven Approach Applied to DBpedia. InProceedings of SAC 2018: Symposium on Applied Computing , Pau, France,April 9–13, 2018 (SAC 2018), 8 pages.https://doi.org/10.1145/3167132.31671641 INTRODUCTIONA large number of RDF knowledge bases are created by transform-ing non-RDF data sources into RDF. Such non-RDF formats includerelational databases, CSV files, key-value pairs, etc. A key input tothis transformation process is a mapping that defines how to trans-form the non-RDF source data into RDF. Such mapping specifieshow tomap the source schema into RDF vocabularies, and possibly",
        "file_name": "10!1145%3167132!3167164.pdf",
        "file_path": ".\\PDFs\\10!1145%3167132!3167164.pdf"
    },
    {
        "title": "Relationship recommender system in a business and employment-oriented social network",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2017.12.050",
        "arxiv": null,
        "abstract": "levels of abstractions and openness, which is not the case of most solutions [13] . Moreover, although agent frameworks andplatforms have similarities, there are subtle differences too. Each platform uses different models and syntax and providesdifferent libraries. In our case [41] , we have tried to use standards whose robustness has already been demonstrated andwhich are known within the research community, making the implementation of the system easier and highly reliable. Aservice oriented platform that allows for the implementation of an open MAS is used [41] to take maximum advantage ofthe distribution of resources. To this end, all services are implemented as Web Services. Due to its service orientation, dif-ferent tools modeled with agents that consume Web services can be integrated and operated from the platform, regardlessof their physical location or implementation. Distributed MAS have become very sophisticated in the last years [14,48] , with a rising potential to handle large vol-umes of data and coordinate the operations of many organizations. MAS offer a general computing paradigm for solv-ing difficult computational problems as well as for characterizing complex systems based on concepts such as autonomy,self-aggregation, self-organization and emergent behavior. Due to its characteristics, MAS are especially suitable for solvinglarge-scale and distributed problems such as social network presented in this research. Looking into the current researchesbackground, it is possible to observe that MAS are been widely used to model and solve complex real-world problems intext mining and recommender systems in social networks. New methods have been proposed and applied to different areasincluding social network analysis, gene network analysis or web clustering. Most of the existing methods for mining com-munities are centralized, although in studies such as [24,47] . In this case, a group of autonomous agents work together tomine a network through a proposed self-aggregation and self-organization mechanism. In [27] a new model is presentedfor finding influential agent groups based on group centrality analyses in citation networks. The authors present a modelfor finding influential agent groups in multi-agent software systems considering that the impact of an agent is mainly char-acterized by its citation relations (a group’s impact is determined by both direct and indirect citations). Another exampleis presented in [15] , a novel distributed model for HMAS. The proposed model assumes an interaction network among theagents of a MAS. This decentralized and local-info-based nature of the suggested model makes it an appropriate approachfor large-scale open MAS. The outcomes of this work can be used in a wide range of distributed applications. There is a notable trend in filling the gap between social network analysis and control. This trend was triggered by theadvancements in complex networks theory and MAS, the inclusion of new mathematical models describing dynamics ofsocial groups, and the development of new computational tools for relevant information analysis. Control of collective behavior is one of the most desirable goals in many applications related to social network analysisand mining. This work also intends to go a step further in this regard, by integrating a comprehensive recommender systeminto a social based agent architecture for the extraction and analysis of relevant information. The system was implementedin an agent framework that was subsequently used to extract the behavior of users in a social network. 3. Proposed system This section describes the process that is carried out before a relationship is recommended (or not) to a user. To providea recommendation, the system must detect a tie between two users or between a user and a job offer. According to thesocial tie proposals [31] , which are based on mathematical sociology, ties can be classified into three types: • Strong ties : these ties represent a formal and direct link. On the social network, a strong tie represents an existing anddirect relationship between users or a user and a job offer. • Weak ties : these are the most common ties on social networks. They are created when there is a link between twoentities (user-user or user-job offer) that is not strong, but there are some elements connect users or a user and a joboffer within the system. • Absent ties : ties between objects that do not substantiate any true link, these are not considered by the system. In such way, the system searches for weak ties and evaluates them, recommending suitable relationships to users. Oncea user accepts a recommendation, a weak tie converts into a strong tie. In addition, we should specify that these links are not bidirectional, they are unidirectional. Therefore, the system cansuggest a relationship to only one of the users, since the affinity is calculated in such a way that it adapts to the profile ofeach user individually and automatically. In the job offer RS, particular job offers are recommended to compatible users. The rest of this section addresses the findings and evaluations made in the four categories: (i) VOs of agents; (ii) extrac-tion of relevant information from unstructured texts; (iii) user-user relationships; (iv) user-job offer relationships. 3.1. Virtual organizations By using VO, the system can be structured as a group of agents which are coordinated through the integrated platform",
        "file_name": "10!1016%j!ins!2017!12!050.pdf",
        "file_path": ".\\PDFs\\10!1016%j!ins!2017!12!050.pdf"
    },
    {
        "title": "A Context-Aware Indoor Air Quality System for Sudden Infant Death Syndrome Prevention",
        "implementation_urls": [],
        "doi": "10.3390/s18030757",
        "arxiv": null,
        "abstract": "Abstract: Context-aware monitoring systems designed for e-Health solutions and ambient assistedliving (AAL) play an important role in today’s personalized health-care services. The majority of thesesystems are intended for the monitoring of patients’ vital signs by means of bio-sensors. At present,there are very few systems that monitor environmental conditions and air quality in the homes ofusers. A home’s environmental conditions can have a significant influence on the state of the healthof its residents. Monitoring the environment is the key to preventing possible diseases caused byconditions that do not favor health. This paper presents a context-aware system that monitors airquality to prevent a specific health problem at home. The aim of this system is to reduce the incidenceof the Sudden Infant Death Syndrome, which is triggered mainly by environmental factors. In theconducted case study, the system monitored the state of the neonate and the quality of air while it wasasleep. The designed proposal is characterized by its low cost and non-intrusive nature. The resultsare promising.Keywords: context-aware; SIDS; non-intrusive; e-health; pediatric1. IntroductionDue to the recent advances in sensor systems, the Internet-of-Things, and medical devices, it ispossible to provide personalized and continuous health care at home [1]. People with chronic diseasesor elderly people are the ones that most benefit from these home care systems [2]. Thanks to thesedevelopments, patients can be more independent and reduce their visits to the doctor. This contributesgreatly to the quality of their life [3]. The evolution of monitoring systems entails sensors collectinggreater amounts of data; this means that we have to be prepared to process more medical data [4].It is therefore essential to find solutions that will allow for the management of large amountsof data in a fast, efficient, and accurate way [5]. To achieve efficient management, it is essential toapply a context layer to the collected data. Context-aware systems play a notable role in the processingand analyzing of data [6]. The correct contextualization of the collected information is essential forunderstanding it and generating knowledge that can be used in decision making. To contextualize data,it is necessary to have additional information on the context that will make the collected data coherentand reliable [7]. In many cases, additional context information comes from heterogeneous data sources,and it is necessary that they are reliable and accurate context providers. Poorly contextualized datasetscan lead intelligent applications and systems to make the wrong decisions. This is a very seriousissue for medical systems, in which a wrong decision caused by a contextualization error can put thehealth of the patient at risk [8]. Hence, when designing homecare systems, it is important to pay closeattention to the performance of the systems that add and manage contexts in context-aware systems.Sensors 2018, 18, 757; doi:10.3390/s18030757 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18030757http://www.mdpi.com/journal/sensorsSensors 2018, 18, 757 2 of 22Another important issue in today’s homecare systems is environmental factors such as air quality.It has been demonstrated that acting over these factors is a determinant in the prevention of differentdiseases [9]. The problem is that much of current sensor systems employ only biomedical measurementdevices that users must carry with them or with which they must interact in some way (such as a pulsesensor, body temperature meter, or glucometer). Just a few projects implement systems that monitorenvironmental factors in the home.The lack of research in this area creates a necessity for new proposals in the field of air qualitymonitoring systems in homecare. These systems must measure air quality in domestic environmentswith the aim of improving user’s health. Current systems are compatible with traditional health",
        "file_name": "pdf?version=1519986905",
        "file_path": "PDFs\\pdfversion1519986905.pdf"
    },
    {
        "title": "Students’ Evaluation of a Virtual World for Procedural Training in a Tertiary-Education Course",
        "implementation_urls": [],
        "doi": "10.1177/0735633117706047",
        "arxiv": null,
        "abstract": "Abstract This article presents an investigation on the educational value of virtual worlds intended for the acquisition of procedural knowledge. This investigation takes as a case of study a virtual laboratory on biotechnology. A remarkable feature in this virtual laboratory is an automatic tutor that supervises student’s actions and provides tutoring feedback when it is appropriate. The study presented in this article covers two different aspects of the system. First, it analyzes the impact of this virtual world in learning some concepts related to a biotechnology practice; and second, it surveys the students’ opinion on the virtual world by means of three open questions (posi­tive, negative, and general impressions). Results demonstrated that the virtual world had a positive influence in the students’ knowledge, and it was well received by them. 1School of Computer Engineering, UPM, Spain 2Department of Artificial Intelligence, UPM, Spain 3MONTES (School of Forest Engineering and Natural Environment), UPM, Spain Corresponding Author: Jaime Ram ı́rez, Escuela Sup. de Ing. Informa t́icos, Calle de los Ciruelos Montegancedo Campus, Universidad Polite´cnica de Madrid, Building 5, Floor 1, Office 5112, 28660 Boadilla del Monte, Madrid, Spain. Email: jramirez@fi.upm.es There are many examples in the literature that show how interactive simulations are successfully employed for educational purposes. Among these interactive simulations, three-dimensional (3D) virtual environments represent a group of applications that each day is attracting more and more the interest of the edu­cational community. This is because these systems are engaging and allow students to learn new concepts and procedures by recreating situations that in the real world would be too expensive, dangerous, or simply out of reach for the students. We can see 3D virtual environments as a tool for implementing the ‘‘learning by doing’’ approach derived from the constructivist pedagogy (Huang, Rauch, & Liaw, 2010). When several users can visit a shared 3D virtual environment through Internet and interact by means of desktop devices, this 3D virtual environment is usually referred to as a virtual world. In addition to the benefits of 3D simulations, virtual worlds also provide the possibility of reproducing real scen­arios where several people (students or teachers) are able to chat and share rooms, instruments, and so forth. Moreover, as students connect to virtual worlds through Internet, they can perform virtual practices at home, without the physical presence of a teacher close to them. This opens the door to virtual practices from distant places where sufficiently equipped real labs are not available. In recent years, the development of 3D virtual worlds has become more inex­pensive thanks to platforms such as Second Life (http://secondlife.com/), OpenSimulator (OS; http://opensimulator.org/), Open Wonderland (http://open-wonderland.org/), or Bitmanagement Software Collaborate System (http://www. bitmanagement.com/), which provide developers with a basic layer of function­ality. As a result of this, more academic institutions have been able to develop and use their own virtual worlds. This article presents a study on the educational value of a virtual world for procedural learning. In the literature on virtual worlds, we can find many works on how this kind of systems can support different learning activities. However, there are just a few works that show how this kind of systems can support task ",
        "file_name": "10!1177%0735633117706047.pdf",
        "file_path": ".\\PDFs\\10!1177%0735633117706047.pdf"
    },
    {
        "title": "Increasing the Intensity over Time of an Electric-Assist Bike Based on the User and Route: The Bike Becomes the Gym",
        "implementation_urls": [],
        "doi": "10.3390/s18010220",
        "arxiv": null,
        "abstract": "Abstract: Nowadays, many citizens have busy days that make finding time for physical activitydifficult. Thus, it is important to provide citizens with tools that allow them to introduce physicalactivity into their lives as part of the day’s routine. This article proposes an app for an electricpedal-assist-system (PAS) bicycle that increases the pedaling intensity so the bicyclist can achievehigher and higher levels of physical activity. The app includes personalized assist levels that havebeen adapted to the user’s strength/ability and a profile of the route, segmented according to itsslopes. Additionally, a social component motivates interaction and competition between users basedon a scoring system that shows the level of their performances. To test the training module, acase study in three different European countries lasted four months and included nine people whotraveled 551 routes. The electric PAS bicycle with the app that increases intensity of physical activityshows promise for increasing levels of physical activity as a regular part of the day.Keywords: personalized assistance level; coaching; physical activity; electric bicycles1. IntroductionAdvances in the field of technology and developments in common transport systems have greatlyreduced people’s physical activity [1]. In developed countries, the majority of people travel to andfrom work using motor transport systems, e.g., statistics from 2011 show that, in England and Wales,85% of the population used motorized transport as their usual commute mode [2]. Nowadays, citizensspend much more time at sedentary activities, such as working in front of the computer. In 2012, datafrom 66 high and low income countries show that the percentage of adults who spent four or morehours sitting each day was 41.5% [1]. Due to these changes in our lifestyle, the risk of suffering healthproblems as a result of physical activity is increasingly high [3]. Experts from diverse entities, suchas WHO (World Health Organization), recommend an average of 150 min of exercise per week or30 min daily [4]. Physical activity provides a well-known set of health benefits [5]. Exercise has beenproven to reduce the risk of suffering from high blood pressure, stroke and others [4]. It increasescardiorespiratory and muscular fitness, bone health or increased functional health. Moreover, it canhelp prevent depression [4]. In 2010, the Global Health Observatory (GHO) estimated that the dailyphysical activity of more than 20% of adults is insufficient [6]. The low exercise, combined with thedaily ingestion of fat and calorie rich foods, is leading our society to an obesity epidemic [7].Nowadays, people who wish to make exercise a part of their daily routine usually go to gymsor sign up for different sports. This commitment implies an economic cost of registration and sportsequipment, travel to sports centers, as well as the necessary free time to carry out the activity and aSensors 2018, 18, 220; doi:10.3390/s18010220 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743https://orcid.org/0000-0002-2829-1829http://dx.doi.org/10.3390/s18010220http://www.mdpi.com/journal/sensorsSensors 2018, 18, 220 2 of 21willingness to attend regularly. Performing a team sport (e.g., basketball, football or volleyball) alsorequires adequate sports facilities, a group of people to carry out the activity and a skill in that sport.For these reasons, many people do not perform physical activity regularly [8,9]. As a result, the dailyuse of a bicycle in routine trips is an important alternative to the gym or other sports [10].One of the most widespread ways of fostering active transport among users is by promoting theuse of bicycles in the city [11,12]. Biking will not only help people to get fit but will also reduce trafficcongestion, environmental contamination, climate change and energetic sustainability [13,14]. Theupgrading of infrastructures for cyclists helps provide a positive experience and as a result increases theuse of bicycles in the city [15,16]. In recent years, cities have been promoting the use of bicycles through",
        "file_name": "pdf?version=1515922049",
        "file_path": "PDFs\\pdfversion1515922049.pdf"
    },
    {
        "title": "Multi-Agent System for Demand Prediction and Trip Visualization in Bike Sharing Systems",
        "implementation_urls": [
            {
                "identifier": "https://github.com/novafloss/workalendar",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\pdfversion1515142581.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Available online: https://github.com/novafloss/workalendar (accessed on 5 October 2017)."
                    }
                ]
            }
        ],
        "doi": "10.3390/app8010067",
        "arxiv": null,
        "abstract": "Abstract: This paper proposes a multi agent system that provides visualization and prediction toolsfor bike sharing systems (BSS). The presented multi-agent system includes an agent that performsdata collection and cleaning processes, it is also capable of creating demand forecasting models foreach bicycle station. Moreover, the architecture offers API (Application Programming Interface)services and provides a web application for visualization and forecasting. This work aims to make thesystem generic enough for it to be able to integrate data from different types of bike sharing systems.Thus, in future studies it will be possible to employ the proposed system in different types of bikesharing systems. This article contains a literature review, a section on the process of developing thesystem and the built-in prediction models. Moreover, a case study which validates the proposedsystem by implementing it in a public bicycle sharing system in Salamanca, called SalenBici. It alsoincludes an outline of the results and conclusions, a discussion on the challenges encountered in thisdomain, as well as possibilities for future work.Keywords: bike sharing systems (BSS); regression models; open data; data visualization; multi agentsystems; organizations and institutions; socio-technical systems1. IntroductionThere is a consensus in the literature [1,2] which states that bicycles are one of the most sustainablemodes of urban transport and they are suitable for both short trips and medium distance trips.Riding a bicycle does not have any negative impact on the environment [3], it promotes physicalactivity and improves health. Furthermore, its use is cost-effective from the perspective of usersand infrastructure.Moreover, due to the increased CO2 levels, the European Union and other states are takingmeasures to reduce greenhouse gas emissions in every sector of the economy [4].These facts explain the growing popularity of sustainable means of transport such as bike sharingsystems. From 1965 when they came into use in Amsterdam to 2001, there were only few systemsaround the world. Bike sharing systems (BSS) began to spread in 2012, when their number increased toover 400 [5]. By 2014 this number had doubled [6] and nowadays there are approximately 1175 cities,municipalities or district jurisdictions in 63 different countries where these systems are in active use,according to BikeSharingMap [7].Appl. Sci. 2018, 8, 67; doi:10.3390/app8010067 www.mdpi.com/journal/applscihttp://www.mdpi.com/journal/applscihttp://www.mdpi.comhttps://orcid.org/0000-0002-0493-4471https://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0001-5697-0354https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/app8010067http://www.mdpi.com/journal/applsciAppl. Sci. 2018, 8, 67 2 of 21Bike sharing systems allow users to travel in the city at a low cost or even for free. They can pickup a bicycle at one of the stations distributed across the city and leave it at another. These systemshave evolved over time [8] and today the vast majority include sensors that provide information on theinteraction of users with the system. However, the management of these systems and the data collectedby them, is often poor and as a result the numbers of bicycles available at stations are not sufficient.These are the reasons as to why bike sharing systems should be improved with data producedby the systems themselves. They should include predictive models for user behaviour and demand,which will notify the system administrator of the stations where more bicycles are required forsatisfying user demand. This will also allow to set up new stations in places where the demand is highor, on the contrary, to close down the stations at which the demand is too low.This article presents a multi-agent system which collects bike sharing system data together withother useful data. The system uses these data to create demand prediction models and to offer services",
        "file_name": "pdf?version=1515142581",
        "file_path": "PDFs\\pdfversion1515142581.pdf"
    },
    {
        "title": "Combination of Multi-Agent Systems and Wireless Sensor Networks for the Monitoring of Cattle",
        "implementation_urls": [],
        "doi": "10.3390/s18010108",
        "arxiv": null,
        "abstract": "Abstract: Precision breeding techniques have been widely used to optimize expenses and increaselivestock yields. Notwithstanding, the joint use of heterogeneous sensors and artificial intelligencetechniques for the simultaneous analysis or detection of different problems that cattle may presenthas not been addressed. This study arises from the necessity to obtain a technological tool thatfaces this state of the art limitation. As novelty, this work presents a multi-agent architecture basedon virtual organizations which allows to deploy a new embedded agent model in computationallylimited autonomous sensors, making use of the Platform for Automatic coNstruction of orGanizationsof intElligent Agents (PANGEA). To validate the proposed platform, different studies have beenperformed, where parameters specific to each animal are studied, such as physical activity,temperature, estrus cycle state and the moment in which the animal goes into labor. In addition, a setof applications that allow farmers to remotely monitor the livestock have been developed.Keywords: birth sensor; bovine embedded hardware; ambient intelligence; virtual organizationsof agents1. IntroductionThe last decade saw a breakthrough in the field of Ambient Intelligence (AmI), resulting in animprovement in the quality of people’s lives. The main objective of AmI is to adapt technology tothe needs of the people in a way that allows users to interact in a natural and effortless manner withthe different systems that make up the environment. Technology must act in a transparent way byadapting to the individuals and their context; simplifying the accomplishment of daily tasks and thecommunication between them and the environment. To achieve this goal, it is necessary to develop newtechnological models that allow users to interact with multiple devices simultaneously. The devicesmust collaborate in the accomplishment of daily tasks without the individuals being aware of it. Dueto the miniaturization of sensors and the reduction of costs in manufacturing processes, AmI can nowprovide new solutions to daily problems that were unthinkable just a few years ago. One field to whichAmI can be applied to is that of livestock and natural resources management.Currently in Spain, there are more than 23 million head of swine and more than 18 million ofsheep, representing 15% of the total of the European Union [1]. The continuous growth that thefarms are experiencing and the increasing demand for farm produce, make technology an essentialinstrument of continuous improvement in the development of farms. Requirements are also becomingincreasingly strict for both the breeders and the livestock, so the study of factors such as food, physicalactivity or the animal’s health is necessary. On large farms, it is difficult to devote time to observing theSensors 2018, 18, 108; doi:10.3390/s18010108 www.mdpi.com/journal/sensorshttp://www.mdpi.com/journal/sensorshttp://www.mdpi.comhttps://orcid.org/0000-0002-6536-2251https://orcid.org/0000-0002-4392-4743http://dx.doi.org/10.3390/s18010108http://www.mdpi.com/journal/sensorsSensors 2018, 18, 108 2 of 27behavior of each animal. Technology must provide solutions that simplify farmers’ work, especiallyin repetitive and difficult tasks, such as those that entail the study of the factors mentioned above.The research and development of systems that detect anomalies in animals at early stages, is vitallyimportant. Numerous research groups are working on the use of electronic systems in livestock withthe aim of improving productivity and reducing operating costs. AmI allows farmers to remotelyaccess up to date information on the animals and obtain their complete traceability. The joint use ofinformation technology and electronic devices, allows to monitor and study parameters as well asthe consumption of energy on the farm, the level of food in the feeders, the lighting, climatology, thestate of animals’ health, their physical activity, etc. Smart-farming or precision farming consists inapplying information and communication technologies to livestock and agriculture. Its main objectiveis to increase the efficiency and quality of production through rapid decision making in cases where",
        "file_name": "pdf?version=1516694198",
        "file_path": "PDFs\\pdfversion1516694198.pdf"
    },
    {
        "title": "A SAREF Extension for Semantic Interoperability in the Industry and Manufacturing Domain",
        "implementation_urls": [],
        "doi": "10.1002/9781119564034.ch25",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1002%9781119564034!ch25.pdf",
        "file_path": ".\\PDFs\\10!1002%9781119564034!ch25.pdf"
    },
    {
        "title": "PROFILES &amp; DATA",
        "implementation_urls": [],
        "doi": "10.1145/3184558.3192316",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1145%3184558!3192316.pdf",
        "file_path": ".\\PDFs\\10!1145%3184558!3192316.pdf"
    },
    {
        "title": "Advances in Pattern-Based Ontology Engineering",
        "implementation_urls": [],
        "doi": "10.3233/ssw51",
        "arxiv": null,
        "abstract": "Chapter 19. Syntactic Regularities Based on Language Abstractions 312Christian Kindermann, Bijan Parsia and Uli SattlerChapter 20. Towards Easy Reusability in the Semantic Web 331Johannes Lipp, Lars Gleim and Stefan DeckerChapter 21. An Ontology Design Pattern for Modeling Bias 337Amrit Mohan Kaushik and Raghava MutharajuChapter 22. OTTR: Formal Templates for Pattern-Based Ontology Engineering 349 Martin G. Skjceveland, Daniel P. Lupp, Leif Harald Karlsen and Johan W. KluwerChapter 23. The Core OTTR Template Library 378Martin G. SkjcevelandAuthor Index 395",
        "file_name": "10!3233%ssw51.pdf",
        "file_path": ".\\PDFs\\10!3233%ssw51.pdf"
    },
    {
        "title": "Linked Open Vocabularies (LOV): A gateway to reusable semantic vocabularies on the Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-160213",
        "arxiv": null,
        "abstract": "Abstract. This article addresses a number of limitations of state-of-the-art methods of Ontology Alignment: 1) they primarilyaddress concepts and entities while relations are less well-studied; 2) many build on the assumption of the ‘well-formedness’ ofontologies which is unnecessarily true in the domain of Linked Open Data; 3) few have looked at schema heterogeneity from asingle source, which is also a common issue particularly in very large Linked Dataset created automatically from heterogeneousresources, or integrated from multiple datasets. We propose a domain- and language-independent and completely unsupervisedmethod to align equivalent relations across schemata based on their shared instances. We introduce a novel similarity measureable to cope with unbalanced population of schema elements, an unsupervised technique to automatically decide similaritythreshold to assert equivalence for a pair of relations, and an unsupervised clustering process to discover groups of equivalentrelations across different schemata. Although the method is designed for aligning relations within a single dataset, it can alsobe adapted for cross-dataset alignment where sameAs links between datasets have been established. Using three gold standardscreated based on DBpedia, we obtain encouraging results from a thorough evaluation involving four baseline similarity measuresand over 15 comparative models based on variants of the proposed method. The proposed method makes significant improvementover baseline models in terms of F1 measure (mostly between 7% and 40%), and it always scores the highest precision and isalso among the top performers in terms of recall. We also make public the datasets used in this work, which we believe make thelargest collection of gold standards for evaluating relation alignment in the LOD context.Keywords: Ontology alignment, ontology mapping, Linked Data, DBpedia, similarity measure1. IntroductionThe Web of Data is currently seeing remarkablegrowth under the Linked Open Data (LOD) commu-nity effort. The LOD cloud currently contains over*Corresponding author. E-mail: ziqi.zhang@sheffield.ac.uk.9,000 datasets and more than 85 billion triples.1 Itis becoming a gigantic, constantly growing and ex-tremely valuable knowledge source useful to many ap-plications [16,30]. Following the rapid growth of theWeb of Data is the increasingly pressing issue of het-1http://stats.lod2.eu/, visited on 30-09-2015.This article is published online with Open Access and distributed under the terms of the Creative Commons Attribution Non-Commercial License.1570-0844/15/$35.00 © 2015 – IOS Press and the authors.mailto:ziqi.zhang@sheffield.ac.ukmailto:a.gentile@sheffield.ac.ukmailto:i.augenstein@sheffield.ac.ukmailto:f.ciravegna@sheffield.ac.ukmailto:eva.blomqvist@liu.semailto:ziqi.zhang@sheffield.ac.ukhttp://stats.lod2.eu/UNCORRECTED  PROOF2 Z. Zhang et al. / An unsupervised data-driven method to discover equivalent relations in large Linked Datasetserogeneity, the phenomenon that multiple vocabular-ies exist to describe overlapping or even the samedomains, and the same objects are labeled with dif-ferent identifiers. The former is usually referred toschema-level heterogeneity and the latter as data orinstance-level heterogeneity. It is widely recognizedthat currently LOD datasets are characterized by denselinks at data-level but very sparse links at schema-level[15,24,38]. This may hamper the usability of data overlarge scale and reduces interoperability between Se-mantic Web applications built on LOD datasets. This",
        "file_name": "10!3233%sw-160213.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-160213.pdf"
    },
    {
        "title": "Ontological requirement specification for smart irrigation systems: a SOSA/SSN and SAREF comparison",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": null,
        "file_name": "document",
        "file_path": "PDFs\\document.pdf"
    },
    {
        "title": "Besoins ontologiques d'un système d'irrigation intelligent: comparaison entre SSN et SAREF",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": null,
        "file_name": "document",
        "file_path": "PDFs\\document.pdf"
    },
    {
        "title": "CREATING AND IMPROVING EDUCATIONAL MATERIALS: AN APPROACH BASED ON CROWDSOURCING",
        "implementation_urls": [],
        "doi": "10.21125/iceri.2018.1854",
        "arxiv": null,
        "abstract": "Abstract One way of increasing the quantity and quality of educational materials that are available on-line is to apply collaborative and crowdsourcing techniques during the production process. The idea of involving groups of learners working together to solve a problem, to complete a task, or to create a product is an educational approach to teach and learn called Collaborative Learning (CL). One way of applying the CL approach is by using a collaborative authoring platform such as SlideWiki. This paper presents a crowdsourced-based approach for involving students in the creation and improvement of educational materials using the SlideWiki platform. Keywords: Crowdsourcing, Educational Material, Collaborative Learning. 1 INTRODUCTION  Open Educational Resources in general and OpenCourseWare (OCW) in particular can be shared and reused, thus fostering knowledge interchange among teachers and learners. Currently, there are different OCW platforms that open new horizons for knowledge sharing and e-learning by reaching learners beyond the constraints of traditional learning systems. However, there is a lack of on-line educational content due to the fact that their creation and maintenance is tedious, time-consuming and expensive.  Costs associated to the creation of educational materials as well as quality of those materials can be improved by applying collaborative techniques during the production process [1]. Collaborative authoring has improved the efficiency, effectiveness, quality and timeliness of content creation in different domains such as text, geospatial content, and software code. Therefore it is to be expected that in the educational domain, a crowdsourcing approach helps to increase the quantity and quality of OCW.  The idea of involving groups of learners working together to solve a problem, to complete a task, or to create a product is an educational approach to teaching and learning called Collaborative Learning (CL) [2]. One way of applying the CL approach is by using a collaborative authoring platform such as the SlideWiki platform1. The SlideWiki platform allows users to collaboratively create and maintain OCW (e.g. slide presentations) in a crowdsourcing and on-line fashion. This platform is part of the so-called Web 2.0 technologies in which users can be active contributors rather than just passive observers. This collective intelligence can be used for both instructors and students to create educational materials that can have great value [3]. SlideWiki [4] is a crowdsourcing platform that aims to rethink the creation and sharing of knowledge by providing an environment where authors can collaborate, reuse, adapt and share slide content for educational purposes. The SlideWiki platform, which follows a similar approach to Wikipedia, is being developed as part of a European project of the same name2.  The SlideWiki platform decomposes learning materials into fine-grained artifacts (e.g., slides and decks) that are organized into tree structures; such artifacts are described semantically using ontologies. Educational artifacts (e.g., slides and decks) are internally represented using HTML, but import and export to widely-used presentation formats (e.g., PPTX) are available. In this paper we present a novel approach based on crowdsourcing to involve students in the creation and improvement of educational materials that serve for the study and/or self-learning of knowledge  1 http://slidewiki.org/ 2 http://slidewiki.eu Proceedings of ICERI2018 Conference 12th-14th November 2018, Seville, SpainISBN: 978-84-09-05948-53852included in university subjects. The creation of educational materials following this approach allows the development of an extensive repository of educational content that can be shared and reused within the educational community. In order to validate our proposal, the approach will be applied in ",
        "file_name": "10!21125%iceri!2018!1854.pdf",
        "file_path": ".\\PDFs\\10!21125%iceri!2018!1854.pdf"
    },
    {
        "title": "Machine Learning-based Query Augmentation for SPARQL Endpoints",
        "implementation_urls": [],
        "doi": "10.5220/0006925300570067",
        "arxiv": null,
        "abstract": "Abstract:Linked Data repositories have become a popular source of publicly-available data. Users accessingthis data through SPARQL endpoints usually launch several restrictive yet similar consecutivequeries, either to find the information they need through trial-and-error or to query related re-sources. However, instead of executing each individual query separately, query augmentation aimsat modifying the incoming queries to retrieve more data that is potentially relevant to subsequentrequests. In this paper, we propose a novel approach to query augmentation for SPARQL end-points based on machine learning. Our approach separates the structure of the query from itscontents and measures two types of similarity, which are then used to predict the structure andcontents of the augmented query. We test the approach on the real-world query logs of the Spanishand English DBpedia and show that our approach yields high-accuracy prediction. We also showthat, by caching the results of the predicted augmented queries, we can retrieve data relevant toseveral subsequent queries at once, achieving a higher cache hit rate than previous approaches.1 INTRODUCTIONLinked Data repositories have grown to providea wealth of publicly-available data, with somerepositories containing millions of concepts de-scribed by RDF triples (e.g. DBpedia1, FOAF2,GeoNames3). Users access the data in theserepositories through public SPARQL endpointsthat allow them to issue SPARQL queries, thestandard query language for RDF stores. Consec-utive queries received from the same client usuallyexhibit some patterns, such as querying identicalor similar resources than previous queries.Caching query results was first proposed tokeep recently retrieved data in a memory cachefor use with later queries (Dar et al., 1996; Mar-tin et al., 2010; Yang and Wu, 2011). However,caching only works if the exact same data is ac-cessed multiple times. In reality, it is more com-1DBpedia: https://wiki.dbpedia.org/2FOAF: http://www.foaf-project.org/3GeoNames: http://www.geonames.org/mon to have similar consecutive queries that re-trieve related resources from the repository (Boni-fati et al., 2017; Mario et al., 2011). Queryaugmentation takes advantage of this fact, re-trieving data that will potentially be used by fu-ture queries before the queries are received bythe SPARQL endopint. Previous approaches toquery augmentation are divided into two maincategories, (1) techniques based on informationfound in the data source, and (2) techniquesbased on analysis of previous (historic) queries,as discussed in section 2.In this paper, we present an approach to queryaugmentation for SPARQL endpoints based ondetecting recurring patterns in historic querylogs. The novelty of our approach is that we",
        "file_name": "10!5220%0006925300570067.pdf",
        "file_path": ".\\PDFs\\10!5220%0006925300570067.pdf"
    },
    {
        "title": "Fuzzy Semantic Labeling of Semi-structured Numerical Datasets",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-03667-6_2",
        "arxiv": null,
        "abstract": "Abstract. SPARQL endpoints provide access to rich sources of data(e.g. knowledge graphs), which can be used to classify other less struc-tured datasets (e.g. CSV files or HTML tables on the Web). We proposean approach to suggest types for the numerical columns of a collection ofinput files available as CSVs. Our approach is based on the applicationof the fuzzy c-means clustering technique to numerical data in the inputfiles, using existing SPARQL endpoints to generate training datasets.Our approach has three major advantages: it works directly with liveknowledge graphs, it does not require knowledge-graph profiling before-hand, and it avoids tedious and costly manual training to match val-ues with types. We evaluate our approach against manually annotateddatasets. The results show that the proposed approach classifies most ofthe types correctly for our test sets.Keywords: Fuzzy Clustering, Semantic Labeling, Semantic Web1 IntroductionA massive number of data are stored and made publicly available on the Webin semi-structured formats, such as spreadsheets. This is especially the case foropen data made available by public administrations, since the publication ofCSV data grants them three stars in the 5-star open data scheme1.A major drawback of the publication of data in spreadsheets is the difficultyfor potential data consumers to understand and interpret their content. This isbecause the terms used for column headings in these files are commonly not suf-ficiently informative and lack a data dictionary where their meaning is provided.Therefore, the automatic classification of such semi-structured data sources maybe useful to improve their usage. For example, such characterization may allowsearch engines to improve the relevancy of results [4]. It may also be used to(partially) automate the generation of mappings (e.g. RML [7] and R2RML [6])that may be used to generate RDF on the fly without actually transforming thedata.Meanwhile, data are also exposed on the Web by means of Linked Dataprinciples or via SPARQL endpoints, which can be considered as rich sources1 http://5stardata.info/en/https://doi.org/10.1007/978-3-030-03667-6_2.2 A. Alobaid et al.of more structured and well-described data. Our hypothesis is that such datacan be useful to train models that are able to characterize the numerical semi-structured data sources that we were referring to in the previous paragraph.In this paper, we describe an approach for the characterization of semi-structured data sources (e.g., CSVs) that uses the content available in SPARQLendpoints for such characterization. Our approach is based on the usage of thefuzzy c-means clustering technique. We have identified the following advantages:– It is domain agnostic. That is, it performs the semantic labeling of semi-structured data sources regardless of their domain, what makes it applicableto a wide range of datasets.– No manual training is needed. It does not require users to manually typesamples of the data beforehand or to use a training dataset that has beenconstructed before. Instead, it works with existing data available as SPARQLendpoints.– It does not require exact matches for the numerical values whose columnsit classifies. The correct typing of data sources is not prevented by having",
        "file_name": "10!1007%978-3-030-03667-6_2.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-03667-6_2.pdf"
    },
    {
        "title": "Assess and Enhancing Attention in Learning Activities",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-73210-7_93",
        "arxiv": null,
        "abstract": "Abstract. The rapid progress of technologies has enabled the development of innovative environment in learning activities when the student used computer devices with access to Internet. The goal of this paper is to propose an ambient intelligent (AmI) system, directed at the teacher that indicates the level of atten-tion of the students in the class when it requires the use of the computer con-nected to the Internet. This AmI system captures, measures, and supervises the interaction of each student with the computer (or laptop) and indicates the level of attention of students in the activities proposed by the teacher. When the teacher has big class, he/she can visualize in real time the level of engagement of the students in the proposed activities and act accordingly when necessary. Measurements of attention level are obtained by a proposed model, and user for training a decision support system that in a real scenario makes recommenda-tions for the teachers so as to prevent undesirable behaviour and change the learning styles. Keywords: Ambient Intelligent System, Learning Activities, Attentiveness, Learning Styles, and Innovative Environment 1 Introduction The rapid progress of wireless communication and sensing technologies has enabled the development of innovative environment in learning activities. For this reason making learning systems innovative and smart has been the objective of many re-searchers in both the fields of computer science and education [1]. In the field of computer science an innovative environment is a digitally augment-ed physical world where sensor-enabled and networked devices work continuously and collaboratively to make lives of inhabitants more comfortable. Indeed, tremen-dous advances in smart devices, wireless mobile communications, sensor networks, pervasive computing, machine learning, robotics, middleware and agent technologies, and human computer interfaces have made the dream of smart and innovative envi-ronments a reality.  In the field of education learning theories provide insights into the very complex processes and factors that influence learning and give precious information to be used in designing instruction that will produce optimum results. The learning models are designed in order to supply to the students with practice, evaluation and improvement procedures which will adjust the model [2]. The teaching process first requires that the instructor creates a pedagogical design of the objectives and determines the con-tent to be taught. Second, a pre-assessment is used to determine learning abilities. Third, pedagogical procedures are used when teaching is initiated. Finally, assessment is applied to determine what learners have achieved, and, according to the assessment results, instructors should use feedback to determine the cause of ineffective instruc-tion [2, 3]. It’s crucial to improve the learning process and to mitigate problems that might oc-cur in an environment with learning technologies. Besides, each student has its own particular way of assimilating knowledge, that is, his learning style. Learning styles specify a student’s own way of learning. Someone that has a specific learning style can have difficulties when submitted to another learning style. When the given in-struction style matches the student’s learning style, the process is maximized which guarantees that the student learns more and more easily. Attention is a very complex process through which one individual is able to continuously analyse a spectrum of stimuli and, in a sufficiently short amount of time, chose one to focus on. In most of us, who can only focus on a very reduced group of stimuli at a time, this implies ig-noring other perceivable stimuli and information.  ",
        "file_name": "10!1007%978-3-319-73210-7_93.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-319-73210-7_93.pdf"
    },
    {
        "title": "Machine Learning-based Query Augmentation for SPARQL Endpoints",
        "implementation_urls": [],
        "doi": "10.5220/0006925300570067",
        "arxiv": null,
        "abstract": "Abstract:Linked Data repositories have become a popular source of publicly-available data. Users accessingthis data through SPARQL endpoints usually launch several restrictive yet similar consecutivequeries, either to find the information they need through trial-and-error or to query related re-sources. However, instead of executing each individual query separately, query augmentation aimsat modifying the incoming queries to retrieve more data that is potentially relevant to subsequentrequests. In this paper, we propose a novel approach to query augmentation for SPARQL end-points based on machine learning. Our approach separates the structure of the query from itscontents and measures two types of similarity, which are then used to predict the structure andcontents of the augmented query. We test the approach on the real-world query logs of the Spanishand English DBpedia and show that our approach yields high-accuracy prediction. We also showthat, by caching the results of the predicted augmented queries, we can retrieve data relevant toseveral subsequent queries at once, achieving a higher cache hit rate than previous approaches.1 INTRODUCTIONLinked Data repositories have grown to providea wealth of publicly-available data, with somerepositories containing millions of concepts de-scribed by RDF triples (e.g. DBpedia1, FOAF2,GeoNames3). Users access the data in theserepositories through public SPARQL endpointsthat allow them to issue SPARQL queries, thestandard query language for RDF stores. Consec-utive queries received from the same client usuallyexhibit some patterns, such as querying identicalor similar resources than previous queries.Caching query results was first proposed tokeep recently retrieved data in a memory cachefor use with later queries (Dar et al., 1996; Mar-tin et al., 2010; Yang and Wu, 2011). However,caching only works if the exact same data is ac-cessed multiple times. In reality, it is more com-1DBpedia: https://wiki.dbpedia.org/2FOAF: http://www.foaf-project.org/3GeoNames: http://www.geonames.org/mon to have similar consecutive queries that re-trieve related resources from the repository (Boni-fati et al., 2017; Mario et al., 2011). Queryaugmentation takes advantage of this fact, re-trieving data that will potentially be used by fu-ture queries before the queries are received bythe SPARQL endopint. Previous approaches toquery augmentation are divided into two maincategories, (1) techniques based on informationfound in the data source, and (2) techniquesbased on analysis of previous (historic) queries,as discussed in section 2.In this paper, we present an approach to queryaugmentation for SPARQL endpoints based ondetecting recurring patterns in historic querylogs. The novelty of our approach is that we",
        "file_name": "10!5220%0006925300570067.pdf",
        "file_path": ".\\PDFs\\10!5220%0006925300570067.pdf"
    },
    {
        "title": "Assigning Creative Commons Licenses to Research Metadata: Issues and Cases",
        "implementation_urls": [],
        "doi": "10.1007/978-3-030-00178-0_16",
        "arxiv": "1609.05700",
        "abstract": "Abstract. This paper discusses the problem of lack of clear licensing and transparency of usage terms and conditions for research metadata. Making research data connected, discoverable and reusable are the key enablers of the new data revolution in research. We discuss how the lack of transparency hinders discovery of research data and make it disconnected from the publication and other trusted research outcomes. In addition, we discuss the application of Creative Commons licenses for research metadata, and provide some examples of the applicability of this approach to internationally known data infrastructures. Keywords. Semantic Web, research metadata, licensing, discoverability, data infrastructure, Creative Commons, open data  Introduction The emerging paradigm of open science relies on increased discovery, access, and sharing of trusted and open research data. New data infrastructures, policies, principles, and standards already provide the bases for data-driven research. For example, the FAIR Guiding Principles for scientific data management and stewardship [23] describe the four principles—findability, accessibility, interoperability, and reusability—that should inform how research data are produced, curated, shared, and stored. The same principles are applicable to metadata records, since they describe datasets and related research information (e.g. publications, grants, and contributors) that are essential for data discovery and management. Research metadata are an essential component of the open science ecosystem and, as stated in [17], “for a molecule of research metadata to                                                              1 Corresponding Author: marta.pobletbalcell@rmit.edu.au 1155810Texto escrito a máquina1155810Texto escrito a máquinaPreprint of: Poblet, Marta; Aryani, Amir; Manghi, Paolo; Unsworth, Kathryn; Wang, Jingbo; Hausstein,Brigitte;  \rDallmeier-Tiessen, Sunje; Klas, Claus-Peter; Casanovas, Pompeu; Rodriguez Doncel, Victor; \"Assigning Creative\rCommons Licenses to Research Metadata: Issues and Cases\" submited to 29 th International Conference on Legal Knowledge and Information Systems (JURIX 2016), Nice (France),14-16 December 2016.1155810Cuadro de textomove effectively between systems, the contextual information around it - the things that are linked to, must also be openly and persistently available”.  Yet, finding relevant, trusted, and reusable datasets remains a challenge for many researchers and their organisations. New discovery services address this issue by drawing on open public information, but the lack of transparency about legal licenses and terms of use for metadata records compromises their reuse. If licenses and terms of use are absent or ambiguous, discovery services lack basic information on how metadata records can be used, to what extent they can be transformed or augmented, or whether they can be utilised as part of commercial applications. Ultimately, legal uncertainty hinders investment and innovation in this domain. The rest of this paper is organised as follows: Section 1 presents the most widely adopted research metadata protocols and practices; Section 2 provides some global figures about the types of licenses used for research metadata; Section 3 identifies the main stakeholders; Section 4 reviews the most common choices for metadata licenses and discusses both advantages and disadvantages of such choices; Section 5 offers six compact case studies from different research data services. Finally, the conclusion raises some questions to guide future work.  1. Research metadata protocols and practices   A number of instruments covering the management of research metadata are currently available. For example, the Open Archives Initiative (OAI) developed the Protocol for ",
        "file_name": "10!1007%978-3-030-00178-0_16.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-030-00178-0_16.pdf"
    },
    {
        "title": "LawORDate: A service for distinguishing legal references from temporal expressions",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. References to documents in the legal domain usually followpatterns containing temporal information in different forms (e.g. ’Di-rective 2001/29’). These references mislead algorithms detecting puretemporal references, and false positives occur in named entity recogni-tion algorithms searching dates or intervals. This paper presents meth-ods and techniques to identify these references, applied to two differentdomains. The first domain is that of news, where the temporal infor-mation plays a crucial role for their understanding and automaticallybuilding timelines can be hampered by the errors induced from these le-gal references. The second domain is that dataset descriptions. Datasetdescriptions sometimes contain temporal information, not only in theirdedicated metadata fields (e.g. dataset creation) but also within the textof their description. LawORDate, the system presented in this paper, isa web service able to detect legal references with temporal informationin Spanish texts. The service identifies these references, avoiding theirannotation by temporal taggers and enabling a further step of linkingthe references to the original sources and building co-reference graphs.Keywords: legal references, temporal expressions, news, dataset de-scription1 IntroductionTemporal expressions detection, mainly focused on news, is a emerging fieldgaining more and more importance in NLP. Efforts such as the NewsReaderproject1 and the TempEval [1, 2] initiatives in SemEval, along with subsequentmore specific temporal tasks [3, 4] show the interest in processing the temporaldimension on all kind of texts. Usually processing of temporal expressions isdone regarding the concrete type of text being faced, both depending on its field(such as news, clinical domain or historical texts) or extension (free texts orlength-limited tweets). Due to this specialization, systems do not usually reactwell when they find expressions from other fields, such as is the case of legalreferences in news or dataset description.1 http://www.newsreader-project.eu/results/data/wikinews/Proceedings of the 1st Workshop on Technologies for Regulatory Compliance25The boom of open data portals also present this kind of mixed information.Thousands of datasets become publicly available everyday, sometimes presentingjust basic scarce metadata such as title and description. Being able to extractadditional information and new search parameters from them, such as named en-tities or temporal references, would facilitate managing them, along with linkingthem resources or queries.To this end, a system2 was built to extract temporal coverage from bothnews and related datasets in Spanish, some of them in the legal domain, and beable to link them based in the temporal dimension. This system calls an existingtemporal tagger, HeidelTime [5], able to detect temporal expressions in texts inSpanish and tag them following the TIMEX3 annotation standard. Nevertheless,this tagger happened to tag as temporal expressions references to Spanish lawsand legal documents that led to false positives, such as shown in the exampleexposed in Fig. 1, extracted from a real article3. The result of the tagging byHeidelTime can be found in Fig. 2.Estas actividades están reguladas por Real Decreto 1341/2007, de 11 de octubresobre la gestión de la calidad de las aguas de baño, incorporando al derecho español",
        "file_name": "04paper.pdf",
        "file_path": "PDFs\\04paper.pdf"
    },
    {
        "title": "Building the legal knowledge graph for smart compliance services in multilingual Europe",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract.  This position paper describes the vision, objectives and methodology of the LYNX project. The aim of Lynx is to create services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law and standards. Keywords. Compliance, Legal Knowledge Graph, regtech, regulatory compliance, semantic web Introduction The term compliance is widely used to refer “to the conformance to a set of laws, regulations, policies, or best practices” [1]. Every company performing almost any activity has some concern with compliance-related problems.  These problems are a bigger concern for small and medium-sized enterprises (SMEs), which cannot afford expensive consultancy services and they are a bigger hurdle for companies trying to sell abroad, as they usually lack the knowledge on the applicable conditions in the target country. According to the European Commission1, only 7% of European SMEs sell across borders, but those who do, exhibit 7% job growth and 26% innovate in their offering, greater numbers than the 1% and 8% of SMEs that do not go outside their local markets.  The European Commission is aware of these legal and language barriers and is trying to build a single market with less entry barriers. The LYNX project is a European research project funded as an H2020 Innovation Action covering the topic ICT-14: Big Data PPP: cross-sectorial and cross-lingual data integration and experimentation. The project is expected to last three years, starting in December 2017. This position paper briefly presents this research project. 1 http://europa.eu/rapid/press-release_IP-10-895_en.htm Proceedings of the 1st Workshop on Technologies for Regulatory Compliance151. The LYNX project Having identified these compliance problem, and having assumed that technology can help lowering these legal and language barriers, the main objective of Lynx can be stated as “to create an ecosystem of cloud services to better manage compliance, based on a legal knowledge graph which integrates and links heterogeneous compliance data sources including legislation, case law, standards and other aspects”. Other specific objectives follow:  to provide a platform that helps companies solving questions and cases related to compliance in different sectors and jurisdictions.  to help European SMEs to reduce costs and effort in organising and monitoring legislation, regulations, and sectorial good practices.  to provide citizens a better access to legal and regulatory information from multiple jurisdictions and to facilitate their in the legislative processes and in the writing process of standards.  to draw a legal knowledge graph across different jurisdictions, comprising legislation, case law, doctrine, standards, norms, and other documents.  to deliver the necessary domain-neutral common services (such as document annotation, interlinking, etc.) as building blocks to orchestrate aggregated business-oriented services.  to cover some particular business case necessities, validating the project’s developed ecosystem.  The referred legal knowledge graph is a collection of structured data and ",
        "file_name": "02paper.pdf",
        "file_path": "PDFs\\02paper.pdf"
    },
    {
        "title": "Architecting data science education",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Data scientists are currently among the most demanded professionals in many spheres, including industries, governments, public sector, among oth-ers. This is due to several good reasons. Probably an important one of those rea-sons is the growing demand to find proper ways to face the challenges of estab-lishing data-driven economies and societies. As academics and educationalists, but also Data Science professionals, we look at how to bring up this kind of specialists such that to meet the current shortages but also mid-term demands. In this position paper we deliberate about how to architect thematically, didacti-cally, and organizationally a university program under the thematic umbrella of Data Science. We focus on the selection of learning units or disciplines to be covered in order to produce the M.Sci. and Ph.D. graduates who will be ready to face the future challenges in the mid-term perspective. We outline our rec-ommendation on using learning tools and materials. We also concisely present the approach for stimulating competitive and cooperative atmosphere in the class that stimulates intensive collective and individual learning. We recom-mend to reinforce an academic program by involving industrial partners inten-sively in the process. We ground our deliberation on our experience in imple-menting relevant M.Sci. and Ph.D. programs in Data Science and Semantic Technologies. Keywords: Data Science education, topical scope, program structure, learning tools, didactics, collaboration with industries.  1 Introduction The boost in the abundance, complexity, and variety of data in all spheres of human activity is a phenomenon that leaves a rare information professional negligent these days. Industries are entering into data driven economy, which demands having and using data as a primary asset. On the other hand, the shift to more intensive use of data results in the increase of data generation and storage at unprecedented scales in terms of volumes and rates. A few topical examples are as follows (c.f. [1]): https://orcid.org/0000-0002-5159-254X“Exponential growth of data volumes is accelerated by the dramatic increase of social networking applications that allow non-specialist users create a huge amount of content easily and freely. Equipped with rapidly evolving mobile devices, a user is becoming a nomadic gateway boosting the generation of ad-ditional real-time sensor data. The emerging Internet of Things makes every-thing a data or content, adding billions of additional artificial and autonomic sources of data to the overall picture. Smart spaces, where people, devices, and their infrastructure are all loosely connected, also generate data of unprece-dented volumes and with velocities rarely observed before.” Hence, data generation is a phenomenon that fuels itself and so far we do not ob-serve any signs of saturation for this process. Straightforwardly, the societal demand for the professionals capable of efficient and effective processing of these data also increases at unprecedented rate. These gave rise to Data Science as a discipline and community. As denoted by Hoehndorf and Queralt-Rosinach [2]: “Data Science has as its subject matter the extraction of knowledge from data. While data has been analyzed and knowledge extracted for millennia, the rise of “Big” data has led to the emergence of Data Science as its own discipline that studies how to translate data through analytical algorithms typically taken from statistics, machine learning or data mining, and turning it into knowledge. Data Science also encompasses the study of principles and methods to store, process and communicate with data throughout its life cycle, and starts just af-",
        "file_name": "paper_266.pdf",
        "file_path": "PDFs\\paper_266.pdf"
    },
    {
        "title": "Challenge-based learning in computational biology and data science",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Data Science is an interdisciplinary field devoted to extractknowledge from large amounts of data. There is a great variety of pro-grams that address the teaching of this field with a growing demand ofprofessionals. However, data science pedagogy tends to emphasize gen-eral aspects of data and the use of tools instead of the its scientificdimension. This position paper describes an ongoing educational inno-vation project for the use of the Challenge-based Learning approach toteach and learn Data Science. In this approach, students work on solv-ing complex and real world problems while the learning is obtained byiterating through three main phases: engage, investigate, and act.Keywords: Challenge-based learning, active learning, experiential learn-ing, project based learning, data science, computational biology.1 IntroductionData science (DS) is an interdisciplinary field devoted to identify patterns and ex-tract knowledge by mining large amounts of structured and unstructured data.Among others, DS includes: machine learning, data processing, statistical re-search, and their related methods. This science has become a revolution thathas changed our manner of doing business, health, politics, education and in-novation [11]. Scientific breakthroughs will be increasingly assisted by advancedcomputing capabilities and DS methods that help researchers manipulate andexplore massive datasets [9].Challenge-based learning (CBL) is a new learning approach created by AppleInc. in collaboration with teachers and leaders in the education community. CBLis “an engaging, multidisciplinary approach that starts with standards-basedcontent and lets students leverage the technology they use in their daily livesto solve complex, real-world problems” [5]. In CBL, students work with otherstudents, their teachers, and experts in their communities and around the worldto develop deeper knowledge of the subjects they are studying.Data science is in a privileged position with respect to other branches ofknowledge to articulate learning through experiences and challenges [18]. The? ORCID ID: 0000-0001-7587-0703?? ORCID ID: 0000-0002-0792-4156Kaggle platform [2] periodically releases a series of competitions on real problemssuch as “Predicting a Biological Response” [4]; which offered 20,000$ to thebest predictive model that linked a biological response of molecules to theirchemical properties. These public competitions have the potential to involveactively the student in a real, significant, and related problematic situation;including a framework for the implementation of a solution to the challenge.This position paper presents an ongoing educational innovation project fo-cusing on using the CBL approach in a DS course, as part of a ComputationalBiology master degree at the Technical University of Madrid (UPM). Studentswill work on challenges at the level of a Kaggle competition with special pref-erence for active and multidisciplinary problems. Based on the 2016 update forthe CBL framework proposed by Apple Inc. [12], students will learn by followingthree main phases: engage, investigate, and act.The paper outline is as follows: after describing the background of the pre-sented innovation project in section 2, the project details are given in section3. These include the scope and students’ profile, the project goals, the time-line and educational resources, the evaluation, resulting products, and diffusionplan. Section 4 explains the expected contribution to the improvement of learn-",
        "file_name": "paper_236.pdf",
        "file_path": "PDFs\\paper_236.pdf"
    },
    {
        "title": "Refining terminological saturation using string similarity measures",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. This paper reports on the refinement of the THD algorithm, developed in the OntoElect framework. This baseline THD algorithm used exact string matches for key term comparison. It has been refined by introducing an appro-priate string similarity metric for grouping the terms having similar meaning and looking similar as text strings. To choose the most appropriate metric, several existing metrics have been cross-evaluated on the developed test set of multi-word terms in English. The rationale for creating this test set is also presented. Further, the refined algorithm for measuring terminological difference has been cross-evaluated with the baseline THD algorithm. For this cross-evaluation, the bags of terms extracted from the TIME collection of scientific papers were used. The experiment revealed that using the refined algorithm yielded better and quicker terminological saturation, compared to the baseline. Keywords: Automated Term Extraction, OntoElect, Terminological Difference, Key Term, Linguistic Similarity Metric, Bag of Terms, Terminological Satura-tion. 1 Introduction The research presented in this paper is the part of the development of the methodolog-ical and instrumental components for extracting representative (complete) sets of sig-nificant terms from the representative sub-collections of textual documents having min-imal possible size. These terms are further interpreted as the required features for engi-neering an ontology in a particular domain of interest. Therefore, it is assumed that the documents in a collection cover a single and well circumscribed domain. The main hypothesis, put forward in this work, is that a sub-collection can be considered as rep-resentative to describe the domain, in terms of its terminological footprint, if any addi-tions of extra documents from the entire collection to this sub-collection do not notice-ably change this footprint. Such a sub-collection is further considered as complete and mailto:aluonac@i.uamailto:rodeonpopov@gmail.com therefore yields a representative bag of significant terms describing its domain. The approach to assess the representativeness does so by evaluating terminological satura-tion in a document (sub-)collection [1], [31]. Detecting saturation is done by measuring terminological difference (thd) among the pairs of the consecutive incrementally enlarged datasets, as described in Section 4. This set measure is of course based on measuring differences between individual terms.   A (baseline) THD algorithm [1] has been developed and implemented in the OntoElect project1. This THD algorithm, however, uses a simple string equivalence check for de-tecting similar individual terms. The objective of the research presented in this paper was to find out if it is possible to achieve better performance in measuring terminolog-ical difference by using a proper string similarity measure to compare individual terms.  The remainder of the paper is structured as follows. Section 2 reviews the related work. Section 3 reports on the implementation of the chosen string similarity measures and selecting the proper term similarity thresholds for their use. Section 4 sketches out the approach of OntoElect for measuring thd and our refinement of the baseline THD algorithm. Section 5 presents the set-up and results of our evaluation experiments. Our conclusions and plans for the future work are given in Section 6.  2 Related Work The work reported in this paper aims at improving the measures of terminological dif-ference between the bags of terms extracted from textual documents. The improvement is sought via the proper choice and use of existing string metrics for measuring linguis-",
        "file_name": "10000003.pdf",
        "file_path": "PDFs\\10000003.pdf"
    },
    {
        "title": "Ontological requirement specification for smart irrigation systems: A SOSA/SSN and SAREF comparison",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": null,
        "file_name": "document",
        "file_path": "PDFs\\document.pdf"
    },
    {
        "title": "Besoins ontologiques d'un système d'irrigation intelligent: Comparaison entre SSN et SAREF",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": null,
        "file_name": "document",
        "file_path": "PDFs\\document.pdf"
    },
    {
        "title": "Towards integrating public procurement data into a semantic knowledge graph",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Public procurement accounts for a substantial part of thepublic investment and global economy. Therefore, improving effectiveness,efficiency, transparency and accountability of government procurement isof broad interest. To this end, in this poster paper, we present our approachfor integrating procurement data, including public spending and corporatedata, from multiple sources across the EU into a semantic knowledge graph.We are aiming to improve procurement processes through supportingmultiple stake holders, such as government agencies, companies, controlauthorities, journalists, researchers, and individual citizens.Keywords: Knowledge graph · Public procurement · Ontology.1 IntroductionPublic procurement accounts for a substantial part of the public investment andglobal economy. Every year, over 250 000 public authorities in the EU spendaround 14% of GDP on the purchase of services, works and supplies1. Therefore,improving effectiveness, efficiency, transparency and accountability of governmentprocurement is of broad interest [1]. To this end, European Commission hasput several relevant directives forward, i.e., for public sector information (e.g.,Directive 2003/98/EC) and public procurement (e.g., Directive 2014/24/EU8),to improve public procurement practices. As a result of these, national publicprocurement portals have been created, which live together with regional, localas well as EU-wide public procurement portals. However, there is no commonagreement across the EU (not even, in many cases, inside the same country)on the data formats to be used for exposing such data sources and on the datamodels that need to be used for exposing such data, which leads to a largeheterogeneity in the data that is being exposed.? This work is funded by EU H2020 TheyBuyForYou project (780247).?? Corresponding author. Email: ahmet.soylu@sintef.no1 https://ec.europa.eu/growth/single-market/public-procurement_enhttps://ec.europa.eu/growth/single-market/public-procurement_enIn Europe, contracting portals like Tenders Electronic Daily2 (TED) may beseen as a way to homogenise the data that is being provided, but unfortunatelythis portal is only used for those contracts that are larger than a predefinedbudget threshold, and hence this does not cover the whole richness of types ofpublic contracts nor does it force the usage of this format for those contracts thatdo not need to be published there. The only relevant data model that is gettingsome important traction worldwide is the Open Contracting Data Standard3(OCDS). However, it has been mostly developed with a focus on transparency inthe public contracting procedures. Though, several ontologies, such as LOTED2[2], PPROC [3], PCO[4] and upcoming eProcurement ontology4, are developedwith different levels of detail and focus for representing procurement data, there isno solution integrating supplier and procurement data enabling such as matchingof suppliers and buyers and advanced analytics and procurement intelligence.In this poster paper, we present our approach, in the context of TheyBuy-ForYou5 project, for integrating procurement data, including public spending andcorporate data, from multiple sources across the EU into a knowledge graph. Weare aiming to improve procurement processes through supporting multiple stakeholders, such as government agencies, companies, control authorities, journalists,researchers, and individual citizens. The proposed solution enables developersto create fully functional, robust, and scalable data integration pipelines, fromincluding sourcing the data, to pre-processing, augmenting, and interlinking it.",
        "file_name": "ekaw-poster-01.pdf",
        "file_path": "PDFs\\ekaw-poster-01.pdf"
    },
    {
        "title": "NLP4Types: predicting types using NLP",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "Abstract. Type inference for resources in Knowledge Graphs is a widelystudied problem, for which different approaches have been proposed, in-cluding reasoning, statistical analysis, and the usage of the textual in-formation related to the resources. We focus on the latter, exploitingtext classification techniques for predicting semantic types from textualdescriptions. In this paper we introduce NLP4Types, an online tool thatcombines different standard NLP techniques and classifiers for predicttypes based on DBpedia abstracts, as well as to collect feedback fromthe users for those predictions.Keywords: DBpedia, Natural Language Processing, Data Quality, Linked Data1 IntroductionType statements, that is, assertions of types for entities, are the most basic andfundamental piece of information for semantic resources. This information can begenerated by different means, including manual, automated and semi-automatedapproaches. In this paper we cover DBpedia, which is generated automaticallyfrom the information contained in Wikipedia, using a set of translation map-pings, from the entries in tabular format contained the infoboxes of each page.As not all pages contain infoboxes it is not always possible to generate type infor-mation. According to our calculation, around a 16% of resources from Wikipediado not have any type mapped to DBpedia. We have also to take into accountthat, even in those cases in which this information can be generated, it is not al-ways complete or correct, as mappings are defined manually and collaborativelyby users.In this paper we explore how textual abstracts can be exploited, using NLPtechniques, to classify entries into the DBpedia ontology. We combine document-to-term matrix and Named Entity Recognition to train a model that we lateruse to predict types from free text on our tool. We have evaluated our modelusing K-fold evaluation and a well-known gold standard, obtaining high results.The final result of this process is NLP4Types1, an online tool that allows userto explore type predictions and to collect feedback from them.∗This work was partially funded by projects RTC-2016-4952-7, TIN2013-46238-C4-2-R and TIN2016-78011-C4-4-R, from the Spanish State Investigation Agency of theMINECO and FEDER Funds.1http://nlp4types.linkeddata.es2 Related WorkTyping resources on large datasets is a widely studied problem that has beenaddressed during last decade, being SDType [7] the most prominent system.SDType exploits the statistical information of property distribution to infer newtyping statements. Other approaches have been introduced, exploiting differentNLP-based techniques for type assignment based on text [3, 2]. In [5] a hier-archy of Support Vector Machines (hSVM) is introduced for applying lexico-syntactic patterns using a bag-of-words model, extracted from short abstractsand Wikipedia categories. This work extends the Linked Hypernym DatasetFramework [4], by the same authors, for extracting these pattern-based struc-tures. These works introduce also the LHD Gold Standard dataset, which we usein this paper, to measure the performance of our system and compare it to otherexisting tools. This gold standard has been produced, as reported by authors,using experts to assign types to a subset of the English DBpedia resources. Wehave used it to evaluate our system, as it provides means for comparing ourcontribution to both, hSVM and SDType.",
        "file_name": "ekaw-demo-19-poster.pdf",
        "file_path": "PDFs\\ekaw-demo-19-poster.pdf"
    },
    {
        "title": "A comprehensive quality model for Linked Data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170267",
        "arxiv": null,
        "abstract": "classes and properties in daQ are defined as abstract and, therefore, they are not directly used. Instead, the intended use of this ontology implies the creation of specific classes and properties defined as subclasses and sub-properties of those defined in daQ. This means that, unlike in Q M O and E V A L , where the elements such as measures and characteristics are defined as in­stances, when using daQ these elements are mainly de­fined as classes. In the context of the conceptual model, daQ is equivalent to Q M O , with the difference of the usage of different terminology. The Data Quality Vocabulary ̂ ( D Q V ) is an ontol­ogy for representing the quality of dataseis that is be­ing developed by the W 3 C . Similarly as daQ, and un­like Q M O and E V A L , D Q V is an ontology specifically developed having in mind dataseis. Currently, D Q V provides classes and properties for capturing informa­tion about quality categories, dimensions and metrics of a dataset, as well as about quality certificates, stan­dards and provenance related to a dataset. However, at this point in time D Q V is still under development and changes to the current design can be expected in the future. In the context of the conceptual model, D Q V tends to provide the means for capturing both the de­tails about quality (i.e., characteristics and measures) and about quality values (results of evaluation). Fur­thermore, although D Q V is specifically designed for dataseis, it does not provide the means to describe some specific aspects of Linked Data. With respect to the state of the art described in this section, this paper contributes with the extension of ex­isting ontologies in order to enable capturing informa­tion related to Linked Data which is not covered by the existing ontologies, as well as with bringing existing ontologies under unique umbrella by connecting their semantically related concepts. 3. Quality model for Linked Data This section describes a quality model for Linked Data and how it was defined using the bottom-up methodology proposed by Radulovic et al. [11]. The starting point for the definition of the quality model was the state of the art in Linked Data quality assess-'http://purl.org/net/EvaluatlonResult# http://www.w3 . org/IR/vocab-dgv/ http://purl.org/net/EvaluatlonResult%23http://www.w3ment and specification, and in particular the work done by Zaveri et al. [16]. Since the quality model presented in this section describes a classification of quality mea­sures (i.e., base measures, derived measures and indi­cators), w e have decided to adopt the terminology as ",
        "file_name": "10!3233%sw-170267.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-170267.pdf"
    },
    {
        "title": "The apertium bilingual dictionaries on the web of data",
        "implementation_urls": [],
        "doi": "10.3233/sw-170258",
        "arxiv": null,
        "abstract": "Abstract. Bilingual electronic dictionaries contain collections of lexical entries in two languages, with explicitly declared trans-lation relations between such entries. Nevertheless, they are typically developed in isolation, in their own formats and accessiblethrough proprietary APIs. In this paper we propose the use of Semantic Web techniques to make translations available on theWeb to be consumed by other semantic enabled resources in a direct manner, based on standard languages and query means.In particular, we describe the conversion of the Apertium family of bilingual dictionaries and lexicons into RDF (Resource De-scription Framework) and how their data have been made accessible on the Web as linked data. As a result, all the converteddictionaries (many of them covering under-resourced languages) are connected among them and can be easily traversed from oneto another to obtain, for instance, translations between language pairs not originally connected in any of the original dictionaries.Keywords: Linguistic linked data, multilingualism, Apertium, bilingual dictionaries, lexicons, lemon, translation1. IntroductionThe publication of bilingual and multilingual lan-guage resources as linked data (LD) on the Web canlargely benefit the creation of the critical mass of cross-lingual connections required by the vision of the mul-tilingual Web of Data [9]. The benefits of sharing lin-guistic information on the Web of Data have beenrecently recognized by the language resources com-munity, which has shown increasing interest in pub-lishing their linguistic data and metadata as LD onthe Web [2]. As a result of interlinking multilingualand open language resources, the Linguistic LinkedOpen Data (LLOD) cloud is emerging,1 that is, a new*Corresponding author. E-mail: jgracia@fi.upm.es.1An updated picture of the current LLOD cloud can be found athttp://linguistic-lod.org/linguistic ecosystem based on the LD principles thatwill allow the open exploitation of such data at globalscale.In this article we will focus on the case of electronicbilingual dictionaries as a particular type of languageresources. Bilingual dictionaries are specialized dictio-naries that describe translations of words or phrasesfrom one language to another. They can be unidirec-tional or bidirectional, allowing translation, in the lat-ter case, to and from both languages. A bilingual dic-tionary can contain additional information such as partof speech, gender, declension model and other gram-matical properties.Electronic bilingual dictionaries have been typicallydeveloped in isolation, in their own formats and acces-sible through proprietary APIs. We propose the use ofSemantic Web techniques to make translations avail-mailto:jgracia@fi.upm.esmailto:asun@fi.upm.esmailto:marta.villegas@upf.edumailto:nuria.bel@upf.edumailto:jgracia@fi.upm.eshttp://linguistic-lod.org/able on the Web to be consumed by other semantic en-abled resources in a direct manner, based on standard",
        "file_name": "10!3233%sw-170258.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-170258.pdf"
    },
    {
        "title": "Predicting the risk of suffering chronic social exclusion with machine learning",
        "implementation_urls": [],
        "doi": "10.1007/978-3-319-62410-5_16",
        "arxiv": null,
        "abstract": "Abstract. The fight against social exclusion is at the heart of the Eu-rope 2020 strategy: 120 million people are at risk of suffering this con-dition in the EU. Risk prediction models are widely used in insurancecompanies and health services. However, the use of these models to allowan early detection of social exclusion by social workers is not a commonpractice. This paper describes a data analysis of over 16K cases withover 60 predictors from the Spanish region of Castilla y León. The useof machine learning paradigms such as logistic regression and randomforest makes possible a high precision in predicting chronic social exclu-sion. The paper is complemented with a responsive web available onlinethat allows social workers to calculate the risk of a social exclusion caseto become chronic through a smartphone.Keywords: Social exclusion, Social services, Data analysis, Machinelearning, Data mining.1 IntroductionSocial exclusion is a complex and multi-dimensional process involving the lack ofresources, rights, goods and services, and the inability to participate in the nor-mal relationships and activities, available to most people in a society, whether ineconomic, social, cultural or political scopes [8]. Social exclusion affects not onlythe quality of life of individuals, but also the equity and cohesion of society as awhole. The economic crisis is undermining the sustainability of social protectionsystems in the EU [1]: 24% of all the EU population (over 120 million people)are at risk of poverty or social exclusion [1]. The fight against poverty and socialexclusion is at the heart of the Europe 2020 strategy for smart, sustainable andinclusive growth.In chronic medical diseases, there is strong evidence supporting that earlydetection results in less severe outcomes. This paper intends to provide socialworkers with methods and tools to bring this early detection, which is so bene-ficial in the medical field, to the challenging problem of chronic social exclusion.To this purpose, the paper contributes with (1) an analysis of the social servicesdata of Castilla y León (CyL), which is the largest region in Spain and countswith around two and a half million inhabitants. This analysis allows getting2 Emilio Serrano et al.insights into why social exclusion can become chronic. Furthermore, a (2) ma-chine learning model capable of quantifying the risk of chronic social exclusionis build. Finally, a (3) a responsive web is deployed to allow queries by socialworkers through a number of devices such as smartphones, tablets, or laptops.A RESTful web service is also provided to integrate the predictive capabilitiesinto other software applications.The paper outline is as follows. After revising some of the most relevantrelated works in section 2, the process used to analyze the data is explained insection 3. Section 4 reports the outcomes of the experiments conducted. Section5 explains, analyzes, and compares the results. Section 6 introduces the webservice implemented. Finally, section 7 concludes and offers future works.2 Related worksRisk prediction models are widely used in insurance companies to allow cus-tomers to estimate their policies cost. Manulife Philippines [2] offers a numberof online tools to calculate the likelihood of disability, critical illness, or deathbefore the age of 65; based on age, gender, and smoking status. Health is anotherapplication field where risk estimations are undertaken for preventive purposes.",
        "file_name": "10!1007%978-3-319-62410-5_16.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-319-62410-5_16.pdf"
    },
    {
        "title": "A guideline for reporting experimental protocols in life sciences",
        "implementation_urls": [],
        "doi": "10.7717/peerj.4795",
        "arxiv": null,
        "abstract": "ABSTRACTExperimental protocols are key when planning, performing and publishing researchin many disciplines, especially in relation to the reporting of materials and methods.However, they vary in their content, structure and associated data elements. This articlepresents a guideline for describing key content for reporting experimental protocolsin the domain of life sciences, together with the methodology followed in order todevelop such guideline. As part of our work, we propose a checklist that contains 17 dataelements that we consider fundamental to facilitate the execution of the protocol. Thesedata elements are formally described in the SMART Protocols ontology. By providingguidance for the key content to be reported, we aim (1) to make it easier for authorsto report experimental protocols with necessary and sufficient information that allowothers to reproduce an experiment, (2) to promote consistency across laboratories bydelivering an adaptable set of data elements, and (3) to make it easier for reviewers andeditors to measure the quality of submitted manuscripts against an established criteria.Our checklist focuses on the content, what should be included. Rather than advocatinga specific format for protocols in life sciences, the checklist includes a full descriptionof the key data elements that facilitate the execution of the protocol.Subjects Biochemistry, Biotechnology, Cell Biology, Molecular Biology, Plant ScienceKeywords Checklist, Experimental protocols, Guidelines, Recommendations, Good practices forreporting protocols, Open science, ReproducibilityINTRODUCTIONExperimental protocols are fundamental information structures that support thedescription of the processes by means of which results are generated in experimentalresearch (Giraldo et al., 2017; Freedman, Venugopalan & Wisman, 2017). Experimentalprotocols, often as part of ‘‘Materials and Methods’’ in scientific publications, are centralfor reproducibility; they should include all the necessary information for obtainingconsistent results (Casadevall & Fang, 2010; Festing & Altman, 2002). Although protocolsare an important component when reporting experimental activities, their descriptionsare often incomplete and vary across publishers and laboratories. For instance, whenreporting reagents and equipment, researchers sometimes include catalog numbersand experimental parameters; they may also refer to these items in a generic manner,e.g., ‘‘Dextran sulfate, Sigma-Aldrich’’ (Karlgren et al., 2009). Having this information isimportant because reagents usually vary in terms of purity, yield, pH, hydration state,How to cite this article Giraldo et al. (2018), A guideline for reporting experimental protocols in life sciences. PeerJ 6:e4795; DOI10.7717/peerj.4795https://peerj.commailto:ogiraldo@fi.upm.eshttps://peerj.com/academic-boards/editors/https://peerj.com/academic-boards/editors/http://dx.doi.org/10.7717/peerj.4795http://creativecommons.org/licenses/by/4.0/http://creativecommons.org/licenses/by/4.0/http://dx.doi.org/10.7717/peerj.4795grade, and possibly additional biochemical or biophysical features. Similarly, experimentalprotocols often include ambiguities such as ‘‘Store the samples at room temperature untilsample digestion’’ (Brandenburg et al., 2002); but, how many Celsius degrees? What is theestimated time for digesting the sample? Having this information available not only savestime and effort, it also makes it easier for researchers to reproduce experimental results;adequate and comprehensive reporting facilitates reproducibility (Freedman, Venugopalan& Wisman, 2017; Baker, 2016).",
        "file_name": "10!7717%peerj!4795.pdf",
        "file_path": ".\\PDFs\\10!7717%peerj!4795.pdf"
    },
    {
        "title": "Efficient Clustering from Distributions over Topics",
        "implementation_urls": [
            {
                "identifier": "https://doi.org/10.5281/zenodo.931305",
                "type": "zenodo",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "bidir",
                        "location": "ZENODO",
                        "location_type": "TITLE",
                        "source": "SSKG"
                    }
                ]
            }
        ],
        "doi": "10.1145/3148011.3148019",
        "arxiv": "2012.08206",
        "abstract": "ABSTRACT�ere are many scenarios where we may want to �nd pairs of tex-tually similar documents in a large corpus (e.g. a researcher doingliterature review, or an R&D project manager analyzing projectproposals). To programmatically discover those connections canhelp experts to achieve those goals, but brute-force pairwise com-parisons are not computationally adequate when the size of thedocument corpus is too large. Some algorithms in the literaturedivide the search space into regions containing potentially simi-lar documents, which are later processed separately from the restin order to reduce the number of pairs compared. However, thiskind of unsupervised methods still incur in high temporal costs. Inthis paper, we present an approach that relies on the results of atopic modeling algorithm over the documents in a collection, asa means to identify smaller subsets of documents where the simi-larity function can then be computed. �is approach has provedto obtain promising results when identifying similar documentsin the domain of scienti�c publications. We have compared ourapproach against state of the art clustering techniques and withdi�erent con�gurations for the topic modeling algorithm. Resultssuggest that our approach outperforms (> 0.5) the other analyzedtechniques in terms of e�ciency.CCS CONCEPTS•Mathematics of computing ! Probability and statistics; •Information systems!Document topic models; •Appliedcomputing ! Document management and text processing;KEYWORDStopic models; semantic similarity; large-scale text analysis; schol-arly data1 INTRODUCTIONGiven the huge amount of information about any domain that isbeing produced or captured daily, it becomes crucial to providemechanisms for automatically identifying the elements that canbring value for the involved agents (general consumers, experts,�is work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R,�nanced by the Spanish Ministry MINECO and co-�nanced by FEDER..Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro�t or commercial advantage and that copies bear this notice and the full citationon the �rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi�ed. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci�c permission and/or afee. Request permissions from permissions@acm.org.K-CAP 2017, Austin, TX, USA© 2017 ACM. 978-1-4503-5553-7/17/12. . .$15.00DOI: 10.1145/3148011.3148019companies, investors…) and discard the noisy, non-relevant infor-mation. Much of the information is presented in the form of textualdocuments, making necessary for experts to browse through manyof these texts to �nd relevant data. A way to explore the knowledge",
        "file_name": "10!1145%3148011!3148019.pdf",
        "file_path": ".\\PDFs\\10!1145%3148011!3148019.pdf"
    },
    {
        "title": "Distributing Text Mining tasks with <i>librAIry</i>",
        "implementation_urls": [],
        "doi": "10.1145/3103010.3121040",
        "arxiv": null,
        "abstract": "ABSTRACTWe present librAIry, a novel architecture to store, process and an-alyze large collections of textual resources, integrating existingalgorithms and tools into a common, distributed, high-performancework�ow. Available text mining techniques can be incorporated asindependent plug&play modules working in a collaborative mannerinto the framework. In the absence of a pre-de�ned �ow, librAIryleverages on the aggregation of operations executed by di�erentcomponents in response to an emergent chain of events. Extensiveuse of Linked Data (LD) and Representational State Transfer (REST)principles are made to provide individually addressable resourcesfrom textual documents. We have described the architecture designand its implementation and tested its e�ectiveness in real-worldscenarios such as collections of research papers, patents or ICT aids,with the objective of providing solutions for decision makers andexperts in those domains. Major advantages of the framework andlessons-learned from these experiments are reported.CCS CONCEPTS•Applied computing→Documentmanagement and text pro-cessing; •Computer systems organization → Architectures;KEYWORDSlarge-scale text analysis; NLP; scholarly data; text mining; dataintegration1 INTRODUCTIONGiven the huge amount of textual data about any domain that isdaily being produced or captured in any imaginable domain, itbecomes crucial to provide mechanisms for programmatically pro-cessing this raw data so we can make sense out of it: discarding allthe noisy, non-relevant information and keeping only the data thatcan bring value for the involved agents (general consumers, experts,companies, investors…). While some speci�c tools already allowfor advanced sense-making operations, others opt for composing a�is work is supported by project Datos 4.0 with reference TIN2016-78011-C4-4-R,�nanced by the Spanish Ministry MINECO and co-�nanced by FEDER.Author’s addresses: C. Badenes-Olmedo and J.L. Redondo-Garcı́a and O. Corcho ,Ontology Engineering Group, Universidad Politécnica de Madrid.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor pro�t or commercial advantage and that copies bear this notice and the full citationon the �rst page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permi�ed. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior speci�c permission and/or afee. Request permissions from permissions@acm.org.DocEng ’17, September 04–07, 2017, Valle�a, Malta.© 2017 ACM. 978-1-4503-4689-4/17/09. . .$15.00DOI: h�ps://doi.org/10.1145/3103010.3121040solution where di�erent analysis techniques are integrated under auniform data schema. However, this integration involves signi�-cant e�orts on reconciling data sources, coordinating processing",
        "file_name": "10!1145%3103010!3121040.pdf",
        "file_path": ".\\PDFs\\10!1145%3103010!3121040.pdf"
    },
    {
        "title": "Quantifying Reproducibility in Computational Biology: The Case of the Tuberculosis Drugome",
        "implementation_urls": [],
        "doi": "10.1371/journal.pone.0080278",
        "arxiv": null,
        "abstract": "AbstractHow easy is it to reproduce the results found in a typical computational biology paper? Either through experience orintuition the reader will already know that the answer is with difficulty or not at all. In this paper we attempt to quantify thisdifficulty by reproducing a previously published paper for different classes of users (ranging from users with little expertiseto domain experts) and suggest ways in which the situation might be improved. Quantification is achieved by estimatingthe time required to reproduce each of the steps in the method described in the original paper and make them part of anexplicit workflow that reproduces the original results. Reproducing the method took several months of effort, and requiredusing new versions and new software that posed challenges to reconstructing and validating the results. The quantificationleads to ‘‘reproducibility maps’’ that reveal that novice researchers would only be able to reproduce a few of the steps in themethod, and that only expert researchers with advance knowledge of the domain would be able to reproduce the methodin its entirety. The workflow itself is published as an online resource together with supporting software and data. The paperconcludes with a brief discussion of the complexities of requiring reproducibility in terms of cost versus benefit, and adesiderata with our observations and guidelines for improving reproducibility. This has implications not only in reproducingthe work of others from published papers, but reproducing work from one’s own laboratory.Citation: Garijo D, Kinnings S, Xie L, Xie L, Zhang Y, et al. (2013) Quantifying Reproducibility in Computational Biology: The Case of the TuberculosisDrugome. PLoS ONE 8(11): e80278. doi:10.1371/journal.pone.0080278Editor: Christos A. Ouzounis, The Centre for Research and Technology, Hellas, GreeceReceived September 18, 2012; Accepted October 10, 2013; Published November 27, 2013Copyright: � 2013 Garijo et al. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permitsunrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.Funding: This research is sponsored by Elsevier Labs, the National Science Foundation with award number -0 , the Air Force Office of ScientificResearch with award number FA9550-11-1-0104, internal funds from the University of Southern California’s Information Sciences Institute and from the Universityof California, San Diego, and by a Formacistudy design, data collection and analysis, decision to publish, or preparation of the manuscript.Competing Interests: The research presented here has been sponsored partly by Elsevier Labs. This does not alter the authors’ adherence to all the PLOS ONEpolicies on sharing data and materials.* E-mail: pbourne@ucsd.edu (PEB); gil@isi.edu (YG)IntroductionComputation is now an integral part of the biological scienceseither applied as a technique or as a science in its own right -bioinformatics. As a technique, software becomes an instrument toanalyze data and uncover new biological insights. By reading thepublished article describing these insights, another researcherhopes to understand what computations were carried out, replicatethe software apparatus originally used and reproduce theexperiment. This is rarely the case without significant effort, andsometimes impossible without asking the original authors. In short,reproducibility in computational biology is aspired to, but rarelyachieved. This is unfortunate since the quantitative nature of thescience makes reproducibility more obtainable than in cases whereexperiments are qualitative and hard to describe explicitly.An intriguing possibility where potential quantification exists isto extend articles through the inclusion of scientific workflows thatrepresent computations carried out to obtain the published results,thereby capturing data analysis methods explicitly [1]. This wouldmake scientific results more reproducible because articles wouldhave not only a textual description of the computational processdescribed in the article but also a workflow that, as acomputational artifact, could be analyzed and re-run automati-cally. Consequently, workflows can make scientists more produc-",
        "file_name": "journal.pone.0080278&type=printable",
        "file_path": "PDFs\\journal.pone.0080278&typeprintable.pdf"
    },
    {
        "title": "Review of the state of the art: discovering and associating semantics to tags in folksonomies",
        "implementation_urls": [],
        "doi": "10.1017/s026988891100018x",
        "arxiv": null,
        "abstract": "AbstractThis paper describes and compares the most relevant approaches for associating tags with semantics inorder to make explicit the meaning of those tags. We identify a common set of steps that are usuallyconsidered across all these approaches and frame our descriptions according to them, providing aunified view of how each approach tackles the different problems that appear during the semanticassociation process. Furthermore, we provide some recommendations on (a) how and when to use eachof the approaches according to the characteristics of the data source, and (b) how to improve results byleveraging the strengths of the different approaches.Keywords: folksonomies, ontologies, tagging, semantics, clustering.1 IntroductionIn recent years we have witnessed the transition from a Web where the content is generated mainly bythe owners of websites to a more open and social Web where users are not only information consumersbut also producers (prosumers - Tapscott & Williams, 2006). This new age of the Web, also known asWeb 2.01, has brought a diversity of new social applications like wikis, blogs, social networks, socialbookmarks, and photo, music and video sharing sites. These applications made it possible for all Webusers to contribute and share huge amounts of multimedia content, and to tag these content resources withfree-form keywords.Tags serve multiple purposes, such as content organisation, description, and searching. In 2003,Delicious2 was released as a social bookmarking tool where users are able to assign tags to URLs in acollaborative manner. One year later, Flickr3 was presented as a social network for photo sharing whereusers can assign tags to their own photos or to other photos from their colleagues. Nowadays, tagging ispart of many popular applications such as Amazon, YouTube and Last.Fm, to name a few, where users canassign tags to products, videos and songs respectively.In 2004, Vander Wal4 coined the term Folksonomy to describe the new structure of users, tags andobjects. Folksonomy is defined as the result of personal free tagging of information and objects (anything1http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/what-is-web-20.html2http://delicious.com/3http://www.flickr.com/4http://www.vanderwal.net/folksonomy.html2 A. GARCÍA-SILVA ET AL.with a URL) for one’s own retrieval. The tagging is done in a social environment (usually shared and opento others). Folksonomies are good sources of terminology frequently updated by large communities ofusers. This contrasts with other classification schemes, such as thesauri or taxonomies, which are generallycreated and maintained by controlled user groups. Hence one of the advantages of folksonomies is theirability to rapidly adapt to new changes in terminologies and domains. Furthermore, as time goes by userstend to stabilize the vocabulary used to tag a resource (Golder & Huberman, 2006). This stabilization is theresult of several user iterations and of a tag recommendation strategy based on previously assigned tags.These vocabularies can be seen as shared conceptualizations by groups of users with respect to groups ofresources.The success of tagging is attributed to two main factors; (a) they are very easy to create, where usersdo not need any special skills or experience to tag, and (b) the benefits of tagging are immediate (Hothoet al., 2006). However, current tagging technology suffers from two main problems. First problem isthat folksonomies lack a uniform representation to facilitate their sharing and reuse. Some Web 2.0applications provide APIs to export their folksonomies. However, they do it in proprietary formats. Toovercome this problem, ontologies have been proposed to model the tagging activities in folksonomies,with semantic concepts to represent users, tags, resources, etc. (Echarte et al., 2007; Gruber, 2005; Kimet al., 2008a; Knerr, 2006; Newman, 2005; Passant, 2008; Scerri et al., 2007). One example of theseontologies is the SCOT ontology (Kim et al., 2008a), which is depicted in Figure 1. This ontologymodels tagging information, and includes concepts such as User, Item, Tag and Tag Cloud as well as",
        "file_name": "10!1017%s026988891100018x.pdf",
        "file_path": ".\\PDFs\\10!1017%s026988891100018x.pdf"
    },
    {
        "title": "Integrating WordNet and Wiktionary with lemon",
        "implementation_urls": [],
        "doi": "10.1007/978-3-642-28249-2_3",
        "arxiv": null,
        "abstract": "abstractFormprefRefaltRefhiddenRefWordPhrasePartlemFig. 1 The core lemon modelThe core classes of the lemon model can be seen in Figure 1. The core classes arethe ones that form the main path between the Ontology and the lexical realisationrepresented in the Lexical Entry class. A Lexical Entry may also have multiple Lex-ical Forms representing morphological variants, each of which is associated with aWritten Representation. The Lexical Sense class provides a principled link betweenan ontology concept and its lexical realization. Since ‘concepts’ or world objects,as defined in ontologies, and ‘lexical entries’, as defined in lexicons, can rarely besaid to truly overlap, the Lexical Sense class provides the adequate restrictions (us-age, context, register, etc.) that make a certain lexical entry appropriate for naminga certain concept in the specific context of the ontology being lexicalised.The design principles of this model make it ideal for interchanging lexica onthe Web. Since lemon builds on the RDF data model, URIs are used to name anddereference linguistic annotations, and links can be easily created between lexiconsusing RDF triples. Moreover, the model is modular in the sense that, according tothe final application needs, certain modules can be used or not. This also allows fornew modules to be created if this is required by a certain application. In this sense,the lemon model can be said to be suited for the publication and linking of lexicalresources on the Web.4 John McCrae and Elena Montiel-Ponsoda and Philipp CimianoMultilingualism is also foreseen in lemon, as several lexica in different languagescan be associated to one and the same ontology. Moreover, translation relations canbe established at the Lexical Sense class, even allowing for conceptualization mis-matches between languages to be represented, if needed. In fact, a specific modulefor representing translations has been proposed for lemon (Montiel-Ponsoda et al.(2011)). The main idea of this module is to provide metadata about translations(such as provenance, confidence level, etc.), as well as to capture different types oftranslations (descriptive translations vs. culturally equivalent translations).4 Methods4.1 WordNetThe transformation of WordNet into lemon has been already described before (Mc-Crae et al. (2011)). This conversion was performed automatically based on the man-ual alignment of the WordNet vocabulary to the lemon vocabulary. Hereby, synsetsin WordNet were essentially converted into ontology concepts, words into lemonlexical entries, and senses into lemon lexical senses, respectively. The major changewas the modelling of forms as RDF resources, in contrast to treating them as prop-erties. A disadvantage of using ad-hoc formats when publishing lexical resources asLinked Data is the fact that schema changes might be required when the schema ofthe underlying resources changes. For example, when using an ad-hoc conversion toRDF schema, the conversion of WordNet 3.0 and WordNet 2.0 would yield differentschemas as form variants are specified in WordNet 3.0 in extra files. Having a prin-cipled and uniform format such as lemon would overcome this issue of changing",
        "file_name": "10!1007%978-3-642-28249-2_3.pdf",
        "file_path": ".\\PDFs\\10!1007%978-3-642-28249-2_3.pdf"
    },
    {
        "title": "A Semantically Enhanced UPnP Control Point for Sharing Multimedia Content",
        "implementation_urls": [],
        "doi": "10.1109/mic.2011.83",
        "arxiv": null,
        "abstract": "Abstracts on Human Factors in Computing Systems (CHI 97), ACM Press, pp. 168–169. 11. J.P. Chin, V.A. Diehl, and K.L. Norman, “Development of an Instrument Measuring User Satisfaction of the Human-Computer Interface,” Proc. ACM Conf. Human Factors in Computing Systems (CHI 88), ACM Press, 1988, pp. 213–218.Mariano Rico is a teaching assistant in the computer sci-ence department at the Universidad Autónoma de Madrid and a collaborator in the Ontology Engineering Group at the Universidad Politécnica de Madrid. His research interests include Semantic Web technologies, virtual worlds, user interfaces, and natural language processing. Rico has a PhD in computer science from the Universidad Autónoma de Madrid. Contact him at mariano.rico@uam.es.Oscar Corcho is an associate professor in the artificial intelligence department at the Universidad Politéc-nica de Madrid, where he’s a member of the Ontology Engineering Group. His research activities focus on semantic e-Science and real-world Internet, although he also conducts research in the more general areas of Semantic Web and ontological engineering. Corcho has a PhD in artificial intelligence from the Universi-dad Politécnica de Madrid. Contact him at ocorcho@fi.upm.es.Víctor Méndez is a research fellow at Intelligent Software Components (iSOCO) in Madrid. His research interests include Semantic Web applied to multimedia, opinion mining, reputation, and provenance. Méndez has a BSc in computer engineering from the Carlos III University of Madrid. Contact him at vmendez@isoco.com.Jose Manuel Gomez-Perez is the director of R&D at Intel-ligent Software Components (iSOCO) in Madrid. His research aims to support users in creating, shar-ing, and accessing knowledge and spans knowledge acquisition, provenance analysis, intelligent infor-mation access, and their applications. Gomez-Perez has a PhD in computer science from the Universidad Politécnica de Madrid. Contact him at jmgomez@isoco.com.IC-15-06-Rico.indd   64 10/8/11   5:46 PM",
        "file_name": "10!1109%mic!2011!83.pdf",
        "file_path": ".\\PDFs\\10!1109%mic!2011!83.pdf"
    },
    {
        "title": "The landscape of multimedia ontologies in the last decade",
        "implementation_urls": [],
        "doi": "10.1007/s11042-011-0905-z",
        "arxiv": null,
        "abstract": "Abstract Many efforts have been made in the area of multimedia to bridge the so-called “semantic-gap” with the implementation of ontologies from 2001 to the present. Inthis paper, we provide a comparative study of the most well-known ontologies related tomultimedia aspects. This comparative study has been done based on a frameworkproposed in this paper and called FRAMECOMMON. This framework takes into accountprocess-oriented dimension, such as the methodological one, and outcome-orienteddimensions, like multimedia aspects, understandability, and evaluation criteria. Finally,we derive some conclusions concerning this one decade state-of-art in multimediaontologies.Keywords Ontology . Multimedia . RDF(S) . OWL . Comparative Framework1 IntroductionVision and sound are the most used senses to communicate experiences and knowledge. Theseexperiences or knowledge are normally recorded in media objects, which are generallyassociated to text, image, sound, video and animation. In this regard, a multimedia object can beconsidered as a composite media object (text, image, sound, video, or animation) that iscomposed of a combination of different media objects.Multimed Tools ApplDOI 10.1007/s11042-011-0905-zM. C. Suárez-Figueroa (*) : O. CorchoOntology Engineering Group (OEG). Facultad de Informática,Universidad Politécnica de Madrid (UPM), Madrid, Spaine-mail: mcsuarez@fi.upm.esO. Corchoe-mail: ocorcho@fi.upm.esURL: http://www.oeg-upm.net/G. A. AtemezingEurecom, MM Department, Sophia-Antipolis, Franceemail: ghislain.atemezing@gmail.comNowadays, a growing amount of multimedia data is being produced, processed, andstored digitally. We are continuously consuming multimedia contents in different formatsand from different sources using Google1, Flickr2, Picasa3, YouTube4, and so on. Theavailability of huge amounts of multimedia objects implies the need for efficientinformation retrieval systems that facilitate storage, retrieval, and browsing of not onlytextual, but also image, audio, and video objects. One potential approach can be based onthe semantic annotation of the multimedia content to be semantically described andinterpreted both by human agents (users) and machines agents (computers). Hence, there isa strong need of annotating multimedia contents to enhance the agents’ interpretation andreasoning for an efficient search.The annotation of multimedia objects is difficult because of the so-called semantic gap[24]; that is, the disparity between low level features (e.g., colour, textures, fragments) thatcan be derived automatically from the multimedia objects and high level concepts (mainlyrelated to domain content), which are typically derived based on human experience andbackground. In other words, the semantic gap refers to the lack of coincidence between theinformation that machines can extract from the visual data and the interpretation that thesame data have for a particular person in a given situation. The challenge of unifying bothlow level elements and high level descriptions of multimedia contents in a unique ontologyis one of the ways to contribute to bridge this semantic gap.The need for a high level representation that captures the true semantics of amultimedia object led at the beginning to the development of the MPEG-7 standard[9] for describing multimedia documents. This standard provides metadata descriptors",
        "file_name": "10!1007%s11042-011-0905-z.pdf",
        "file_path": ".\\PDFs\\10!1007%s11042-011-0905-z.pdf"
    },
    {
        "title": "Five challenges for the Semantic Sensor Web",
        "implementation_urls": [],
        "doi": "10.3233/sw-2010-0005",
        "arxiv": null,
        "abstract": "Abstract. The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet. Most efforts in this area focused on the provision of platforms that could be used to build sensor-based applications more efficiently, considering some of the most important challenges in sensor-based data management and sensor network configuration. The introduction of semantics into these platforms provides the opportunity of going a step forward into the understanding, management and use of sensor-based data sources, and this is a topic being ex-plored by ongoing initiatives. In this paper we go through some of the most relevant challenges of the current Sensor Web, and describe some ongoing work and open opportunities for the introduction of semantics in this context. Keywords: Sensor, ontology, query language  1. Introduction The combination of sensor networks with the Web, web services and database technologies, was named some years ago as the Sensor Web or the Sensor Internet [1,6,7,11,15]. Most of the work done on this topic, performed in some cases under the umbrella of the OGC Sensor Web Enablement Working Group1, focused on the creation of specifications for different functionalities related to the management of sensor-based data (observations, measurements, sensor net-work descriptions, transducers, data streaming, etc.), and for the different types of services that may han-dle these data sources (planning, alert, observation and measurement collection and management, etc.).  Some additional work has focused on the provi-sion of platforms that provide the services needed to develop sensor-based applications. These platforms include libraries for common domain-independent data management tasks, such as data cleaning, stor-age, aggregation, query processing, etc., and they are                                                            * Corresponding author. E-mail:  ocorcho@fi.upm.es. 1 http://www.opengeospatial.org/projects/groups/sensorweb used to provide domain-specific aggregated services (e.g., coastal imaging [6], patient care [15]).  Finally, centralized registries for sensor-based data have appeared (e.g., Pachube2, SensorMap3), focused on the registration of sensor-based data sources, and on the provision of access to them in multiple ways, by means of REST-based interfaces, web services, or ad-hoc query languages, to name a few. Figure 1 presents a general architecture of Sensor Web applications; which can be characterised by:   • variability and heterogeneity of data, devices and networks (including unreliable nodes and links, noise, uncertainty, etc.);  • use of rich data sources (sensors, images, GIS, etc.) in different settings (live, streaming, his-torical, and processed);  • existence of multiple administrative domains; and  ",
        "file_name": "10!3233%sw-2010-0005.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-2010-0005.pdf"
    },
    {
        "title": "Developing Ontologies within Decentralised Settings",
        "implementation_urls": [],
        "doi": "10.1007/978-1-4419-5908-9_4",
        "arxiv": null,
        "abstract": "Abstract This chapter addresses two research questions: \"How should a well-engineered methodology facilítate the development of ontologies within com-munities of practice?\" and \"What methodology should be used?\" If ontologies are to be developed by communities then the ontology development life eyele should be better understood within this context. This chapter presents the Melting Point (MP), a proposed new methodology for developing ontologies within decentralised settings. It describes how MP was developed by taking best practices from other methodologies, provides details on recommended steps and recommended pro-cesses, and compares MP with alternatives. The methodology presented here is the product of direct first-hand experience and observation of biological communities of practice in which some of the authors have been involved. The Melting Point is a methodology engineered for decentralised communities of practice for which the designers of technology and the users may be the same group. As such, MP pro­vides a potential foundation for the establishment of standard practices for ontology engineering. 4.1 Introduction The maturity of a particular scientific discipline can be defined by its progress through three main stages. First, innovation followed by the subsequent dissemi-nation of the resulting knowledge or artefact. Second, the formation of communities or collaborations, that utilise or build upon the innovations. Third, the proposal and agreement upon standards for protocols to achieve the unified and consistent pro-gression of innovation and knowledge [1]. The discipline of ontology engineering can be thought of as progressing through the second stage of scientific maturity, moving from ontologies developed by a single authoritative expert to harvesting the collective intelligence of an application domain [2^1]. This trend is also reflected in the availability of software supporting the engagement of several domain experts,communities, representing knowledge and developing ontologies [2, 5]. Therefore,ontology engineering is on the cusp of the third stage of scientific maturity, requiringthe development of common working practices or standard methodologies.Knowledge engineering (KE) is a field that involves integrating knowledgewithin computer systems [6] or the building, maintaining and development ofknowledge-based systems [7]. Therefore, some of the methods proposed within thefield of KE are applicable when building ontologies [8]. However, the experiencesfrom KE have not always been applied when developing ontologies. In general KEmethodologies focus primarily on the use of the ontology by a software system asopposed to the development of the ontology [9].Within the domain of ontology engineering several methodologies have been pro-posed and applied [10–17]. The majority of the proposed methodologies have beenengineered for centralised settings. However, none of these have gained widespreadacceptance, predominant use or have been proven to be applicable for multipleapplication domains or development environments [18]. To date the communityhas not been widely involved or considered within ontology engineering method-ologies. This situation has encouraged debate amongst those within the ontologycommunity as to which methodology or combination of methodologies are mostapplicable [18, 9].The language choice for encoding an ontology is still an open debate acrossthe ontology building communities. This situation can be illustrated by the useof both the OBO format and the OWL within the bio-ontology community [19].Conforming to or accepting a single formalism for ontology encoding would bringconsistency and standardisation to the engineering methodology, such as tool sup-",
        "file_name": "10!1007%978-1-4419-5908-9_4.pdf",
        "file_path": ".\\PDFs\\10!1007%978-1-4419-5908-9_4.pdf"
    },
    {
        "title": "A framework and computer system for knowledge-level acquisition, representation, and reasoning with process knowledge",
        "implementation_urls": [],
        "doi": "10.1016/j.ijhcs.2010.05.004",
        "arxiv": null,
        "abstract": "ABSTRACT The development of knowledge-based systems is usually approached through the combined skills of software and knowledge engineers (SEs and KEs, respectively) and of subject matter experts (SMEs). One of the most critical steps in this task aims at transferring knowledge from SMEs’ expertise to formal, machine-readable representations, which allow systems to reason with such knowledge. However, this process is costly and error prone. Alleviating such knowledge acquisition bottleneck requires enabling SMEs with the means to produce the target knowledge representations, minimizing the intervention of KEs. This is especially difficult in the case of complex knowledge types like processes. The analysis of scientific domains like Biology, Chemistry, and Physics uncovers: i) that process knowledge is the single most frequent type of knowledge occurring in such domains and ii) specific solutions need to be devised in order to allow SMEs to represent it in a computational form. We present a framework and computer system for the acquisition and representation of process knowledge in scientific domains by SMEs. We propose methods and techniques to enable SMEs to acquire process knowledge from the domains, to formally represent it, and to reason about it. We have developed an abstract process metamodel and a library of Problem Solving Methods (PSMs), which support these tasks, respectively providing the terminology for SME-tailored process diagrams and an abstract formalization of the strategies needed for reasoning about processes. We have implemented this approach as part of the DarkMatter system and formally evaluated it in the context of the intermediate evaluation of Project Halo, an initiative aiming at the creation of question answering systems by SMEs.   1. INTRODUCTION Building knowledge-based systems is an activity that has been traditionally carried out by a combination of software and knowledge engineers and of subject matter experts (SMEs), also known as domain experts. Software engineers (SEs) are focused on architectural and user interface issues related to the development of software. Knowledge engineers (KEs) are focused on knowledge acquisition and representation tasks, with the aim of building the required knowledge bases. For these tasks, they normally work in collaboration with SMEs, who act as repositories of domain knowledge to a large extent. The combination of KEs and SMEs is feasible for a number of domains. However, it has two main drawbacks, first characterized as the knowledge acquisition bottleneck by Feigenbaum in 1977: i) it is costly and ii) it can be error prone, especially in complex domains.   A large amount of work in knowledge-based systems in the past three decades has concentrated on providing frameworks and tools that support the collaboration of KEs and SMEs with the goal of alleviating the knowledge acquisition bottleneck. Despite such work, existing knowledge acquisition tools are still not effective and intuitive enough to allow SMEs to capture the knowledge from a domain by themselves.   Among the different types of knowledge that can be used in knowledge-based systems, in our work we focus on the particular case of process knowledge. Process knowledge is one of the most widely used but also complex types of knowledge across domains, posing important challenges for knowledge acquisition. A process can be considered as a special concept which encapsulates such things as preconditions, results, contents, ",
        "file_name": "10!1016%j!ijhcs!2010!05!004.pdf",
        "file_path": ".\\PDFs\\10!1016%j!ijhcs!2010!05!004.pdf"
    },
    {
        "title": "A Tool Suite to Enable Web Designers, Web Application Developers and End-users to Handle Semantic Data1",
        "implementation_urls": [],
        "doi": "10.4018/jswis.2010070103",
        "arxiv": null,
        "abstract": "’97 extended abstracts on human factors in computing systems(pp. 168–169). New York, NY, USA: ACM. (Los Angeles, USA.April 18-23)Pietriga, E. (2001). Isaviz: A visual authoring tool for rdf.see http://www.w3.org/2001/11/isaviz/. Available from http://www.w3.org/2001/11/IsaViz/Quan, D., & Karger, D. (2004). How to Make a Semantic WebBrowser. In International world wide web conference. proceed-ings of the 13th international conference on world wide web.session: Semantic interfaces and owl tools. (ISBN:1-58113-844-X)Rico, M., Camacho, D., & Corcho Óscar. (2008). VPOET: Usinga Distributed Collaborative Platform for Semantic Web Appli-cations. In C. Badica, G. Mangioni, V. Carchiolo, & D. Bur-descu (Eds.), Intelligent distributed computing, systems and ap-plications. proc. 2nd international symposium on intelligent dis-tributed computing (idc’2008) (pp. 167–176). Springer. (ISBN:978-3-540-85256-8)Rico, M., Camacho, D., & Corcho Óscar. (2009a). Macros vs.scripting in VPOET. In 5th Workshop on Scripting and De-velopment for the Semantic Web (SFSW2009) at the 6th AnnualEuropean Semantic Web Conference (ESWC). CEUR online pro-ceedings, Volume 449.Rico, M., Camacho, D., & Corcho Óscar. (2009b). VPOET Tem-plates to Handle the Presentation of Semantic Data Sources inWikis. In Fourth Workshop on Semantic Wikis: The SemanticWiki Web (SemWiki2009) at the 6th Annual European Seman-tic Web Conference (ESWC). CEUR online proceedings, Volume464 (pp. 186–190).Rico, M., Camacho, D., & Corcho Óscar. (2010). A Contribution-based Framework for the Creation of Semantically-enabled WebApplications. Journal of Information Sciences, 180(10), 1850–1864.Rochen, R., Rosson, M., & Pérez, M. (2006). End user Devel-opment of Web Applications. In H. Lieberman, F. Paternò, &V. Wulf (Eds.), (p. 161-182). Springer.Souzis, A. (2006). Bringing the wiki-way to the seman-tic web with rhizome. In M. Völkel & S. Schaffert (Eds.),Semwiki2006, proceedings of the first workshop on semanticwikis. CEUR-WS.org. Available from http://www.ceur-ws.org/Vol-206/paper19.pdf (Budva, Montenegro, June 12,2006)",
        "file_name": "10!4018%jswis!2010070103.pdf",
        "file_path": ".\\PDFs\\10!4018%jswis!2010070103.pdf"
    },
    {
        "title": "A contribution-based framework for the creation of semantically-enabled web applications",
        "implementation_urls": [],
        "doi": "10.1016/j.ins.2009.07.004",
        "arxiv": null,
        "abstract": "example, the F-plugin contains one instance of the class Vpoet. Abstract classes and interfaces are written in italics. Fig. 5 shows the layers involved in the development of F-plugins. The upper layer is the API provided by JSPWiki, which provides the abstract class WikiPiugin. Below this layer, the Fortunata API provides a set of generic classes that can be exploited by specific application classes. This is the case of the semantic web application VPOET, which uses the class Vpoet, shown in the layer named \"Semantic Web application\". The last layer, named in the figure \"F-plugin\", is for specific appli­cation plugins. The class Adcivisuai izat ionis an example of the kind of classes that exist in this layer, and howit is related to the other layers. A semantically-enabled web application is represented by a class derived from the abstract class FortunataSWAppii-cat ion, which provides developers with useful methods as well as forcé developers to implement the methods c r éa t e i n ­d iv idua l () and f i i iDataModei( ) concerning semantics persistence. All the plugins in a Fortunata-based application share a semantic web application. In this example, the figure shows the class AddVisual izat ion. This class is an F-plugin, and consequently it inherits the methods implemented in the base class For tuna taP lug in and it is forced to implement three methods (one from the interface WikiPiugin and two from the class For tunataPlugin) . This plugin contains an instance of the class Vpoet, which implements two methods from the class FortunataSWApplication concerning seman­tic data management. The process to créate and contribute an F-plugin is detailed in the upper part of Fig. 6, and follows the usual procedure in any plugin-based architecture. First, the developer must créate the F-plugin locally (steps 1-3) and perform an ade-quate number of tests to check that it is working correctly (step 4). Then she must proceed to the publication (step 5) of the plugin source code and of the documentation about its usage. The bottom part of the figure depicts the process to créate new functionality by reusing the initial functionality following a \"contributively collaboration\" schema. It com-prises the following steps: installation of an existing F-plugin (step 6), reading and understanding of its associated ontol-ogies (either by manually reading the OWL files, using any off-the-shelf ontology editor, or by means of OMEMO) in order to find the elements that must be added, removed or modified, or in order to decide whether a new set of ontologies has to be imported and used (step 7), local creation of the extended plugin (steps 8-10), local tests (step 11) and publication (step 12). The purpose of this detailed explanation is to show the low complexity of the plugin reuse and contribution process. Table 1 summarizes the development tasks that are normally associated to the development of a typical semantic web application, and compares the skills that are required to perform these tasks when using a traditional development approach and a Fortunata-based approach. Traditional development requires more competencies (more development tools and roles) than Fortunata-based development. This is one of the main results of the comparison performed with real developers, which is described in Section 4. 3. OMEMO and VPOET: examples of Fortunata-based semantic web applications This section illustrates how the Fortunata framework can be used to créate two prototypical semantically-enabled web applications. These applications are not intended to be original or innovative, since similar types of applications are available in the current state of the art, but we aim at demonstrating that they are easy to implement and extend using our approach. OMEMO is focused on the HTML publication of ontologies (in a similar fashion to systems like OWLDoc (See http://www.co-ode.org/downloads/owldoc/)), and it is interesting as a case study since it exploits many features of the wiki infrastructure, such as orphan links or the simplicity of the wiki syntax. VPOET is focused on semantic data visualization, and especially http://www.coode.org/downloads/owldoc/http://www.coode.org/downloads/owldoc/http://ode.org/downloads/owldoc/)),r r OÍ Q. 3 ai o O CD ' ü i ",
        "file_name": "10!1016%j!ins!2009!07!004.pdf",
        "file_path": ".\\PDFs\\10!1016%j!ins!2009!07!004.pdf"
    },
    {
        "title": "Semantic Web-Enabled Protocol Mediation for the Logistics Domain",
        "implementation_urls": [],
        "doi": "10.4018/978-1-60566-066-0.ch004",
        "arxiv": null,
        "abstract": "Abstract Among the problems that arise when trying to make different applications interoperate with each other, protocol mediation is one of the most difficult ones and for which less relevant literature can be found. Protocol mediation is concerned with non-matching message interaction patterns in application interaction. In this paper we describe the design and implementation of a protocol mediation component that has been applied in the interoperation between two heterogeneous logistic provider systems (using two different standards: RosettaNet and EDIFACT), for a specific freight forwarding task. 1 Current Situation Logistics management is a typical business problem where the use of a Service Oriented Architecture is clearly suited. As pointed out in (Evans-Greenwood and Stason, 2006) the current trend in logistics is to divide support between planning applications, which compute production plans overnight, and execution applications, which manage the flow of events in an operational environment. This disconnection forces users to deal with business exceptions (lost shipments, for example), manually resolving the problems by directly updating the execution and planning applications. However, this human-dependency problem can be ameliorated by using Web technology to create a heterogeneous composite application involving all participants in the process, providing a complete Third-Party Logistics solution, and giving users a single unified view into the logistics pipeline. This consolidated logistics solution greatly simplifies the task of identifying and correcting business exceptions (e.g., missing shipments or stock shortages) as they occur. Furthermore, (Evans-Greenwood and Stason, 2006) also talk about the possibility of combining multiple Third-Party Logistics solutions into a single heterogeneous virtual logistics network. With such a virtual network, each shipment is assigned a route dynamically assembled from one or more individual logistics providers, using dynamically created virtual supply chains. Most of these business functions are still manual and offline, but most of them can be automated with the use of Service Oriented Architectures, as will be presented in this chapter. Obviously, the main advantages of using such solutions are the decreases in cost and speed in transactions, which influence in a better quality of the service provided to customers. The main barrier to set up a business relationship with a company in the logistics domain is that it usually requires an initial large investment of time and money. This is ameliorated by the emergence of some industry standards like EDIFACT (EDIFACT), AnsiX12 (AnsiX12) or RosettaNet (RosettaNet), which ease the integration tasks between information systems that comply with them. However, given that these standards have some flexibility in what respects the content and sequencing of the messages that can be exchanged, the integration of systems is still time and effort consuming. Besides, there is sometimes a need to integrate systems that use different standards, what makes the integration task even more time and effort consuming. This is the focus of one of the four case studies developed in the context of the EU project SWWS1 (Semantic-Web enabled Web Services), a demonstrator of business-to-business integration in the logistics domain using Semantic Web Service technology. All the features of this                                                            demonstrator are described in detail in (Preist et al., 2005), including aspects related to the discovery and selection of relevant services, their execution and the mediation between services following different protocols.  In this chapter we will focus on the last aspect (mediation) and more specifically on protocol mediation, which is concerned with the problem of non-matching message interaction patterns. We will describe the design and implementation of the protocol mediation component applied in this case study to show how to make logistic provider systems using two different standards (RosettaNet and EDIFACT) interoperate for a specific freight forwarding task.  The chapter is structured as follows. The rest of this section introduces a motivating example, focusing on the needs for protocol mediation, and gives some background on how the problem of mediation can be characterised in general and on the approaches for mediation proposed in the ",
        "file_name": "10!4018%978-1-60566-066-0!ch004.pdf",
        "file_path": ".\\PDFs\\10!4018%978-1-60566-066-0!ch004.pdf"
    },
    {
        "title": "Semantic Web Technologies and E-Business",
        "implementation_urls": [],
        "doi": "10.4018/978-1-59904-192-6",
        "arxiv": null,
        "abstract": "Service/Protocol/Abstract Process Mediation The parties involved in a conversation may have not only the needs for data transformation described in the previous section, but also different message interaction models. It is at this stage where service mediation (aka protocol mediation or abstract process mediation) is needed.  Let us see first how the message interaction patterns to be used in a conversation are expressed in the context of service-oriented architectures. In the Web services world, two different types of interactions are distinguished: orchestration and choreography. Web service orchestration describes how Web services can interact with each other at the message level, including the business logic and execution order of the interactions. Web service choreography tracks the sequence of messages that may involve multiple parties and multiple sources (requesters, providers, etc.). With the last enhancements and standards this distinction has almost disappeared and in many cases both aspects are generally called as Web service orchestration. Early work in Web services orchestration included eCo (from CommerceNet, focused on the integration of e-commerce services), WSCL (Web Services Conversation Language, from HP), XLANG (from Microsoft, included in the Microsoft Biztalk Server) and WSFL (Web Services Flow Language, from IBM).  Some of these early proposals were superseded by other languages, and currently the two most important proposals are the following: - BPEL4WS [BPEL4WS] (Business Process Execution Language for Web Services). It superseded XLANG and WSFL and now is being further developed with a different name, WS-BPEL [WS-BPEL], under the standardisation process in OASIS.  - WS-CDL (Web Services Choreography Description Language). It evolves from WSCI [WSCI] (Web Service Choreography Interface) and BPML [BPML] (Business Process Management Language), which in their turn evolved from WSCL. This language is now following the standardisation process in the W3C. All these languages and proposals allow establishing the order in which a set of Web services have to exchange their messages in order to create a more complex business process. In other words, they allow specifying the control flow of composite business processes by means of allowing an effective communication between the Web services involved in the process, including sequential and parallel activities, loops, etc. They provide execution engines that are able to read such specifications and drive the communication, and they also take into account aspects such as the transactionality of operations, callback mechanisms, etc. By allowing the expression of the control flow that a set of Web services must abide to in order to form a complex service, these languages aim at reducing the complexity required to orchestrate Web services, thereby reducing time-to-market and costs, and increasing the overall efficiency for building a complex service. However, the interaction patterns of several Web services involved in a communication do not always have a precise match, that is, do not always follow exactly the same pattern in realising the complex process. Sometimes there can be mismatches like the ones identified in [Cimpian and Mocan, 2005], and summarised in Figure 4: - Unexpected messages. One of the parties receives does not expect to receive a message issued by another. For instance, in a commercial transaction with a credit card a service sends the credit card type, the credit card number, the expiration date, the full name and the pin code, while the service that receives those messages does not expect to receive a pin code, since it does not use it. - Messages in different order. The parties involved in a communication send and receive messages in different orders. In the previous case the sender may send the messages in the order specified above while the receiver expects first the full name and then the rest of the messages. - Messages that need to be split. One of the parties sends a message with multiple ",
        "file_name": "10!4018%978-1-59904-192-6.pdf",
        "file_path": ".\\PDFs\\10!4018%978-1-59904-192-6.pdf"
    },
    {
        "title": "Architectural Patterns for the Semantic Grid",
        "implementation_urls": [],
        "doi": "10.1007/978-0-387-37831-2_8",
        "arxiv": null,
        "abstract": "Abstract The Semantic Grid reference architecture, S-OGSA, includes semantic provi-sioning services that are able to produce semantic annotations of Grid resources,and semantically aware Grid services that are able to exploit those annotations invarious ways. In this paper we describe the dynamic aspects of S-OGSA by pre-senting the typical patterns of interaction among these services. A use case fora Grid meta-scheduling service is used to illustrate how the patterns are appliedin practice.Keywords: Semantic Grid, Grid services, architectural patterns.∗1. IntroductionThe Grid aims to support secure, flexible and coordinated resource sharingby providing a middleware platform for advanced distributed computing[6] . Grid middleware architectures aim to allow collections of any kind ofresourcescomputing, storage, data sets, digital libraries, scientific instruments,people, etc to easily form Virtual Organizations (VOs) that cross organiza-tional boundaries in order to work together to solve a problem. However,existing Grid middleware architectures and the standards on which they arebased on, fall short of addressing some of the original vision of configurable,self-healing, adaptive, and interoperable middleware [6]. This is due mainlyto the following reasons:Knowledge burial. Knowledge and metadata regarding Grid entities iscurrently generated and used in an ad hoc fashion, much of it buried in themiddleware’s code libraries and database schemas. This esoteric expressionand use of knowledge hinders interoperability when it comes to building open,interoperable and adaptive systems. Existing Grid middleware is thereforeconsiderably affected by syntactic changes in protocols and representations,and it becomes highly dependent on human intervention during its operation.Dominance of XML-based vocabularies and protocols. The Grid com-munity has developed a number of specifications and standards that aim toincrease interoperability among middleware components. XML has becomethe de-facto language not only for expressing these specifications, but alsofor describing Grid entities and their behaviour. However, XML-basedspecifications do not provide a complete solution to the problem of knowledgeburial due to the lack of a shared formal interpretation of XML documents.Lack of models for Grid processes. Many aspects of the Grid are still notformally defined, therefore it becomes difficult to identify the challengesand even more difficult to find solutions. Take as an example the formationof Virtual Organizations (VOs); creating a model for forming VOs canhelp setting-up a community-wide terminology, highlight differences amongexisting systems and bring about previously unforeseen issues to be solvedfor interoperability. This model should be the product of a knowledgeacquisition process, similar to those being undertaken by the Web [4], WebServices [3]and Semantic Web Services communities [9, 14]. The outcomeof the modeling process can be used for the development of interoperablemetadata based on explicit semantics.The Semantic Grid is an extension of the current Grid in which informationand services are given well defined and explicitly represented meaning, betterenabling computers and people to work in cooperation [8]. In the SemanticGrid, the goal of sharing virtualized computational and data resources is ex-tended to include explicit metadata and knowledge. During the last few years,",
        "file_name": "10!1007%978-0-387-37831-2_8.pdf",
        "file_path": ".\\PDFs\\10!1007%978-0-387-37831-2_8.pdf"
    },
    {
        "title": "Semantic Web-Based Information Systems",
        "implementation_urls": [],
        "doi": "10.4018/978-1-59904-426-2",
        "arxiv": null,
        "abstract": "ABSTRACT In this paper we present a model for building ontology translation systems between ontology languages and/or ontology tools, where translation decisions are defmed at four different layers: lexical, syntax, semantic, and pragmatic. This layered approach provides a major contribution to the current state of the art in ontology translation, since it makes ontology translation systems easier to build and understand and, consequently to maintain and reuse. As part of this model, we propose a method that guides in the process of developing ontology translation systems according to this approach. The method identifies four main activities: feasibility study, analysis ofsource, and target formats, design, and implementation of the translation system, with their decomposition in tasks, and recommends the techniques to be used inside each of them. Keywords: ontologies; semiotics; transformation languages; transformation models INTRODUCTION An ontology is defined as a \"formal explicit specification of a shared conceptualization\" (Studer etal., 1998); that is, an ontology must be machine read-able (it is formal), all its components must be described clearly (it is explicit), it de­scribes an abstract model of a domain (it is a conceptualization), and it is the prod-uct of a consensus (it is shared). Ontologies can be implemented in varied ontology languages, which are usu-ally divided in two groups: classical and ontology markup languages. Among the classical languages used for ontology con-struction, we can cite (in alphabetical or-der): CycL (Lenat& Guha, 1990), FLogic (Kifer et al., 1995), KIF (Genesereth & Fikes, 1992), LOOM (MacGregor, 1991), OCML (Motta, 1999), and Ontolingua (Gruber, 1992). Among the ontology markup languages used in the context of the Semantic Web, we can cite (in alphabetical order): DAML+OIL (Horrocks and vanHarmelen, 2001), OIL (Horrocks et al., 2000), OWL (Dean & Schreiber, 2004), RDF (Lassila& Swick, 1999), RDF Schema (Brickley & Guha, 2004), SHOE (Luke & Hefflin, 2000), and XOL (Karp et al., 1999). Each of these languages has its own syntax, its own expressiveness, and its own reasoning capabilities provided by different inference engines. Languages also are based on dif­ferent knowledge representation para-digms and combinations of them (frames, first order logic, description logic, seman­",
        "file_name": "10!4018%978-1-59904-426-2.pdf",
        "file_path": ".\\PDFs\\10!4018%978-1-59904-426-2.pdf"
    },
    {
        "title": "Legal Ontologies for the Spanish e-Government",
        "implementation_urls": [],
        "doi": "10.1007/11881216_32",
        "arxiv": null,
        "abstract": "Abstract. The Electronic Government is a new field of applications for the se-mantic web where ontologies are becoming an important research technology. The e-Government faces considerable challenges to achieve interoperability given the semantic differences of interpretation, complexity and width of scope. In this paper we show the results obtained in an ongoing project commissioned by the Spanish government that seeks strategies for e-Government to reduce the problems encountered when delivering services to citizens. Here we present an e-Government ontology model; within this model a set of legal ontologies are devoted to represent the Real-estate transaction domain used to illustrate this paper; some examples of use of these legal ontologies are given. 1   Introduction and Motivation Electronic Government (e-Gov) is an important application field [2] for the transfor-mations that governments and public administrations will have to undergo in the next decades. Therefore to transform the e-Gov into the e-Governance, the e-Gov research needs to be based on a robust theory, on modelling approaches, and on planning. In this scenario, it is crucial to manage the legal knowledge for improving the systems applications in different ways.  For over more than two decades the AI and Law community has been very active and productive. In the early 80´s, research was focused on logic programming, and all the efforts were centered on legislation and on legal reasoning. Another approach adopted was the case-based reasoning, which was not as formal as logic was, aimed at finding similarities in legal cases and allowed retrieving relevant cases for the judges. Knowledge engineering was also of interest for the research community and the field most applied; this area allowed developing and using the legal ontologies that underlie the growing of the Semantic Web. The Semantic Web was proposed by Tim Berners-Lee [7] as a new field of re-search, and according to World Wide Web Consortium1 (W3C) the Semantic Web is defined as “an extension of the current Web in which information is given well-defined meaning, better enabling computers and people to work in cooperation. It is based on the idea of having data on the Web defined and linked such that it can be                                                            1 http://www.w3.org/2001/sw used for more effective discovery, automation, integration, and reuse across various applications”. The Semantic Web at e-Gov is new; it features knowledge representation, knowl-edge engineering, database design, information systems, database integration, natural language understanding, information retrieval and semantic portals, among others. The Semantic Web is considered to be the infrastructure upon which all intelligent e-Gov applications will be built in the near future. Within the objectives of the Semantic Web the ontologies play an important role. “Ontology” is a word taken from Philosophy where it is used as a systematic explana-tion of \"existence\". In the field of the Artificial Intelligence, Neches [11] defined an ontology for the first time in the following way: \"Ontology defines the basic terms and the relations that include the vocabulary of a specific area, in addition to the rules to combine terms and relations to define extensions to the vocabulary\". It is possible to say that this definition serves us as a kind of guide to construct ontologies. Accord-ing to Neche´s definition, an ontology does not include only the terms that explicitly are defined in it, but also those that can be inferred using rules. Gruber defines the ontology as: \"An explicit specification of a conceptualization\" [5, 6]. The e-Gov has been strengthened with all these previous studies carried out by the ",
        "file_name": "10!1007%11881216_32.pdf",
        "file_path": ".\\PDFs\\10!1007%11881216_32.pdf"
    },
    {
        "title": "Ontology based document annotation: trends and open research problems",
        "implementation_urls": [],
        "doi": "10.1504/ijmso.2006.008769",
        "arxiv": null,
        "abstract": "Abstract: Metadata is used to describe documents and applications, improving information seeking and retrieval and its understanding and use. Metadata can be expressed in a wide variety of vocabularies and languages, and can be created and maintained with a variety of tools. Ontology based annotation refers to the process of creating metadata using ontologies as their vocabularies. We present similarities and differences with respect to other approaches for metadata creation, and describe languages and tools that can be used to implement these annotations. Keywords: ontology; metadata; annotation. Biographical notes: Osear Corcho is working as a Marie Curie Fellow at the Information Management Group, University of Manchester. Previously, he has worked at iSOCO as a Research Manager and at the Ontological Engineering Group of Universidad Politécnica de Madrid (UPM). He graduated in Computer Science from UPM in 2000, and received the third Spanish award in computer science from the Spanish Government. He obtained his MSc in Software Engineering from UPM in 2001, and his PhD in Artificial Intelligence in 2004. His research activities include ontology languages and tools, the Semantic Web and the Semantic Grid. 1 Introduction Metadata is usually defined as 'data about data', which aims at expressing the 'semantics' of information, henee improving information seeking, retrieval, understanding and use. Metadata can be attached to a wide range of documents. These documents may be available electronically in the form of HTML, PDF, Látex, etc., in the Web or in our hard disks or on paper in a library, among others. Not only can metadata be applied to documents, but also to applications running in our computers or available in the web in the form of web services. Metadata can be expressed in a wide range of languages (from natural to formal ones) and with a wide range of vocabularies (from simple ones, based on a set of agreed keywords, to complex ones, with agreed taxonomies and formal axioms). It may be available in different formats: electronically or even physically (written down in the margins of a textbook). And it can be created and maintained, using different types of tools (from text editors to metadata generation tools), either manually or automatically. In this paper we will only deal with the management of metadata attached to electronic documents, expressed with formal languages and using ontologies as vocabularies. We will neither deal with the management of metadata for applications, ñor with the creation of metadata based on other types of vocabularies. We will describe the advantages and disadvantages of using ontologies as the vocabularies on which the metadata is based (Section 2); we will describe some of the formal languages that can be used to express metadata (Section 3); and we will describe the tools currently available for ontology based document annotation (Section 4). ",
        "file_name": "10!1504%ijmso!2006!008769.pdf",
        "file_path": ".\\PDFs\\10!1504%ijmso!2006!008769.pdf"
    },
    {
        "title": "Semantic Web-Based Information Systems",
        "implementation_urls": [],
        "doi": "10.4018/978-1-59904-426-2",
        "arxiv": null,
        "abstract": "ABSTRACT In this paper we present a model for building ontology translation systems between ontology languages and/or ontology tools, where translation decisions are defmed at four different layers: lexical, syntax, semantic, and pragmatic. This layered approach provides a major contribution to the current state of the art in ontology translation, since it makes ontology translation systems easier to build and understand and, consequently to maintain and reuse. As part of this model, we propose a method that guides in the process of developing ontology translation systems according to this approach. The method identifies four main activities: feasibility study, analysis ofsource, and target formats, design, and implementation of the translation system, with their decomposition in tasks, and recommends the techniques to be used inside each of them. Keywords: ontologies; semiotics; transformation languages; transformation models INTRODUCTION An ontology is defined as a \"formal explicit specification of a shared conceptualization\" (Studer etal., 1998); that is, an ontology must be machine read-able (it is formal), all its components must be described clearly (it is explicit), it de­scribes an abstract model of a domain (it is a conceptualization), and it is the prod-uct of a consensus (it is shared). Ontologies can be implemented in varied ontology languages, which are usu-ally divided in two groups: classical and ontology markup languages. Among the classical languages used for ontology con-struction, we can cite (in alphabetical or-der): CycL (Lenat& Guha, 1990), FLogic (Kifer et al., 1995), KIF (Genesereth & Fikes, 1992), LOOM (MacGregor, 1991), OCML (Motta, 1999), and Ontolingua (Gruber, 1992). Among the ontology markup languages used in the context of the Semantic Web, we can cite (in alphabetical order): DAML+OIL (Horrocks and vanHarmelen, 2001), OIL (Horrocks et al., 2000), OWL (Dean & Schreiber, 2004), RDF (Lassila& Swick, 1999), RDF Schema (Brickley & Guha, 2004), SHOE (Luke & Hefflin, 2000), and XOL (Karp et al., 1999). Each of these languages has its own syntax, its own expressiveness, and its own reasoning capabilities provided by different inference engines. Languages also are based on dif­ferent knowledge representation para-digms and combinations of them (frames, first order logic, description logic, seman­",
        "file_name": "10!4018%978-1-59904-426-2.pdf",
        "file_path": ".\\PDFs\\10!4018%978-1-59904-426-2.pdf"
    },
    {
        "title": "Semantic Web and Databases",
        "implementation_urls": [],
        "doi": "10.1007/b106149",
        "arxiv": null,
        "abstract": "Abstract. We present R2O, an extensible and declarative language to describe mappings between relational DB schemas and ontologies implemented in RDF(S) or OWL. R2O provides an extensible set of primitives with well-defined semantics. This language has been conceived expressive enough to cope with complex mapping cases arisen from situations of low similarity between the ontology and the DB models.  1   Introduction and Motivations There is a large quantity of data on Web pages generated from relational DBs. This information is often referred to as the Deep Web [2] as opposed to the surface Web comprising all static Web pages. In this paper we face the problem of “upgrading” this large amount of existing content into Semantic Web content. Let us set the following scenario: we have a legacy DB and we want to generate Semantic Web content from it. Until now, three approaches have been reported. The first one, described in [11,12] is based in the semi-automatic generation of an ontology from our DB’s relational model. Then mappings are defined between the DB and the generated ontology. Because the level of similarity between both is very high, mappings will be quite direct and complex mapping situations do not usually appear. This approach does not allow the population of an existing ontology, which is a big limitation. A second approach [6], proposes the manual annotation of dynamic Web pages which publish DB content, with information about the underlying DB and how each content item in a page is extracted from the DB. This approach does not deal neither with complex mapping situations and assumes we want to make our database schema public, which is not always the case. The third approach is the one proposed in this paper. It tries to map an existing DB to an appropriate existing ontology implemented in RDF(S) or OWL. The term appropriate may mean, depending on the situation: The one whose domain has a higher coverage of the domain modeled in the DB, the one whose domain best covers the specific part of the DB to be migrated to the Semantic Web or the one that maximizes the extraction of information according to a particular metric, among others. The literature references very few languages for expressing mappings between ontologies and DBs. Recent approaches like D2R MAP [4] and extended D2R [1] have tackled this problem but they lack of expressiveness for writing complex mapping transformations and are not fully declarative. The language presented in this paper is intended to extend and enhance the mapping description capabilities of these two ones. Complementary approaches to this work can be also found in the Intelligent Information Integration area, in which data from existing heterogeneous DBs are extracted according to ontologies and then combined. The main differences with respect to our approach is that in these systems the mapping between the ontologies and the DBs from which the ontology instances are extracted are not created declaratively but with ad-hoc software implementations. Examples of such systems are Observer [8] and Picsel [5], among others.  The most important aspect of our approach is that we will use the DB and the ontology “as they are” and we will just define a declarative specification of the mappings between their modeling components. That is the reason why the R2O (Relational to Ontology) language which is the base of our approach, has been conceived expressive enough to cope with complex mapping situations arisen from low similarity between the ontology and the DB model (one of them is richer, more generic or specific, better structured, etc., than the other).  This paper is organized as follows: Section 2 describes the main features of the ",
        "file_name": "10!1007%b106149.pdf",
        "file_path": ".\\PDFs\\10!1007%b106149.pdf"
    },
    {
        "title": "Reference Ontology and (ONTO)2 Agent: The Ontology Yellow Pages",
        "implementation_urls": [],
        "doi": "10.1007/pl00011649",
        "arxiv": null,
        "abstract": "Abstract. Knowledge reuse by means of ontologies faces three important problems at present: (1) there are no standardized identifying features that characterize ontologies from the user point of view; (2) there are no web sites using the same logical organization, presenting relevant information about ontologies; and (3) the search for appropriate ontologies is hard, time-consuming and usually fruitless. To solve the above problems, we present: (1) a living set of features that allow us to characterize ontologies from the user point of view and have the same logical organization; (2) a living domain ontology about ontologies (called Reference Ontology) that gathers, describes and has links to existing ontologies; and (3) (ONTO)2Agent, the ontology-based WWW broker about ontologies that uses Reference Ontology as a source of its knowledge and retrieves descriptions of ontologies that satisfy a given set of constraints. 1. Introduction and Motivation During recent years, considerable progress has been made in developing the con­ceptual bases for building technology that allows knowledge component reuse and sharing. One of the main motivations underlying both ontologies and problem-solving methods (PSM) is to enable sharing and reuse of knowledge and reasoning behavior across domains and tasks. PSMs and ontologies can be seen as com­plementary reusable components to construct knowledge systems (Gómez-Pérez and Benjamins, 1998). Ontologies are concerned with static domain knowledge and PSMs with dynamic reasoning knowledge. The integration of ontologies and PSMs is a possible solution to the 'interaction problem' (Bylander and Chan-drasekaran, 1988), which states that representing knowledge for the purpose of solving some problem is strongly affected by the nature of the problem and the inference strategy to be applied to the problem. Ontologies are defined as a formal, explicit specification of a shared conceptu­alization (G. Gruber, 1993; Borst, 1997); that is, 'Conceptualization refers to an abstract model of some phenomenon in the world by having identified the relevant concepts of that phenomenon. Explicit means that the type of concepts used, and the constraints on their use are explicitly defined. Formal refers to the fact that the ontology should be machine-readable. Shared reflects the notion that an ontology captures consensual knowledge, that is, it is not private to some individual, but accepted by a group' (Studer et al, 1998). PSMs describe the reasoning process of a knowledge-based system in an implementation- and domain-independent man­ner (Benjamins and Fensel, 1998). There are also the notions of task ontologies (Mizoguchi et al, 1995) and PSM ontologies (Chandrasekaran et al, 1998). Nowadays, it is easy to get information from organizations that have ontolo­gies and PSMs on the web. There are even accessible points that gather informa­tion about ontologies and have links to other web pages containing more explicit information about such ontologies (see The Ontology Page, also known as TOP; http ://www.medg.lcs.mit.edu/doyle/top) and there are also ontology servers, like The Ontology Server (http://www-ksl.standford.edu:5915) (Farquhar et al, 1995, 1996), Cycorp's Upper CYC Ontology Server (http://www.cyc.com) (Lenat, 1990) or Ontosaurus (http://indra.isi.edu:8000/Loom) (Swartout et al, 1997), that col­lect a huge number of very well-known ontologies. In the PSM area, there are also many PSM repositories at different locations but they are not accessible for outsiders and they are not compatible (Benjamins et al, 1998). At present, the knowledge component reuse and sharing community has identified the need to provide intelligent agents or intelligent brokering services on the WWW that ease the search for such knowledge components. In the ontology field, the need for this kind of services was identified in Fikes and Farquhar (1997) ",
        "file_name": "10!1007%pl00011649.pdf",
        "file_path": ".\\PDFs\\10!1007%pl00011649.pdf"
    },
    {
        "title": "Knowledge maps: An essential technique for conceptualisation",
        "implementation_urls": [],
        "doi": "10.1016/s0169-023x(99)00050-6",
        "arxiv": null,
        "abstract": null,
        "file_name": "10!1016%s0169-023x(99)00050-6.pdf",
        "file_path": ".\\PDFs\\10!1016%s0169-023x(99)00050-6.pdf"
    },
    {
        "title": "oeg-upm/kg-scenarios-eval: v1.0.0",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.8179156",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "OME",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764202",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Propagating Ontology Changes to Declarative Mappings in Construction of Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are usually constructed through a set of data transformation pipelines thattransform heterogeneous sources into triples following a set of rules. These rules, usually in the form ofmapping rules (e.g., RML, R2RML, etc.), are a key resource for the construction of the KG as they describethe relationship between the input data sources and the ontology terms. Several efforts have been madeto manage and describe the evolution of the ontology; however, its propagation over interrelated assets(e.g., mapping rules) is commonly done in manual processes. In this paper, we present a first preliminaryapproach to automatically project the evolution of the ontology on the mapping rules used to constructthe KG. For each potential change, we analyse the impact on the mappings and the required steps toensure that the KG is up-to-date w.r.t. the ontology. We implement our solution in fully declarativeworkflows and demonstrate its benefits in a real-world project in the public procurement domain.KeywordsKnowledge Graphs, Ontology Evolution, Mapping Rules, Impact Assessment1. IntroductionKnowledge Graphs (KGs) have been placed as a power mechanism to represent and integratedata on the Web. KGs are often constructed from data sources in diverse formats (e.g., CSV,JSON, etc) using a set of mappings that describe the relationship between the data and terms(i.e. classes and properties) from a target ontology. These mappings rules can be describeddeclaratively using languages such as R2RML [1], RML [2, 3] and SPARQL-Anything [4].The construction of a knowledge graph is formally defined as a data integration system𝐷𝐼𝑆 = (𝑂, 𝑆,𝑀), where 𝑂 is the ontology or vocabulary that defines the global view, 𝑆 are a setof input sources to be integrated w.r.t. 𝑂, and 𝑀 are a set of rules that describe the relationshipKGCW’24: 5th International Workshop on Knowledge Graph Construction, May 26th, 2024, Crete, Greece∗Corresponding author.Envelope-Open diego.conde.herreros@upm.es (D. Conde-Herreros); l.stork@vu.nl (L. Stork); r.pernisch@vu.nl (R. Pernisch);m.poveda@upm.es (M. Poveda-Villalón); oscar.corcho@upm.es (O. Corcho); david.chaves@usc.es(D. Chaves-Fraga)Orcid 0000-0002-4788-1509 (D. Conde-Herreros); 0000-0002-2146-4803 (L. Stork); 0000-0001-8590-1817 (R. Pernisch);0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0002-9260-0753 (O. Corcho); 0000-0003-3236-2789(D. Chaves-Fraga)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:diego.conde.herreros@upm.esmailto:l.stork@vu.nlmailto:r.pernisch@vu.nlmailto:m.poveda@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@usc.eshttps://orcid.org/0000-0002-4788-1509https://orcid.org/0000-0002-2146-4803https://orcid.org/0000-0001-8590-1817https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://creativecommons.org/licenses/by/4.0between 𝑂 and 𝑆 [5]. In this context, ontologies can evolve by incorporating new knowledge orchanging the representation of the domain [6]. Ontology evolution has been widely investigatedin previousworks [7], for instance defining possible change operations [8], developing ontologiesto describe these changes [9] or creating new methodological frameworks [10]. However, tothe best of our knowledge, the study of the impact of ontology evolution on the construction ofknowledge graphs has not yet been investigated.",
        "file_name": "pdf?id=ONL4LGlHNu",
        "file_path": "PDFs\\pdfidONL4LGlHNu.pdf"
    },
    {
        "title": "WIDOCO: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\widoco-iswc2017.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "file_name": "widoco-iswc2017.pdf",
        "file_path": "PDFs\\widoco-iswc2017.pdf"
    },
    {
        "title": "pytada-hdt-entity",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764190",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "RML mapping documentation",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.10732487",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kg-construct/use-cases",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\swj3135.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Knowledge Graphs, R2RML, RML, Scalability 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/SW-223135",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to atarget ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledgegraph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work,we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions aredefined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processedseparately, reducing the total amount of memory and execution time required by the materialization process. We have includedthis optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Ourexperimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presentsthe following advantages: i) it decreases significantly the time required for materialization, ii) it reduces the maximum peak ofmemory used, and iii) it scales to data sizes that other engines are not capable of processing currently.Keywords: Knowledge Graphs, R2RML, RML, Scalability1. IntroductionThe amount of data that is being published in RDFhas been steadily increasing in recent years. The gen-eralized acceptance and use of knowledge graphs(KGs) [1] in a wide range of domains and organiza-tions has contributed to this increase. Given that mostof the data available inside organizations are structuredin heterogeneous data formats, data integration tech-niques are often used in the data transformation andhomogenization process required for knowledge graphconstruction (KGC).*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.KGC engines can be considered as data integra-tion systems DIS = 〈O, S ,M〉 where O is the globalschema expressed in terms of an ontology (or networkof ontologies), S is a set of input data sources and Mare the mapping rules describing the relationships be-tween O and S [2]. Mappings are usually expressed asdeclarative rules, using standard specifications such asthe W3C Recommendation R2RML [3] and its well-known extension for data sources beyond relationaldatabases (RDBs), RML [4]. The construction of KGscan be done using a materialization process or by vir-tualization [5]. Materialization (also known as seman-tic extract-transform-load) uses the rules in M to trans-form all data into RDF. Virtualization uses M to trans-late SPARQL queries into the native query language of1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:david.chaves@upm.esmailto:ja.toledo@upm.esmailto:maria.s.perez@upm.esmailto:oscar.corcho@upm.esmailto:julian.arenas.guerrero@upm.es2 J. Arenas-Guerrero et al. / Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions1 12 23 34 4",
        "file_name": "swj3135.pdf",
        "file_path": "PDFs\\swj3135.pdf"
    },
    {
        "title": "oeg-upm/kg-scenarios-eval: v1.0.0",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.8179156",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "OME",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764202",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Propagating Ontology Changes to Declarative Mappings in Construction of Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are usually constructed through a set of data transformation pipelines thattransform heterogeneous sources into triples following a set of rules. These rules, usually in the form ofmapping rules (e.g., RML, R2RML, etc.), are a key resource for the construction of the KG as they describethe relationship between the input data sources and the ontology terms. Several efforts have been madeto manage and describe the evolution of the ontology; however, its propagation over interrelated assets(e.g., mapping rules) is commonly done in manual processes. In this paper, we present a first preliminaryapproach to automatically project the evolution of the ontology on the mapping rules used to constructthe KG. For each potential change, we analyse the impact on the mappings and the required steps toensure that the KG is up-to-date w.r.t. the ontology. We implement our solution in fully declarativeworkflows and demonstrate its benefits in a real-world project in the public procurement domain.KeywordsKnowledge Graphs, Ontology Evolution, Mapping Rules, Impact Assessment1. IntroductionKnowledge Graphs (KGs) have been placed as a power mechanism to represent and integratedata on the Web. KGs are often constructed from data sources in diverse formats (e.g., CSV,JSON, etc) using a set of mappings that describe the relationship between the data and terms(i.e. classes and properties) from a target ontology. These mappings rules can be describeddeclaratively using languages such as R2RML [1], RML [2, 3] and SPARQL-Anything [4].The construction of a knowledge graph is formally defined as a data integration system𝐷𝐼𝑆 = (𝑂, 𝑆,𝑀), where 𝑂 is the ontology or vocabulary that defines the global view, 𝑆 are a setof input sources to be integrated w.r.t. 𝑂, and 𝑀 are a set of rules that describe the relationshipKGCW’24: 5th International Workshop on Knowledge Graph Construction, May 26th, 2024, Crete, Greece∗Corresponding author.Envelope-Open diego.conde.herreros@upm.es (D. Conde-Herreros); l.stork@vu.nl (L. Stork); r.pernisch@vu.nl (R. Pernisch);m.poveda@upm.es (M. Poveda-Villalón); oscar.corcho@upm.es (O. Corcho); david.chaves@usc.es(D. Chaves-Fraga)Orcid 0000-0002-4788-1509 (D. Conde-Herreros); 0000-0002-2146-4803 (L. Stork); 0000-0001-8590-1817 (R. Pernisch);0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0002-9260-0753 (O. Corcho); 0000-0003-3236-2789(D. Chaves-Fraga)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:diego.conde.herreros@upm.esmailto:l.stork@vu.nlmailto:r.pernisch@vu.nlmailto:m.poveda@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@usc.eshttps://orcid.org/0000-0002-4788-1509https://orcid.org/0000-0002-2146-4803https://orcid.org/0000-0001-8590-1817https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://creativecommons.org/licenses/by/4.0between 𝑂 and 𝑆 [5]. In this context, ontologies can evolve by incorporating new knowledge orchanging the representation of the domain [6]. Ontology evolution has been widely investigatedin previousworks [7], for instance defining possible change operations [8], developing ontologiesto describe these changes [9] or creating new methodological frameworks [10]. However, tothe best of our knowledge, the study of the impact of ontology evolution on the construction ofknowledge graphs has not yet been investigated.",
        "file_name": "pdf?id=ONL4LGlHNu",
        "file_path": "PDFs\\pdfidONL4LGlHNu.pdf"
    },
    {
        "title": "WIDOCO: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\widoco-iswc2017.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "file_name": "widoco-iswc2017.pdf",
        "file_path": "PDFs\\widoco-iswc2017.pdf"
    },
    {
        "title": "pytada-hdt-entity",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764190",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "RML mapping documentation",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.10732487",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kg-construct/use-cases",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\swj3135.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Knowledge Graphs, R2RML, RML, Scalability 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/SW-223135",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to atarget ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledgegraph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work,we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions aredefined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processedseparately, reducing the total amount of memory and execution time required by the materialization process. We have includedthis optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Ourexperimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presentsthe following advantages: i) it decreases significantly the time required for materialization, ii) it reduces the maximum peak ofmemory used, and iii) it scales to data sizes that other engines are not capable of processing currently.Keywords: Knowledge Graphs, R2RML, RML, Scalability1. IntroductionThe amount of data that is being published in RDFhas been steadily increasing in recent years. The gen-eralized acceptance and use of knowledge graphs(KGs) [1] in a wide range of domains and organiza-tions has contributed to this increase. Given that mostof the data available inside organizations are structuredin heterogeneous data formats, data integration tech-niques are often used in the data transformation andhomogenization process required for knowledge graphconstruction (KGC).*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.KGC engines can be considered as data integra-tion systems DIS = 〈O, S ,M〉 where O is the globalschema expressed in terms of an ontology (or networkof ontologies), S is a set of input data sources and Mare the mapping rules describing the relationships be-tween O and S [2]. Mappings are usually expressed asdeclarative rules, using standard specifications such asthe W3C Recommendation R2RML [3] and its well-known extension for data sources beyond relationaldatabases (RDBs), RML [4]. The construction of KGscan be done using a materialization process or by vir-tualization [5]. Materialization (also known as seman-tic extract-transform-load) uses the rules in M to trans-form all data into RDF. Virtualization uses M to trans-late SPARQL queries into the native query language of1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:david.chaves@upm.esmailto:ja.toledo@upm.esmailto:maria.s.perez@upm.esmailto:oscar.corcho@upm.esmailto:julian.arenas.guerrero@upm.es2 J. Arenas-Guerrero et al. / Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions1 12 23 34 4",
        "file_name": "swj3135.pdf",
        "file_path": "PDFs\\swj3135.pdf"
    },
    {
        "title": "oeg-upm/kg-scenarios-eval: v1.0.0",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.8179156",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "OME",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764202",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Propagating Ontology Changes to Declarative Mappings in Construction of Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are usually constructed through a set of data transformation pipelines thattransform heterogeneous sources into triples following a set of rules. These rules, usually in the form ofmapping rules (e.g., RML, R2RML, etc.), are a key resource for the construction of the KG as they describethe relationship between the input data sources and the ontology terms. Several efforts have been madeto manage and describe the evolution of the ontology; however, its propagation over interrelated assets(e.g., mapping rules) is commonly done in manual processes. In this paper, we present a first preliminaryapproach to automatically project the evolution of the ontology on the mapping rules used to constructthe KG. For each potential change, we analyse the impact on the mappings and the required steps toensure that the KG is up-to-date w.r.t. the ontology. We implement our solution in fully declarativeworkflows and demonstrate its benefits in a real-world project in the public procurement domain.KeywordsKnowledge Graphs, Ontology Evolution, Mapping Rules, Impact Assessment1. IntroductionKnowledge Graphs (KGs) have been placed as a power mechanism to represent and integratedata on the Web. KGs are often constructed from data sources in diverse formats (e.g., CSV,JSON, etc) using a set of mappings that describe the relationship between the data and terms(i.e. classes and properties) from a target ontology. These mappings rules can be describeddeclaratively using languages such as R2RML [1], RML [2, 3] and SPARQL-Anything [4].The construction of a knowledge graph is formally defined as a data integration system𝐷𝐼𝑆 = (𝑂, 𝑆,𝑀), where 𝑂 is the ontology or vocabulary that defines the global view, 𝑆 are a setof input sources to be integrated w.r.t. 𝑂, and 𝑀 are a set of rules that describe the relationshipKGCW’24: 5th International Workshop on Knowledge Graph Construction, May 26th, 2024, Crete, Greece∗Corresponding author.Envelope-Open diego.conde.herreros@upm.es (D. Conde-Herreros); l.stork@vu.nl (L. Stork); r.pernisch@vu.nl (R. Pernisch);m.poveda@upm.es (M. Poveda-Villalón); oscar.corcho@upm.es (O. Corcho); david.chaves@usc.es(D. Chaves-Fraga)Orcid 0000-0002-4788-1509 (D. Conde-Herreros); 0000-0002-2146-4803 (L. Stork); 0000-0001-8590-1817 (R. Pernisch);0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0002-9260-0753 (O. Corcho); 0000-0003-3236-2789(D. Chaves-Fraga)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:diego.conde.herreros@upm.esmailto:l.stork@vu.nlmailto:r.pernisch@vu.nlmailto:m.poveda@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@usc.eshttps://orcid.org/0000-0002-4788-1509https://orcid.org/0000-0002-2146-4803https://orcid.org/0000-0001-8590-1817https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://creativecommons.org/licenses/by/4.0between 𝑂 and 𝑆 [5]. In this context, ontologies can evolve by incorporating new knowledge orchanging the representation of the domain [6]. Ontology evolution has been widely investigatedin previousworks [7], for instance defining possible change operations [8], developing ontologiesto describe these changes [9] or creating new methodological frameworks [10]. However, tothe best of our knowledge, the study of the impact of ontology evolution on the construction ofknowledge graphs has not yet been investigated.",
        "file_name": "pdf?id=ONL4LGlHNu",
        "file_path": "PDFs\\pdfidONL4LGlHNu.pdf"
    },
    {
        "title": "WIDOCO: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\widoco-iswc2017.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "file_name": "widoco-iswc2017.pdf",
        "file_path": "PDFs\\widoco-iswc2017.pdf"
    },
    {
        "title": "pytada-hdt-entity",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764190",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "RML mapping documentation",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.10732487",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kg-construct/use-cases",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\swj3135.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Knowledge Graphs, R2RML, RML, Scalability 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/SW-223135",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to atarget ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledgegraph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work,we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions aredefined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processedseparately, reducing the total amount of memory and execution time required by the materialization process. We have includedthis optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Ourexperimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presentsthe following advantages: i) it decreases significantly the time required for materialization, ii) it reduces the maximum peak ofmemory used, and iii) it scales to data sizes that other engines are not capable of processing currently.Keywords: Knowledge Graphs, R2RML, RML, Scalability1. IntroductionThe amount of data that is being published in RDFhas been steadily increasing in recent years. The gen-eralized acceptance and use of knowledge graphs(KGs) [1] in a wide range of domains and organiza-tions has contributed to this increase. Given that mostof the data available inside organizations are structuredin heterogeneous data formats, data integration tech-niques are often used in the data transformation andhomogenization process required for knowledge graphconstruction (KGC).*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.KGC engines can be considered as data integra-tion systems DIS = 〈O, S ,M〉 where O is the globalschema expressed in terms of an ontology (or networkof ontologies), S is a set of input data sources and Mare the mapping rules describing the relationships be-tween O and S [2]. Mappings are usually expressed asdeclarative rules, using standard specifications such asthe W3C Recommendation R2RML [3] and its well-known extension for data sources beyond relationaldatabases (RDBs), RML [4]. The construction of KGscan be done using a materialization process or by vir-tualization [5]. Materialization (also known as seman-tic extract-transform-load) uses the rules in M to trans-form all data into RDF. Virtualization uses M to trans-late SPARQL queries into the native query language of1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:david.chaves@upm.esmailto:ja.toledo@upm.esmailto:maria.s.perez@upm.esmailto:oscar.corcho@upm.esmailto:julian.arenas.guerrero@upm.es2 J. Arenas-Guerrero et al. / Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions1 12 23 34 4",
        "file_name": "swj3135.pdf",
        "file_path": "PDFs\\swj3135.pdf"
    },
    {
        "title": "oeg-upm/kg-scenarios-eval: v1.0.0",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.8179156",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "OME",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764202",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Propagating Ontology Changes to Declarative Mappings in Construction of Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are usually constructed through a set of data transformation pipelines thattransform heterogeneous sources into triples following a set of rules. These rules, usually in the form ofmapping rules (e.g., RML, R2RML, etc.), are a key resource for the construction of the KG as they describethe relationship between the input data sources and the ontology terms. Several efforts have been madeto manage and describe the evolution of the ontology; however, its propagation over interrelated assets(e.g., mapping rules) is commonly done in manual processes. In this paper, we present a first preliminaryapproach to automatically project the evolution of the ontology on the mapping rules used to constructthe KG. For each potential change, we analyse the impact on the mappings and the required steps toensure that the KG is up-to-date w.r.t. the ontology. We implement our solution in fully declarativeworkflows and demonstrate its benefits in a real-world project in the public procurement domain.KeywordsKnowledge Graphs, Ontology Evolution, Mapping Rules, Impact Assessment1. IntroductionKnowledge Graphs (KGs) have been placed as a power mechanism to represent and integratedata on the Web. KGs are often constructed from data sources in diverse formats (e.g., CSV,JSON, etc) using a set of mappings that describe the relationship between the data and terms(i.e. classes and properties) from a target ontology. These mappings rules can be describeddeclaratively using languages such as R2RML [1], RML [2, 3] and SPARQL-Anything [4].The construction of a knowledge graph is formally defined as a data integration system𝐷𝐼𝑆 = (𝑂, 𝑆,𝑀), where 𝑂 is the ontology or vocabulary that defines the global view, 𝑆 are a setof input sources to be integrated w.r.t. 𝑂, and 𝑀 are a set of rules that describe the relationshipKGCW’24: 5th International Workshop on Knowledge Graph Construction, May 26th, 2024, Crete, Greece∗Corresponding author.Envelope-Open diego.conde.herreros@upm.es (D. Conde-Herreros); l.stork@vu.nl (L. Stork); r.pernisch@vu.nl (R. Pernisch);m.poveda@upm.es (M. Poveda-Villalón); oscar.corcho@upm.es (O. Corcho); david.chaves@usc.es(D. Chaves-Fraga)Orcid 0000-0002-4788-1509 (D. Conde-Herreros); 0000-0002-2146-4803 (L. Stork); 0000-0001-8590-1817 (R. Pernisch);0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0002-9260-0753 (O. Corcho); 0000-0003-3236-2789(D. Chaves-Fraga)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:diego.conde.herreros@upm.esmailto:l.stork@vu.nlmailto:r.pernisch@vu.nlmailto:m.poveda@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@usc.eshttps://orcid.org/0000-0002-4788-1509https://orcid.org/0000-0002-2146-4803https://orcid.org/0000-0001-8590-1817https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://creativecommons.org/licenses/by/4.0between 𝑂 and 𝑆 [5]. In this context, ontologies can evolve by incorporating new knowledge orchanging the representation of the domain [6]. Ontology evolution has been widely investigatedin previousworks [7], for instance defining possible change operations [8], developing ontologiesto describe these changes [9] or creating new methodological frameworks [10]. However, tothe best of our knowledge, the study of the impact of ontology evolution on the construction ofknowledge graphs has not yet been investigated.",
        "file_name": "pdf?id=ONL4LGlHNu",
        "file_path": "PDFs\\pdfidONL4LGlHNu.pdf"
    },
    {
        "title": "WIDOCO: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\widoco-iswc2017.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "file_name": "widoco-iswc2017.pdf",
        "file_path": "PDFs\\widoco-iswc2017.pdf"
    },
    {
        "title": "pytada-hdt-entity",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764190",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "RML mapping documentation",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.10732487",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kg-construct/use-cases",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\swj3135.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Knowledge Graphs, R2RML, RML, Scalability 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/SW-223135",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to atarget ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledgegraph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work,we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions aredefined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processedseparately, reducing the total amount of memory and execution time required by the materialization process. We have includedthis optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Ourexperimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presentsthe following advantages: i) it decreases significantly the time required for materialization, ii) it reduces the maximum peak ofmemory used, and iii) it scales to data sizes that other engines are not capable of processing currently.Keywords: Knowledge Graphs, R2RML, RML, Scalability1. IntroductionThe amount of data that is being published in RDFhas been steadily increasing in recent years. The gen-eralized acceptance and use of knowledge graphs(KGs) [1] in a wide range of domains and organiza-tions has contributed to this increase. Given that mostof the data available inside organizations are structuredin heterogeneous data formats, data integration tech-niques are often used in the data transformation andhomogenization process required for knowledge graphconstruction (KGC).*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.KGC engines can be considered as data integra-tion systems DIS = 〈O, S ,M〉 where O is the globalschema expressed in terms of an ontology (or networkof ontologies), S is a set of input data sources and Mare the mapping rules describing the relationships be-tween O and S [2]. Mappings are usually expressed asdeclarative rules, using standard specifications such asthe W3C Recommendation R2RML [3] and its well-known extension for data sources beyond relationaldatabases (RDBs), RML [4]. The construction of KGscan be done using a materialization process or by vir-tualization [5]. Materialization (also known as seman-tic extract-transform-load) uses the rules in M to trans-form all data into RDF. Virtualization uses M to trans-late SPARQL queries into the native query language of1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:david.chaves@upm.esmailto:ja.toledo@upm.esmailto:maria.s.perez@upm.esmailto:oscar.corcho@upm.esmailto:julian.arenas.guerrero@upm.es2 J. Arenas-Guerrero et al. / Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions1 12 23 34 4",
        "file_name": "swj3135.pdf",
        "file_path": "PDFs\\swj3135.pdf"
    },
    {
        "title": "oeg-upm/kg-scenarios-eval: v1.0.0",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.8179156",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "OME",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764202",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Propagating Ontology Changes to Declarative Mappings in Construction of Knowledge Graphs",
        "implementation_urls": [],
        "doi": null,
        "arxiv": null,
        "abstract": "AbstractKnowledge Graphs (KGs) are usually constructed through a set of data transformation pipelines thattransform heterogeneous sources into triples following a set of rules. These rules, usually in the form ofmapping rules (e.g., RML, R2RML, etc.), are a key resource for the construction of the KG as they describethe relationship between the input data sources and the ontology terms. Several efforts have been madeto manage and describe the evolution of the ontology; however, its propagation over interrelated assets(e.g., mapping rules) is commonly done in manual processes. In this paper, we present a first preliminaryapproach to automatically project the evolution of the ontology on the mapping rules used to constructthe KG. For each potential change, we analyse the impact on the mappings and the required steps toensure that the KG is up-to-date w.r.t. the ontology. We implement our solution in fully declarativeworkflows and demonstrate its benefits in a real-world project in the public procurement domain.KeywordsKnowledge Graphs, Ontology Evolution, Mapping Rules, Impact Assessment1. IntroductionKnowledge Graphs (KGs) have been placed as a power mechanism to represent and integratedata on the Web. KGs are often constructed from data sources in diverse formats (e.g., CSV,JSON, etc) using a set of mappings that describe the relationship between the data and terms(i.e. classes and properties) from a target ontology. These mappings rules can be describeddeclaratively using languages such as R2RML [1], RML [2, 3] and SPARQL-Anything [4].The construction of a knowledge graph is formally defined as a data integration system𝐷𝐼𝑆 = (𝑂, 𝑆,𝑀), where 𝑂 is the ontology or vocabulary that defines the global view, 𝑆 are a setof input sources to be integrated w.r.t. 𝑂, and 𝑀 are a set of rules that describe the relationshipKGCW’24: 5th International Workshop on Knowledge Graph Construction, May 26th, 2024, Crete, Greece∗Corresponding author.Envelope-Open diego.conde.herreros@upm.es (D. Conde-Herreros); l.stork@vu.nl (L. Stork); r.pernisch@vu.nl (R. Pernisch);m.poveda@upm.es (M. Poveda-Villalón); oscar.corcho@upm.es (O. Corcho); david.chaves@usc.es(D. Chaves-Fraga)Orcid 0000-0002-4788-1509 (D. Conde-Herreros); 0000-0002-2146-4803 (L. Stork); 0000-0001-8590-1817 (R. Pernisch);0000-0003-3587-0367 (M. Poveda-Villalón); 0000-0002-9260-0753 (O. Corcho); 0000-0003-3236-2789(D. Chaves-Fraga)© 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).mailto:diego.conde.herreros@upm.esmailto:l.stork@vu.nlmailto:r.pernisch@vu.nlmailto:m.poveda@upm.esmailto:oscar.corcho@upm.esmailto:david.chaves@usc.eshttps://orcid.org/0000-0002-4788-1509https://orcid.org/0000-0002-2146-4803https://orcid.org/0000-0001-8590-1817https://orcid.org/0000-0003-3587-0367https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-3236-2789https://creativecommons.org/licenses/by/4.0between 𝑂 and 𝑆 [5]. In this context, ontologies can evolve by incorporating new knowledge orchanging the representation of the domain [6]. Ontology evolution has been widely investigatedin previousworks [7], for instance defining possible change operations [8], developing ontologiesto describe these changes [9] or creating new methodological frameworks [10]. However, tothe best of our knowledge, the study of the impact of ontology evolution on the construction ofknowledge graphs has not yet been investigated.",
        "file_name": "pdf?id=ONL4LGlHNu",
        "file_path": "PDFs\\pdfidONL4LGlHNu.pdf"
    },
    {
        "title": "WIDOCO: a wizard for documenting ontologies",
        "implementation_urls": [
            {
                "identifier": "https://github.com/dgarijo/Widoco",
                "type": "git",
                "paper_frequency": 1,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\widoco-iswc2017.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "WIDOCO is available in GitHub,12 where users can download it, open issues or ask for help."
                    }
                ]
            }
        ],
        "doi": "10.1007/978-3-319-68204-4_9",
        "arxiv": null,
        "abstract": "Abstract. In this paper we describe WIDOCO, a WIzard for DOCu-menting Ontologies that guides users through the documentation processof their vocabularies. Given an RDF vocabulary, WIDOCO detects miss-ing vocabulary metadata and creates a documentation with diagrams,human readable descriptions of the ontology terms and a summary ofchanges with respect to previous versions of the ontology. The docu-mentation consists on a set of linked enriched HTML pages that can befurther extended by end users. WIDOCO is open source and builds onwell established Semantic Web tools. So far, WIDOCO has been used todocument more than one hundred ontologies in different domains.Keywords: Ontology documentation, Ontology evolution, Ontology un-derstanding, OWL OntologiesResource Type: SoftwarePermanent URL: https://w3id.org/widocoSoftware DOI: https://doi.org/10.5281/zenodo.5912941 IntroductionOntology engineering methodologies acknowledge reuse of existing vocabulariesas a crucial step when developing a new ontology [11]. Therefore, ontology au-thors often provide a human-readable documentation of their vocabularies, inorder to facilitate their understanding and adoption by other researchers [9].There are three main aspects related to ontology documentation. The firstone is creating a human-readable representation of the content of the ontology:metadata, definition of classes and properties, visualization (e.g., diagrams relat-ing the different concepts) and versioning (explanation of the difference betweenversions of the ontologies). The second aspect is creating machine-readable an-notations of documentation metadata (e.g., provenance, snippets for facilitatingvocabulary discovery by search engines) and the third aspect is preparing thedocumentation files to be accessed as a web resource (doing content negotiation).Related work has been proposed to facilitate some of these aspects. For ex-ample, ontology editors like Protégé [8], have plugins for automatically creatingan HTML documentation with the definition of classes and properties.1 Simi-larly, approaches like LODE [9] or Parrot [12] provide drag-and-drop services to1 https://protegewiki.stanford.edu/wiki/OWLDocautomatically document ontology terms. However, most approaches are typicallydesigned for Semantic Web experts, presenting some of the following issues:1. Lack of guidelines and best practices for ontology documentation: users de-veloping ontologies may not know which are the common terms used todescribe the metadata of their ontologies. These metadata are important,because they are used by existing tools to create human readable descrip-tions of an ontology.2. Lack of ontology metadata completion: Current efforts do not indicate whichkey information may be missing when documenting an ontology.3. Lack of an ecosystem for ontology documentation and customization: mostexisting approaches focus on specific aspects of ontology documentation.On the one hand, approaches like LODE [9] generate a human readabledescription of the classes and properties of a given ontology, but neglect thegeneration of diagrams. On the other hand, tools like WebVowl [5] createdynamic visualizations of ontologies, but do not deal with the generation oftext. Integrating the outcome of these and other tools and customizing themaccording to user preferences takes time, especially to non programmers.",
        "file_name": "widoco-iswc2017.pdf",
        "file_path": "PDFs\\widoco-iswc2017.pdf"
    },
    {
        "title": "pytada-hdt-entity",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.3764190",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "RML mapping documentation",
        "implementation_urls": [],
        "doi": "10.5281/zenodo.10732487",
        "arxiv": null,
        "abstract": "Conference Abstract Challenges for FAIR Digital Object AssessmentEsteban Gonzalez , Daniel Garijo , Oscar Corcho‡ Universidad Politécnica de Madrid, Madrid, SpainCorresponding author: Esteban Gonzalez (esteban.gonzalez@upm.es), Daniel Garijo (daniel.garijo@upm.es),Oscar Corcho (ocorcho@fi.upm.es)Received: 03 Oct 2022 | Published: 12 Oct 2022Citation: Gonzalez E, Garijo D, Corcho O (2022) Challenges for FAIR Digital Object Assessment. ResearchIdeas and Outcomes 8: e95943. https://doi.org/10.3897/rio.8.e95943AbstractA Digital Object (DO) \"is a sequence of bits, incorporating a work or portion of a work orother information in which a party has rights or interests, or in which there is value\". DOsshould  have  persistent  identifiers,  meta-data  and  be  readable  by  both  humans  andmachines. A FAIR Digital Object is a DO able to interact with automated data processingsystems  (De  Smedt  et  al.  2020)  while  following  the  FAIR  (Findable,  Accessible,Interoperable and Reusable principles) principles (Wilkinson et al. 2016).Although FAIR was originally targeted towards data artifacts, new initiatives have emergedto adapt other research digital resources such as software (Katz et al. 2021) (Lamprecht etal. 2020), ontologies (Poveda-Villalón et al. 2020), virtual research environments and evenDOs (Collins et al. 2018). In this paper, we describe the challenges when assessing thelevel of compliance of a DO with the FAIR principles (i.e., its FAIRness), assuming that aDO  contains  multiple  resources  and  captures  their  relationships.  We  explore  differentmethods to calculate an evaluation score, and we discuss the challeneges and importanceof providing explanations and guidelines for authors.FAIR assessment tools There are a growing number of tools used to assess the FAIRness of DOs. Communitygroups like FAIRassist.org have compiled lists of guidelines and tools for assessing theFAIRness of digital resources. These range from self assessment tools like questionnairesand  checklists  to  semi-automated  validators  (Devaraju  et  al.  2021).  Examples  ofautomated  validation  tools  include  the  F-UJI  Automated  FAIR  Data  Assessment  Tool ‡ ‡ ‡© Gonzalez E et al. This is an open access article distributed under the terms of the Creative Commons Attribution License (CCBY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source arecredited.https://doi.org/10.3897/rio.8.e95943http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12http://crossmark.crossref.org/dialog/?doi=10.3897/rio.8.e95943&domain=pdf&date_stamp=2022-10-12mailto:esteban.gonzalez@upm.esmailto:daniel.garijo@upm.esmailto:ocorcho@fi.upm.eshttps://doi.org/10.3897/rio.8.e95943https://www.dona.net/digitalobjectarchitecturehttps://rd-alliance.org/group/fair-virtual-research-environments-wg/case-%20statement/fair-virtual-research-environments-vrehttps://fairassist.org/https://www.fairsfair.eu/f-uji-automated-fair-data-assessment-too(Devaraju and Huber 2020), FAIR Evaluator and FAIR Checker for datasets or individualDOs; HowFairIs (Spaaks et al. 2021) for code repositories; and and FOOPS (Garijo et al.2021) to assess ontologies.When it comes to assessing FDOs, we find two main challenges:",
        "file_path": "PDFs\\.pdf"
    },
    {
        "title": "Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions",
        "implementation_urls": [
            {
                "identifier": "https://github.com/kg-construct/use-cases",
                "type": "git",
                "paper_frequency": 2,
                "extraction_methods": [
                    {
                        "type": "regex"
                    },
                    {
                        "type": "unidir",
                        "location": "PDFs\\swj3135.pdf",
                        "location_type": "PAPER",
                        "source_paragraph": "Keywords: Knowledge Graphs, R2RML, RML, Scalability 1."
                    }
                ]
            }
        ],
        "doi": "10.3233/SW-223135",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs are often constructed from heterogeneous data sources, using declarative rules that map them to atarget ontology and materializing them into RDF. When these data sources are large, the materialization of the entire knowledgegraph may be computationally expensive and not suitable for those cases where a rapid materialization is required. In this work,we propose an approach to overcome this limitation, based on the novel concept of mapping partitions. Mapping partitions aredefined as groups of mapping rules that generate disjoint subsets of the knowledge graph. Each of these groups can be processedseparately, reducing the total amount of memory and execution time required by the materialization process. We have includedthis optimization in our materialization engine Morph-KGC, and we have evaluated it over three different benchmarks. Ourexperimental results show that, compared with state-of-the-art techniques, the use of mapping partitions in Morph-KGC presentsthe following advantages: i) it decreases significantly the time required for materialization, ii) it reduces the maximum peak ofmemory used, and iii) it scales to data sizes that other engines are not capable of processing currently.Keywords: Knowledge Graphs, R2RML, RML, Scalability1. IntroductionThe amount of data that is being published in RDFhas been steadily increasing in recent years. The gen-eralized acceptance and use of knowledge graphs(KGs) [1] in a wide range of domains and organiza-tions has contributed to this increase. Given that mostof the data available inside organizations are structuredin heterogeneous data formats, data integration tech-niques are often used in the data transformation andhomogenization process required for knowledge graphconstruction (KGC).*Corresponding author. E-mail: julian.arenas.guerrero@upm.es.KGC engines can be considered as data integra-tion systems DIS = 〈O, S ,M〉 where O is the globalschema expressed in terms of an ontology (or networkof ontologies), S is a set of input data sources and Mare the mapping rules describing the relationships be-tween O and S [2]. Mappings are usually expressed asdeclarative rules, using standard specifications such asthe W3C Recommendation R2RML [3] and its well-known extension for data sources beyond relationaldatabases (RDBs), RML [4]. The construction of KGscan be done using a materialization process or by vir-tualization [5]. Materialization (also known as seman-tic extract-transform-load) uses the rules in M to trans-form all data into RDF. Virtualization uses M to trans-late SPARQL queries into the native query language of1570-0844/$35.00 © 0 – IOS Press and the authors. All rights reservedmailto:julian.arenas.guerrero@upm.esmailto:david.chaves@upm.esmailto:ja.toledo@upm.esmailto:maria.s.perez@upm.esmailto:oscar.corcho@upm.esmailto:julian.arenas.guerrero@upm.es2 J. Arenas-Guerrero et al. / Morph-KGC: Scalable Knowledge Graph Materialization with Mapping Partitions1 12 23 34 4",
        "file_name": "swj3135.pdf",
        "file_path": "PDFs\\swj3135.pdf"
    },
    {
        "title": "MuHeQA: Zero-shot question answering over multiple and heterogeneous knowledge bases",
        "implementation_urls": [
            {
                "identifier": "https://github.com/librairy/MuHeQA",
                "type": "git",
                "paper_frequency": 6,
                "extraction_methods": [
                    {
                        "type": "regex",
                        "location": "",
                        "location_type": "",
                        "source": "",
                        "source_paragraph": ""
                    },
                    {
                        "type": "unidir",
                        "location": ".\\PDFs\\10!3233%sw-233379.pdf",
                        "location_type": "PAPER",
                        "source": "",
                        "source_paragraph": "This evaluation set is available as part of our additional material.12 4.2."
                    }
                ]
            }
        ],
        "doi": "10.3233/sw-233379",
        "arxiv": null,
        "abstract": "Abstract. There are two main limitations in most of the existing Knowledge Graph Question Answering (KGQA) algorithms.First, the approaches depend heavily on the structure and cannot be easily adapted to other KGs. Second, the availability andamount of additional domain-specific data in structured or unstructured formats has also proven to be critical in many of thesesystems. Such dependencies limit the applicability of KGQA systems and make their adoption difficult. A novel algorithm isproposed, MuHeQA, that alleviates both limitations by retrieving the answer from textual content automatically generated fromKGs instead of queries over them. This new approach (1) works on one or several KGs simultaneously, (2) does not requiretraining data what makes it is domain-independent, (3) enables the combination of knowledge graphs with unstructured infor-mation sources to build the answer, and (4) reduces the dependency on the underlying schema since it does not navigate throughstructured content but only reads property values. MuHeQA extracts answers from textual summaries created by combininginformation related to the question from multiple knowledge bases, be them structured or not. Experiments over Wikidata andDBpedia show that our approach achieves comparable performance to other approaches in single-fact questions while beingdomain and KG independent. Results raise important questions for future work about how the textual content that can be createdfrom knowledge graphs enables answer extraction.Keywords: Question answering, natural language processing, knowledge graphs1. IntroductionKnowledge graphs are now being applied in multiple domains. Knowledge Graph Question Answering (KGQA)has emerged as a way to provide an intuitive mechanism for non-expert users to query knowledge graphs. KGQAsystems do not require specific technical knowledge (e.g., knowledge of SPARQL or Cypher), providing answers innatural language for questions that are also expressed in natural language.One of the main challenges in the design of KGQA systems is semantic parsing [14]. In this step, natural languagequeries (NLQs) are translated into a specific query language (e.g. SPARQL1 for RDF-based KGs, or Cypher2 for*Corresponding author. E-mail: carlos.badenes@upm.es.1https://www.w3.org/TR/rdf-sparql-query2https://opencypher.org1570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:carlos.badenes@upm.eshttps://orcid.org/0000-0002-2753-9917mailto:oscar.corcho@upm.eshttps://orcid.org/0000-0002-9260-0753mailto:carlos.badenes@upm.eshttps://www.w3.org/TR/rdf-sparql-queryhttps://opencypher.orghttps://creativecommons.org/licenses/by/4.0/CORRECTED  PROOF2 C. Badenes-Olmedo and O. Corcho / MuHeQAproperty graphs). KGQA systems typically use templates with placeholders for relations and entities that mustbe identified in the original NLQ. Once the template is filled in, the generated query (in SPARQL, Cypher, etc.)is evaluated in the system that stores the KG (e.g. Triple store, property graph DB) and the results will providethe answers to the question. Thus, KGQA systems can be reduced to entity and relationship search processes thatpopulate predefined query templates that are commonly identified using supervised machine learning techniques(e.g., supervised classifiers) [23]. This approach is heavily dependent on the data schema to explore the entityrelationships and on the availability of training data to create supervised classification models.Mitigating the dependency of KGQA systems on the underlying graph structure and eliminating the need fortraining sets is crucial to create cross-cutting solutions more efficiently and with less cost. The first research questionaddressed in this work is: “How to extract the answer from a knowledge graph without translating the naturallanguage question into a formal query language?”. Moreover, the dependence on the underlying data schema makesit also difficult for existing KGQA systems to combine knowledge from several graphs to extract a single answer.They typically translate the question into specific queries for each supported KG and obtain multiple answers, rather",
        "file_name": "10!3233%sw-233379.pdf",
        "file_path": ".\\PDFs\\10!3233%sw-233379.pdf"
    },
    {
        "title": "Satellite Earth Observation for Essential Climate Variables Supporting Sustainable Development Goals: A Review on Applications",
        "implementation_urls": [],
        "doi": "10.3390/rs15112716",
        "arxiv": null,
        "abstract": "Abstract: Essential climate variables (ECVs) have been recognized as crucial information for achievingSustainable Development Goals (SDGs). There is an agreement on 54 ECVs to understand climateevolution, and multiple rely on satellite Earth observation (abbreviated as s-ECVs). Despite the effortsto encourage s-ECV use for SDGs, there is still a need to further integrate them into the indicatorcalculations. Therefore, we conducted a systematic literature review to identify s-ECVs used inSDG monitoring. Results showed the use of 14 s-ECVs, the most frequent being land cover, ozone,precursors for aerosols and ozone, precipitation, land surface temperature, soil moisture, soil carbon,lakes, and leaf area index. They were related to 16 SDGs (mainly SDGs 3, 6, 11, 14, and 15), 33 targets,and 23 indicators. However, only 10 indicators (belonging to SDGs 6, 11, and 15) were calculatedusing s-ECVs. This review raises research opportunities by identifying s-ECVs yet to be used in theindicator calculations. Therefore, indicators supporting SDGs must be updated to use this valuablesource of information which, in turn, allows a worldwide indicator comparison. Additionally, thisreview is relevant for scientists and policymakers for future actions and policies to better integrates-ECVs into the Agenda 2030.Keywords: SDG; sustainable development; satellite; Earth observation; review; essential variables;climate1. IntroductionThe Agenda 2030 for Sustainable Development and its 17 goals (SDGs) are connectedwith the environment, economy, and society dimensions of sustainable development [1].The 17 goals, their 169 associated targets, and 231 indicators are based on the first data-driven policy development framework, following the principle of “If you don’t measureit, you can’t manage it” [2] (p. 2). Despite the recognized importance of measuring theprogress towards the SDGs, two-thirds of the indicators remain unreported, especially inlow-income countries [3]. Moreover, less than 44% of the SDG indicators can be easilymeasured [4]. Therefore, it is a priority to boost the measuring and monitoring of theprogress towards the SDGs. In our work, we focus on two key approaches to pursue thisRemote Sens. 2023, 15, 2716. https://doi.org/10.3390/rs15112716 https://www.mdpi.com/journal/remotesensinghttps://doi.org/10.3390/rs15112716https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.comhttps://orcid.org/0000-0002-6926-4827https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0001-7380-0701https://orcid.org/0000-0001-5145-9223https://orcid.org/0000-0001-6559-2033https://doi.org/10.3390/rs15112716https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.com/article/10.3390/rs15112716?type=check_update&version=1Remote Sens. 2023, 15, 2716 2 of 25aim: (1) essential variables (EVs) that have been defined as an intermediate layer betweenobservations and indicators [5] and (2) satellite Earth observation (sEO) data that gainedparticular attention as worldwide feasible, cost-effective, and analysis-ready data acrossscales in remote, non-accessible, and poorly monitored regions [6].The EVs emerged in various social and environmental scientific communities relatedto specific domains such as climate, biodiversity, agriculture, and society [5,7–9]. Referto [10,11] for detailed EV compendiums. These kinds of variables are “a minimal setof variables that determine the system’s state and developments, [which] are crucial for",
        "file_name": "pdf?version=1684894645",
        "file_path": "PDFs\\pdfversion1684894645.pdf"
    }
]