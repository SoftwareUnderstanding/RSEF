[
    {
        "title": "Traffic Optimization Through Waiting Prediction and Evolutive Algorithms",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.12.001",
        "arxiv": null,
        "abstract": "AbstractTraffic optimization systems require optimization procedures to optimize traffic light timing settings in order to improve pedestrian and vehicle mobility. Traffic simulators allow obtaining accurate estimates of traffic behavior by applying different timing configurations, but require considerable computational time to perform validation tests. For this reason, this project proposes the development of traffic optimizations based on the estimation of vehicle waiting times through the use of different prediction techniques and the use of this estimation to subsequently apply evolutionary algorithms that allow the optimizations to be carried out. The combination of these two techniques leads to a considerable reduction in calculation time, which makes it possible to apply this system at runtime. The tests have been carried out on a real traffic junction on which different traffic volumes have been applied to analyze the performance of the system.DOI:  10.9781/ijimai.2023.12.001Traffic Optimization Through Waiting Prediction and Evolutive AlgorithmsFrancisco García1, Helena Hernández2, María N. Moreno-García2, Juan F. De Paz1*, Vivian F. López2, Javier Bajo31 Expert Systems and Applications Lab. University of Salamanca. Plaza de los Caídos s/n. Salamanca (Spain)2 Data Mining Research Group. University of Salamanca Plaza de los Caídos s/n. Salamanca (Spain)3 Department of Artificial Intelligence, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)Received 10 May 2022 | Accepted 22 September 2023 | Early Access 5 December 2023 I. IntroductionACCORDING to United Nations data, in 2018 55% of the population was living in urban spaces, the distribution of the urban population varies considerably by region: Northern America 82%, Latin America and the Caribbean 81%, Europe 74%, Oceania 68%, Asia 60% and Africa 43%. The urban population is continuously increasing; it is estimated that 66% of the population will live in urban areas by 2050, an increase of 16% compared to 2008 [1]. These data are very similar to those provided by the United Nations organization since in 2018 it estimated that 68% of the population will live in urban areas in 2050. This increase implies greater traffic congestion in cities due to both the increase in traffic and the unsuitable infrastructures [1].  For this reason, programs such as Horizonte Europa have analyzed global challenges such as climate, energy and mobility, and in particular, intelligent mobility through the optimization of infrastructures. Due to this increase in population and the need to improve infrastructure management, there is a demand to create systems capable of improving traffic efficiency, which will be applied in this project.Traditional operational research incorporates the use of queuing theory to make predictions about different parameters such as waiting times [2]. The queuing theory approach in which there are usually M/G/s models [3] where M refers to the arrival of vehicles which is represented by a poisson, G the service rate which in certain cases can be modeled by an exponential and finally, s represents the number of servers.  From these definitions, it is possible to determine parameters such as waiting times, which will be the object of study of this project. However, classical queuing theory would not take into account parameters that need to be considered, such as the time lost from the moment a traffic light turns green until the cars start moving. For these reasons, simulators such as SUMO [4] are currently being used ",
        "file_name": "ijimai.2023.12.001",
        "file_path": "PDFs\\ijimai.2023.12.001.pdf"
    },
    {
        "title": "Is Automated Consent in Solid GDPR-Compliant? An Approach for Obtaining Valid Consent with the Solid Protocol",
        "implementation_urls": [
            {
                "url": "https://github.com/besteves4/duo-odrl-dpv",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3390/info14120631",
        "arxiv": null,
        "abstract": "Abstract: Personal Information Management Systems (PIMS) are acquiring a prominent role in thedata economy by promoting services that help individuals to have more control over the processing oftheir personal data, in line with the European data protection laws. One of the highlighted solutionsin this area is Solid, a new protocol that is decentralizing the storage of data, through the usage ofinteroperable web standards and semantic vocabularies, to empower its users to have more controlover the processing of data by agents and applications. However, to fulfill this vision and gatherwidespread adoption, Solid needs to be aligned with the law governing the processing of personaldata in Europe, the main piece of legislation being the General Data Protection Regulation (GDPR).To assist with this process, we analyze the current efforts to introduce a policy layer in the Solidecosystem, in particular, related to the challenge of obtaining consent for processing personal data,focusing on the GDPR. Furthermore, we investigate if, in the context of using personal data forbiomedical research, consent can be expressed in advance, and discuss the conditions for validconsent and how it can be obtained in this decentralized setting, namely through the matching ofprivacy preferences, set by the user, with requests for data and whether this can signify informedconsent. Finally, we discuss the technical challenges of an implementation that caters to the previouslyidentified legal requirements.Keywords: personal information management systems; Solid; semantic web; data protection; consent1. IntroductionThe General Data Protection Regulation (GDPR) [1] has become the gold standard inthe European Union (EU) and its effects are being globally felt in Asia, Latin America andAfrica [2].The purpose of the GDPR is twofold: on the one hand, it protects individuals in whatconcerns their human rights and, on the other hand, it enables the free flow of personaldata (Article 1 GDPR). The EU expressed a vision that encompasses the creation of a singleEuropean market for data, where access to personal and non-personal data from acrossthe world is secure and can be used by an ecosystem of companies, governments, andindividuals to provide high-quality data-driven products and services for its citizens whileensuring that “EU law can be enforced effectively” and data subjects are still in control ofwhat happens to their personal data [3].In addition to the GDPR, novel data-related legislation with new data governanceschemes, such as the Data Governance Act (DGA) [4], is being brought forward by the EUto build an infrastructure for data-sharing and to improve citizens’ trust. In particular, trusthas been proven as an important factor that positively influences the perceived usefulnessInformation 2023, 14, 631. https://doi.org/10.3390/info14120631 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14120631https://doi.org/10.3390/info14120631https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0002-0319-8935https://orcid.org/0000-0003-0259-7560https://doi.org/10.3390/info14120631https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14120631?type=check_update&version=1Information 2023, 14, 631 2 of 33and ease of use of digital personal datastores [5] in data-handling services and allow themto share their sensitive data for the ‘public good’.However, this has not come without challenges in its interpretation and enforcement.",
        "file_name": "pdf?version=1700836284",
        "file_path": "PDFs\\pdfversion1700836284.pdf"
    },
    {
        "title": "The role of Semantic Web Technologies in Legal Terminology",
        "implementation_urls": [],
        "doi": "10.1075/hot.3.rol1",
        "arxiv": null,
        "abstract": "ABSTRACTLanguage resources are an essential component of any natural lan-guage processing system and such systems can only be applied tonew languages and domains if appropriate resources can be found.Currently the task of finding new language resources for a par-ticular task or application is complicated by the fact that recordsabout such resources are stored in different repositories with differ-ent models, different quality and search mechanisms. To remedythis situation, we present Linghub, a new portal that aggregates andindexes data from a range of sources and repositories and appliedthe Linked Data Principles to expose all the metadata under a com-mon interface. Furthermore, we use faceted browsing and SPARQLqueries to show how this can help to answer real user problems ex-tracted from a mailing list for linguists.KeywordsLinked Data, Language Resources, SPARQL, Faceted Browsing1. INTRODUCTIONLanguage resources are essential for nearly all tasks in natural lan-guage processing (NLP) and in particular for the adaptation of re-sources and methods to new domains and languages. In order touse language resources for new purposes they must first be discov-ered and this can only be done if there is a comprehensive list of allresources that may be available. To this there have been a numberof projects that have attempted to collect such a catalogue usingvarious methods and with differing degrees of data quality. Wepresent a new portal, Linghub, that aims to integrate all these datafrom different sources by means of linked data and thus to createa website, whereby all information about language resources canbe included and queried using a common methodology. The goalof Linghub is thus to enable wider discovery of language resourcesfor researchers in NLP, computational linguistics and linguistics.Currently, two approaches to metadata collection for language re-sources can be distinguished. Firstly, we distinguish a curatorialapproach to metadata collection in which a repository of languageresource metadata is maintained by a cross-institution organizationsuch as META-SHARE [7] or CLARIN project’s Virtual LanguageObservatory [17, VLO]. This approach is characterized though high-quality metadata that are entered by experts, at the expense of cov-erage. A collaborative approach, on the other hand, allows any-one to publish language resource metadata. Examples of this arethe LREMap [4] or Datahub1. A process for controlling the qual-ity of metadata entered is typically lacking for such collaborativerepositories, leading to less qualitative metadata and inhomoge-neous metadata resulting from free-text fields, user-provided tagsand the lack of controlled vocabularies.Given the nature of this difference we wish to make data availablefrom multiple sources in a homogeneous manner and we saw thedevelopment of a new linked data portal as the primary method toachieve this. To this end we adopted a model based on the DCATdata model [10] along with properties from Dublin Core [9]. In",
        "file_name": "paper27.pdf",
        "file_path": "PDFs\\paper27.pdf"
    },
    {
        "title": "A Review of Bias and Fairness in Artificial Intelligence",
        "implementation_urls": [],
        "doi": "10.9781/ijimai.2023.11.001",
        "arxiv": null,
        "abstract": "AbstractAutomating decision systems has led to hidden biases in the use of artificial intelligence (AI). Consequently, explaining these decisions and identifying responsibilities has become a challenge. As a result, a new field of research on algorithmic fairness has emerged. In this area, detecting biases and mitigating them is essential to ensure fair and discrimination-free decisions. This paper contributes with: (1) a categorization of biases and how these are associated with different phases of an AI model’s development (including the data-generation phase); (2) a revision of fairness metrics to audit the data and AI models trained with them (considering agnostic models when focusing on fairness); and, (3) a novel taxonomy of the procedures to mitigate biases in the different phases of an AI model’s development (pre-processing, training, and post-processing) with the addition of transversal actions that help to produce fairer models.DOI:  10.9781/ijimai.2023.11.001A Review of Bias and Fairness in Artificial IntelligenceRubén González-Sendino1, Emilio Serrano1, Javier Bajo1, Paulo Novais2 *1 Ontology Engineering Group, Departamento de Inteligencia Artificial, ETSI Informáticos, Universidad Politécnica de Madrid, 28660 Madrid (Spain)2 ALGORITMI Research Centre/LASI, University of Minho, Braga (Portugal)Received 16 September 2022 | Accepted 29 September 2023 | Early Access 10 November 2023 I. IntroductionThe evolution of artificial intelligence (AI) has allowed humans to be heavily supported in the decision-making process of some application domains [1]. The high degree of independence that AI is capable of exhibiting can be problematic [2], [3], especially when humans are not in the loop [4]–[6]. Automatization of decisions can come at the cost of amplifying bias and creating feedback loops [7], [8]. One of the main reasons AI can produce unfair results is due to the data with which it has been trained [9].Although the concept of bias is broad, this paper adheres to the following definition: “the systematic tendency in a model to favor one demographic group/individual over another, which can be mitigated but may well lead to unfairness” [9], [10]. Therefore, the next definition needed to understand the problem this paper studies is Fairness, which is defined as: “ the absence of prejudice or favoritism towards an individual or a group based on its inherent or acquired characteristics” [9].In the AI scope, incorrect predictions do not necessarily indicate that the model is unfair if its development was correct [11]. An unfair model is one whose decisions are biased toward a particular group of people. Moreover, biases cannot always be avoided. Thus, techniques must be used to mitigate their consequences, which aim to increase equality in the results. Data and models can be audited with fairness metrics, which are used to measure fairness between two groups or similar individuals. Furthermore, the categorization of methods for bias and unfairness mitigation depends on the phase of the AI model’s development in which they are used. These phases are typically pre-training, training, and post-training.This paper contributes with a systematic review of bias and fairness in artificial intelligence. The purpose of a systematic review is to provide a comprehensive summary of the literature available which is relevant to several research questions. The three questions addressed in this paper are: (1) What bias affects fairness?; (2) What are the metrics to measure fairness?; and, (3) How are biases mitigated? ",
        "file_name": "ijimai.2023.11.001",
        "file_path": "PDFs\\ijimai.2023.11.001.pdf"
    },
    {
        "title": "A Scoping Review on the Progress, Applicability, and Future of Explainable Artificial Intelligence in Medicine",
        "implementation_urls": [],
        "doi": "10.3390/app131910778",
        "arxiv": null,
        "abstract": "Abstract: Due to the success of artificial intelligence (AI) applications in the medical field overthe past decade, concerns about the explainability of these systems have increased. The reliabilityrequirements of black-box algorithms for making decisions affecting patients pose a challenge evenbeyond their accuracy. Recent advances in AI increasingly emphasize the necessity of integratingexplainability into these systems. While most traditional AI methods and expert systems are in-herently interpretable, the recent literature has focused primarily on explainability techniques formore complex models such as deep learning. This scoping review critically analyzes the existingliterature regarding the explainability and interpretability of AI methods within the clinical do-main. It offers a comprehensive overview of past and current research trends with the objective ofidentifying limitations that hinder the advancement of Explainable Artificial Intelligence (XAI) inthe field of medicine. Such constraints encompass the diverse requirements of key stakeholders,including clinicians, patients, and developers, as well as cognitive barriers to knowledge acquisition,the absence of standardised evaluation criteria, the potential for mistaking explanations for causalrelationships, and the apparent trade-off between model accuracy and interpretability. Furthermore,this review discusses possible research directions aimed at surmounting these challenges. Theseinclude alternative approaches to leveraging medical expertise to enhance interpretability withinclinical settings, such as data fusion techniques and interdisciplinary assessments throughout thedevelopment process, emphasizing the relevance of taking into account the needs of final users todesign trustable explainability methods.Keywords: artificial intelligence; medicine; explainable AI; interpretable AI1. Introduction1.1. AI in Medicine: Opportunities and ChallengesToday’s Artificial Intelligence (AI), with its capability to automate and ease almost anykind of task, frequently appearing to surpass human performance, has become a popularand widespread technology for many applications, especially over the last decade, thanksto advances in deep learning (DL), with clinical healthcare being no exception.Medicine has been one of the most challenging, but also most attention-getting appli-cation fields for AI for the past five decades, with diagnostic decision support, the interpre-tation of medical images and clinical lab tests, drug development, patient management,and others all demonstrating the broad and diverse scope of AI techniques applied tomedical issues.AI methods have promised a range of potential advantages for medical informaticssystems. Automating burdensome tasks can be of great help, alleviating clinicians fromunnecessary efforts and allowing them to focus on more important issues surroundingAppl. Sci. 2023, 13, 10778. https://doi.org/10.3390/app131910778 https://www.mdpi.com/journal/applscihttps://doi.org/10.3390/app131910778https://doi.org/10.3390/app131910778https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/applscihttps://www.mdpi.comhttps://orcid.org/0000-0002-1215-3333https://doi.org/10.3390/app131910778https://www.mdpi.com/journal/applscihttps://www.mdpi.com/article/10.3390/app131910778?type=check_update&version=1Appl. Sci. 2023, 13, 10778 2 of 23patient care. AI systems can perform these tasks with high precision and efficiency, and also,they can assist the extraction of relevant information from the large quantities of databeing produced by modern medicine [1]. AI systems might be particularly beneficial in",
        "file_name": "pdf?version=1695880594",
        "file_path": "PDFs\\pdfversion1695880594.pdf"
    },
    {
        "title": "Automatic thematic classification of documents based on vocabularies and use frequencies. The case of scientific dissemination articles",
        "implementation_urls": [],
        "doi": "10.3989/redc.2023.3.1996",
        "arxiv": null,
        "abstract": "Abstract: It is often necessary to classify documents by assigning them a theme or topic from a series of predefined options. This work is usually done manually, by reading the document by a specialist. This manual process is tedious, requires time and resources, and is prone to bias and preferences of each specialist.As an alternative, this article presents an automatic thematic classification system, capable of classifying hundreds of documents in a few seconds, highly parameterized, and that does not require the specialists intervention. The system is based on predefined thematic vocabularies and frequencies of use of lexical forms, and assigns one or more priority top-ics to each document. The suggested approach has been developed and tested in the context of scientific dissemination articles in the Spanish language.Using this approach, it is possible to systematically classify large amounts of documents by topic, using fewer resources than doing it manually, and avoiding unknown biases. The approach has shown to be as effective as other proposals, but requires less computational resources.Keywords: Document classification; thematic classification; algorithm; vocabularies; lexical frequencies; science dis-semination.Copyright: © 2023 CSIC. Este es un artículo de acceso abierto distribuido bajo los términos de la licencia de uso y distribución Creative Commons Reconocimiento 4.0 Internacional (CC BY 4.0).https://doi.org/10.3989/redc.2023.3.1996mailto:cesar.gonzalez-perez@incipit.csic.eshttps://orcid.org/0000-0002-3976-7589mailto:nacho.vidal@cchs.csic.eshttps://orcid.org/0000-0001-6169-784Xmailto:ana.garcia.g@cchs.csic.eshttps://orcid.org/0000-0002-5952-4971mailto:pcalleja%40fi.upm.es?subject=https://orcid.org/0000-0001-8423-8240https://doi.org/10.3989/redc.2023.3.1996César González-Pérez, José Ignacio Vidal Liy, Ana García García, Pablo Calleja Ibáñez2 Rev. Esp. Doc. Cient., 46(3), julio-septiembre 2023, e362. ISSN-L: 0210-0614. https://doi.org/10.3989/redc.2023.3.19961. INTRODUCCIÓNCualquier institución que trate con un gran nú-mero de documentos, sobre todo si son de pro-cedencia externa, debe clasificarlos temáticamente para su adecuada gestión. Este es el caso de bi-bliotecas o archivos, por ejemplo. La clasificación temática consiste en asignar uno o más temas a cada documento, de un repertorio de temas que puede ser fijo o bien cambiante. En cualquier caso, esta clasificación suele realizarse de forma manual mediante un proceso de análisis de contenido, le-yendo el documento o un resumen de este, si exis-te, por parte de un especialista humano, y asig-nando después uno o más temas. Para decidir qué temas se asignan a un documento, el especialista hace uso de su conocimiento tácito y experiencia, y, a veces, también de criterios previamente espe-cificados. Sea como sea, este proceso es tedioso, requiere mucho tiempo y recursos humanos, y es propenso a los sesgos y preferencias de cada es-pecialista.Hoy en día, en un momento en el que muchos de los documentos que manejamos existen en for-",
        "file_name": "2292",
        "file_path": "PDFs\\2292.pdf"
    },
    {
        "title": "“Who Should I Trust with My Data?” Ethical and Legal Challenges for Innovation in New Decentralized Data Management Technologies",
        "implementation_urls": [],
        "doi": "10.3390/info14070351",
        "arxiv": null,
        "abstract": "Abstract: News about personal data breaches or data abusive practices, such as Cambridge Analytica,has questioned the trustworthiness of certain actors in the control of personal data. Innovations inthe field of personal information management systems to address this issue have regained traction inrecent years, also coinciding with the emergence of new decentralized technologies. However, onlywith ethically and legally responsible developments will the mistakes of the past be avoided. Thiscontribution explores how current data management schemes are insufficient to adequately safeguarddata subjects, and in particular, it focuses on making these data flows transparent to provide anadequate level of accountability. To showcase this, and with the goal of enhancing transparency tofoster trust, this paper investigates solutions for standardizing machine-readable policies to expresspersonal data processing activities and their application to decentralized personal data stores as anexample of ethical, legal, and technical responsible innovation in this field.Keywords: data governance; digital age; transparency; personal data management; identity management1. IntroductionData-driven innovations are expected to deliver further economic and societal develop-ment [1]. Through the analysis, sharing, and (re-)use of data, business models and govern-ments’ processes have been transformed to benefit from those practices [2]. The emergenceof a data-driven society is being fostered by policy actions from different governments on aworldwide scale. The European Union (EU) is no exception to this, as the European Com-mission has put on its agenda the development of “A Europe fit for the Digital Age”. The Eu-ropean Commission’s strategy and related policy documents can be located at the followinglink: https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_en(accessed on 26 May 2023). Regardless of whether it is a Big Tech company based in theUnited States (US), a large data broker in the EU, or a Chinese government-controlledentity, current data practices have been questioned by different societal sectors, from in-dividuals to nongovernmental organizations (NGOs) or from academics to governments.Trust in many digital services has been compromised [3], which has left individuals askingthemselves “who should I trust with my data”.In response to this trust crisis, technology has been looked upon to provide answers.Applied to the field of (personal) data, self-sovereign identity models [4] — as improve-ments over existing Personal Information Management Systems (PIMS) — have been putunder the spotlight due to their potential, but they are also taken with “a grain of salt”, asInformation 2023, 14, 351. https://doi.org/10.3390/info14070351 https://www.mdpi.com/journal/informationhttps://doi.org/10.3390/info14070351https://doi.org/10.3390/info14070351https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/informationhttps://www.mdpi.comhttps://orcid.org/0000-0003-2344-4061https://orcid.org/0000-0002-6820-999Xhttps://orcid.org/0000-0003-0259-7560https://orcid.org/0000-0002-3503-4644https://ec.europa.eu/info/strategy/priorities-2019-2024/europe-fit-digital-age_enhttps://doi.org/10.3390/info14070351https://www.mdpi.com/journal/informationhttps://www.mdpi.com/article/10.3390/info14070351?type=check_update&version=1Information 2023, 14, 351 2 of 17they are not free from shortcomings [5]. Through them, users would be in direct control oftheir information and decide when, how, and who can access such information. Certain",
        "file_name": "pdf?version=1687339409",
        "file_path": "PDFs\\pdfversion1687339409.pdf"
    },
    {
        "title": "MuHeQA: Zero-shot Question Answering over Multiple and Heterogeneous Knowledge Bases",
        "implementation_urls": [
            {
                "url": "https://github.com/librairy/MuHeQA",
                "url_type": "git",
                "frequency": 6,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/egerber/spaCy-entity-linker",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/castorini/SimpleDBpediaQA",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/askplatypus/wikidata-simplequestions",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3233/sw-233379",
        "arxiv": null,
        "abstract": "Abstract. There are two main limitations in most of the existing Knowledge Graph Question Answering (KGQA) algorithms.First, the approaches depend heavily on the structure and cannot be easily adapted to other KGs. Second, the availability andamount of additional domain-specific data in structured or unstructured formats has also proven to be critical in many of thesesystems. Such dependencies limit the applicability of KGQA systems and make their adoption difficult. A novel algorithm isproposed, MuHeQA, that alleviates both limitations by retrieving the answer from textual content automatically generated fromKGs instead of queries over them. This new approach (1) works on one or several KGs simultaneously, (2) does not requiretraining data what makes it is domain-independent, (3) enables the combination of knowledge graphs with unstructured infor-mation sources to build the answer, and (4) reduces the dependency on the underlying schema since it does not navigate throughstructured content but only reads property values. MuHeQA extracts answers from textual summaries created by combininginformation related to the question from multiple knowledge bases, be them structured or not. Experiments over Wikidata andDBpedia show that our approach achieves comparable performance to other approaches in single-fact questions while beingdomain and KG independent. Results raise important questions for future work about how the textual content that can be createdfrom knowledge graphs enables answer extraction.Keywords: Question answering, natural language processing, knowledge graphs1. IntroductionKnowledge graphs are now being applied in multiple domains. Knowledge Graph Question Answering (KGQA)has emerged as a way to provide an intuitive mechanism for non-expert users to query knowledge graphs. KGQAsystems do not require specific technical knowledge (e.g., knowledge of SPARQL or Cypher), providing answers innatural language for questions that are also expressed in natural language.One of the main challenges in the design of KGQA systems is semantic parsing [14]. In this step, natural languagequeries (NLQs) are translated into a specific query language (e.g. SPARQL1 for RDF-based KGs, or Cypher2 for*Corresponding author. E-mail: carlos.badenes@upm.es.1https://www.w3.org/TR/rdf-sparql-query2https://opencypher.org1570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:carlos.badenes@upm.eshttps://orcid.org/0000-0002-2753-9917mailto:oscar.corcho@upm.eshttps://orcid.org/0000-0002-9260-0753mailto:carlos.badenes@upm.eshttps://www.w3.org/TR/rdf-sparql-queryhttps://opencypher.orghttps://creativecommons.org/licenses/by/4.0/CORRECTED  PROOF2 C. Badenes-Olmedo and O. Corcho / MuHeQAproperty graphs). KGQA systems typically use templates with placeholders for relations and entities that mustbe identified in the original NLQ. Once the template is filled in, the generated query (in SPARQL, Cypher, etc.)is evaluated in the system that stores the KG (e.g. Triple store, property graph DB) and the results will providethe answers to the question. Thus, KGQA systems can be reduced to entity and relationship search processes thatpopulate predefined query templates that are commonly identified using supervised machine learning techniques(e.g., supervised classifiers) [23]. This approach is heavily dependent on the data schema to explore the entityrelationships and on the availability of training data to create supervised classification models.Mitigating the dependency of KGQA systems on the underlying graph structure and eliminating the need fortraining sets is crucial to create cross-cutting solutions more efficiently and with less cost. The first research questionaddressed in this work is: “How to extract the answer from a knowledge graph without translating the naturallanguage question into a formal query language?”. Moreover, the dependence on the underlying data schema makesit also difficult for existing KGQA systems to combine knowledge from several graphs to extract a single answer.They typically translate the question into specific queries for each supported KG and obtain multiple answers, rather",
        "file_name": "sw233379?id=semantic-web%2Fsw233379",
        "file_path": "PDFs\\sw233379idsemantic-web%2Fsw233379.pdf"
    },
    {
        "title": "Satellite Earth Observation for Essential Climate Variables Supporting Sustainable Development Goals: A Review on Applications",
        "implementation_urls": [
            {
                "url": "https://github.com/grumets/eneon-graph",
                "url_type": "git",
                "frequency": 6,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3390/rs15112716",
        "arxiv": null,
        "abstract": "Abstract: Essential climate variables (ECVs) have been recognized as crucial information for achievingSustainable Development Goals (SDGs). There is an agreement on 54 ECVs to understand climateevolution, and multiple rely on satellite Earth observation (abbreviated as s-ECVs). Despite the effortsto encourage s-ECV use for SDGs, there is still a need to further integrate them into the indicatorcalculations. Therefore, we conducted a systematic literature review to identify s-ECVs used inSDG monitoring. Results showed the use of 14 s-ECVs, the most frequent being land cover, ozone,precursors for aerosols and ozone, precipitation, land surface temperature, soil moisture, soil carbon,lakes, and leaf area index. They were related to 16 SDGs (mainly SDGs 3, 6, 11, 14, and 15), 33 targets,and 23 indicators. However, only 10 indicators (belonging to SDGs 6, 11, and 15) were calculatedusing s-ECVs. This review raises research opportunities by identifying s-ECVs yet to be used in theindicator calculations. Therefore, indicators supporting SDGs must be updated to use this valuablesource of information which, in turn, allows a worldwide indicator comparison. Additionally, thisreview is relevant for scientists and policymakers for future actions and policies to better integrates-ECVs into the Agenda 2030.Keywords: SDG; sustainable development; satellite; Earth observation; review; essential variables;climate1. IntroductionThe Agenda 2030 for Sustainable Development and its 17 goals (SDGs) are connectedwith the environment, economy, and society dimensions of sustainable development [1].The 17 goals, their 169 associated targets, and 231 indicators are based on the first data-driven policy development framework, following the principle of “If you don’t measureit, you can’t manage it” [2] (p. 2). Despite the recognized importance of measuring theprogress towards the SDGs, two-thirds of the indicators remain unreported, especially inlow-income countries [3]. Moreover, less than 44% of the SDG indicators can be easilymeasured [4]. Therefore, it is a priority to boost the measuring and monitoring of theprogress towards the SDGs. In our work, we focus on two key approaches to pursue thisRemote Sens. 2023, 15, 2716. https://doi.org/10.3390/rs15112716 https://www.mdpi.com/journal/remotesensinghttps://doi.org/10.3390/rs15112716https://creativecommons.org/https://creativecommons.org/licenses/by/4.0/https://creativecommons.org/licenses/by/4.0/https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.comhttps://orcid.org/0000-0002-6926-4827https://orcid.org/0000-0001-5799-469Xhttps://orcid.org/0000-0001-7380-0701https://orcid.org/0000-0001-5145-9223https://orcid.org/0000-0001-6559-2033https://doi.org/10.3390/rs15112716https://www.mdpi.com/journal/remotesensinghttps://www.mdpi.com/article/10.3390/rs15112716?type=check_update&version=1Remote Sens. 2023, 15, 2716 2 of 25aim: (1) essential variables (EVs) that have been defined as an intermediate layer betweenobservations and indicators [5] and (2) satellite Earth observation (sEO) data that gainedparticular attention as worldwide feasible, cost-effective, and analysis-ready data acrossscales in remote, non-accessible, and poorly monitored regions [6].The EVs emerged in various social and environmental scientific communities relatedto specific domains such as climate, biodiversity, agriculture, and society [5,7–9]. Referto [10,11] for detailed EV compendiums. These kinds of variables are “a minimal setof variables that determine the system’s state and developments, [which] are crucial for",
        "file_name": "pdf?version=1684894645",
        "file_path": "PDFs\\pdfversion1684894645.pdf"
    },
    {
        "title": "Gender imbalance in doctoral education: an analysis of the Spanish university system (1977–2021)",
        "implementation_urls": [],
        "doi": "10.1007/s11192-023-04648-y",
        "arxiv": null,
        "abstract": "AbstractDoctoral education is a key feature of university systems, as well as a basic foundation of scientific practice. That period culminates in a dissertation and examination of the candi‑date that has been studied from several points of view. This paper reports the results of an analysis on the evolution and characteristics of gender imbalance of a complete doctoral system for a wide period of time. Data from the database Teseo was used in order to iden‑tify the individuals involved in the process, the scientific fields in which the dissertations where classified, and the institutions in which the examination took place. Results: the Spanish system shows a clear evolution towards gender balance, but also some concern‑ing trends that are worth tracking. Seemingly, STEM disciplines look to be evolving more slowly than other branches of science in several aspects. A leaky pipeline is characterized in this system around the roles of supervisors, candidates, members and chairs of the dis‑sertation committees. Gender assortativity is also studied and described, and its possible effects discussed around the academic relations that surround doctoral examination.Keywords Gender imbalance · STEM · Dissertations · Teseo · Leaky pipeline · Gender assortativity * Rodrigo Sánchez‑Jiménez  rodsanch@ucm.es1 Library and Information Science Department, SCImago Group, Universidad Complutense de Madrid, Madrid, Spain2 Library and Information Science Department (Internet Medialab Research Group), Universidad Complutense de Madrid, Madrid, Spain3 Sales Engineering EMEA, Neo4j, London, UK4 Ontology Engineering Group (Artificial Intelligence Department), Universidad Politécnica de Madrid, Madrid, Spain5 Library and Information Science Department, Universidad Complutense de Madrid, Madrid, Spainhttp://orcid.org/0000-0002-3685-7060http://crossmark.crossref.org/dialog/?doi=10.1007/s11192-023-04648-y&domain=pdf2578 Scientometrics (2023) 128:2577–25991 3IntroductionGender imbalance and gender bias in science have been studied and described for a long time. Zuckerman and Cole (1975) already described this issue in quantitative terms and advanced the “principle of triple penalty” (cultural inappropriateness/perceived incompe‑tence/direct discrimination). Shiebinger (1987) went over the very abundant literature on the history of women in science and described how at least the number of female scientists was growing faster (low numbers having been associated with that cultural inappropriate‑ness) but the perception of a lesser competence by women (women were systematically employed in less prestigious jobs in the academia) and blatant discrimination (unjustified salary gaps were huge) was ever persistent. Etzkowitz, et al (1992) moved on to put the focus on the de‑genderization of science and society, and on the existence of “different gender styles of scientific work”, an idea that has been one way or another behind many studies comparing output, collaboration, and impact of men and women. Bordons et  al. (2003) acknowledge this factor as a warning to interpret their SCI‑based results but take it a step further. They also explained the cumulative advantage of achieving high ranks in academia over productivity, which in turn accounts for the gender differences in pro‑ductivity. Several years later, Lariviere et al., (2011) reached a somewhat different conclu‑sion, finding again that gender differences were present in terms of production and funding, although the nature of these differences was complex. The subject is therefore very much open to debate, and the focus on its study has varied significantly over time (Tomassini, ",
        "file_name": "s11192-023-04648-y.pdf",
        "file_path": "PDFs\\s11192-023-04648-y.pdf"
    },
    {
        "title": "Applying the LOT Methodology to a Public Bus Transport Ontology aligned with Transmodel: Challenges and Results",
        "implementation_urls": [
            {
                "url": "https://github.com/CiudadesAbiertas/vocab-transporte-autobus",
                "url_type": "git",
                "frequency": 12,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/oeg-upm/chowlk_spec",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3233/SW-210451",
        "arxiv": null,
        "abstract": "Abstract. We present an ontology that describes the domain of Public Transport by bus, which is common in cities around theworld. This ontology is aligned to Transmodel, a reference model which is available as a UML specification and which was devel-oped to foster interoperability of data about transport systems across Europe. The alignment with this non-ontological resourcerequired the adaptation of the Linked Open Terms (LOT) methodology, which has been used by our team as the methodologicalframework for the development of many ontologies used for the publication of open city data. The ontology is structured intothree main modules: (1) agencies, operators and the lines that they manage, (2) lines, routes, stops and journey patterns, and (3)planned vehicle journeys with their timetables and service calendars. Besides reusing Transmodel concepts, the ontology alsoreuses common ontology design patterns from GeoSPARQL and the SOSA ontology. As part of the LOT data-driven valida-tion stage, RDF data has been generated taking as input the GTFS feeds (General Transit Feed Specification) provided by theMadrid public bus transport provider (EMT). Mapping rules from structured data sources to RDF were developed using the RDFMapping Language (RML) to generate RDF data, and queries corresponding to competency questions were tested.Keywords: Ontology, Transmodel, public bus, Open Cities, RDF1. IntroductionOpen data initiatives across public administrations worldwide date back to more than a decade ago. In the specificcase of Spanish cities, the most relevant landmarks are associated to the first transposition of the EU Public SectorInformation directive in 2007,1 the publication of the UNE 178301:2015 technical norm on Open Data for SmartCities,2 and the development of the open data guide by the Spanish Federation of Municipalities and Provinces(FEMP) in 2017 [8] and the catalogue of high-value open datasets for cities in 2019 [9].*Corresponding author. E-mail: eruckhaus@fi.upm.es.1https://eur-lex.europa.eu/eli/dir/2003/98, https://www.boe.es/eli/es/l/2007/11/16/37/con.2https://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N00543181570-0844 © 2023 – The authors. Published by IOS Press. This is an Open Access article distributed under the terms of theCreative Commons Attribution License (CC BY 4.0).mailto:eruckhaus@fi.upm.esmailto:adolfo.anton.bravo@upm.esmailto:ocorcho@fi.upm.esmailto:mario.scrocca@cefriel.commailto:eruckhaus@fi.upm.eshttps://eur-lex.europa.eu/eli/dir/2003/98https://www.boe.es/eli/es/l/2007/11/16/37/conhttps://www.en.aenor.com/normas-y-libros/buscador-de-normas/une?c=N0054318https://creativecommons.org/licenses/by/4.0/640 E. Ruckhaus et al. / Applying the LOT Methodology to a Public Bus Transport Ontology aligned with TransmodelDomains that have been addressed in these initiatives include public sector, demography, environment, economy,commerce, transport and treasury, among others. As part of the initiatives and projects that have led the advance-ment of open data among cities in Spain we can cite the Ciudades Abiertas3 project, a public-private collaborativeproject led by four Spanish municipalities (Zaragoza, Madrid, Santiago de Compostela and A Coruña) with thegeneral aim to facilitate the implementation of common Open Government policies that are reusable by many othermunicipalities inside and outside Spain.Among the project actions on open data, several (12) ontologies are being developed using the Linked OpenTerms (LOT) methodology [5,17]. These ontologies allow publishing Open Data homogeneously across cities,using common CSU structures, as well as following Linked Data principles [29]. They are being added to thosethat had been already developed in the context of the Spanish network of Open Data for Smart Cities,4 and theycorrespond to a subset of the catalogue of datasets included in the aforementioned FEMP open data guide [9]. Allof the ontologies are publicly available and versioned in GitHub,5 with the corresponding repositories including usecases and user stories, requirements, the ontology implementation in OWL, the ontology HTML documentation inSpanish and English, and example data and queries.In the area of transport, three ontologies have been developed so far under the umbrella of these initiatives,focused on the representation of open data about Public Bicycles, Motor Vehicle Traffic and Public Bus Transport.In this paper, we will discuss the latter, an ontology that has been specifically developed for structuring how to",
        "file_name": "sw210451?id=semantic-web%2Fsw210451",
        "file_path": "PDFs\\sw210451idsemantic-web%2Fsw210451.pdf"
    },
    {
        "title": "Editorial of transport data on the web",
        "implementation_urls": [
            {
                "url": "https://github.com/NABSA/gbfs",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3233/SW-223278",
        "arxiv": null,
        "abstract": null,
        "file_name": "sw223278?id=semantic-web%2Fsw223278",
        "file_path": "PDFs\\sw223278idsemantic-web%2Fsw223278.pdf"
    },
    {
        "title": "Data Management Documentation in Citizen Science Projects: Bringing Formalisation and Transparency Together",
        "implementation_urls": [
            {
                "url": "https://doi.org/10.5281/zenodo.5101358",
                "url_type": "zenodo",
                "frequency": 4,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.6491235",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.5334/cstp.538",
        "arxiv": null,
        "abstract": "ABSTRACTCitizen science (CS) is a way to open up the scientific process, to make it more accessible and inclusive, and to bring professional scientists and the public together in shared endeavours to advance knowledge. Many initiatives engage citizens in the collection or curation of data, but do not state what happens with such data. Making data open is increasingly common and compulsory in professional science. To conduct transparent, open science with citizens, citizens need to be able to understand what happens with the data they contribute. Data management documentation (DMD) can increase understanding of and trust in citizen science data, improve data quality and accessibility, and increase the reproducibility of experiments. However, such documentation is often designed for specialists rather than amateurs.This paper analyses the use of DMD in CS projects. We present analysis of a qualitative survey and assessment of projects’ DMD, and four vignettes of data management practices. Since most projects in our sample did not have DMD, we further analyse their reasons for not doing so. We discuss the benefits and challenges of different forms of DMD, and barriers to having it, which include a lack of resources, a lack of awareness of tools to support DMD development, and the inaccessibility of existing tools to citizen scientists without formal scientific education. We conclude that, to maximise the inclusivity of citizen science, tools and templates need to be made more accessible for non-experts in data management.mailto:gefion.thuermer@kcl.ac.ukhttps://doi.org/10.5334/cstp.538https://orcid.org/0000-0001-7345-0000https://orcid.org/0000-0003-4112-6825https://orcid.org/0000-0002-1044-3943https://orcid.org/0000-0002-9260-0753https://orcid.org/0000-0003-1722-947X2Thuermer et al. Citizen Science: Theory and Practice DOI: 10.5334/cstp.538Citizen science projects can help advance scientific knowledge, and educate participants about specific topics and the scientific process in general (Bonney et al. 2009). These projects occur at different scales, from local, such as the iSPEX project (http://ispex-eu.org), where citizen scientists use sensors to measure air quality (Volten et al. 2018), to international, such as eBird (https://ebird.org), an online platform used globally to record bird observations (Lagoze 2014). Citizens may create such projects from the bottom up, with or without the support of professional scientists; conduct data collection or analysis in scientist-led projects (Wiggins and Crowston 2011); or contribute to scientific publications (Tinati et al. 2015).The implementation of data management policies can make data and projects more scientifically sound, improve data quality and accessibility, and increase reproducibility. In CS projects, data management is an essential activity that enables citizen scientists to produce data that can be relevant and useful for, and trusted by, researchers (Hunter, Alabri and Ingen 2013). However, in many projects, data management policies or documentation are not systematically applied, leading to the perception that ",
        "file_name": "647dce1d15fad.pdf",
        "file_path": "PDFs\\647dce1d15fad.pdf"
    },
    {
        "title": "Comparison of Knowledge Graph Representations for Consumer Scenarios",
        "implementation_urls": [
            {
                "url": "https://github.com/oeg-upm/kg-scenarios-eval",
                "url_type": "git",
                "frequency": 6,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/usc-isi-i2/kgtk-browser",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/blazegraph/database",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.8179156",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7443836",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.1007/978-3-031-47240-4_15",
        "arxiv": null,
        "abstract": "Abstract. Knowledge graphs have been widely adopted across organi-zations and research domains, fueling applications that span interactivebrowsing to large-scale analysis and data science. One design decisionin knowledge graph deployment is choosing a representation that opti-mally supports the application’s consumers. Currently, however, there isno consensus on which representations best support each consumer sce-nario. In this work, we analyze the fitness of popular knowledge graphrepresentations for three consumer scenarios: knowledge exploration, sys-tematic querying, and graph completion. We compare the accessibilityfor knowledge exploration through a user study with dedicated brows-ing interfaces and query endpoints. We assess systematic querying withSPARQL in terms of time and query complexity on both synthetic andreal-world datasets. We measure the impact of various representations onthe popular graph completion task by training graph embedding modelsper representation. We experiment with four representations: StandardReification, N-Ary Relationships, Wikidata qualifiers, and RDF-star. Wefind that Qualifiers and RDF-star are better suited to support use casesof knowledge exploration and systematic querying, while Standard Reifi-cation models perform most consistently for embedding model inferencetasks but may become cumbersome for users. With this study, we aimto provide novel insights into the relevance of the representation choiceand its impact on common knowledge graph consumption scenarios.Keywords: Knowledge Graphs · Knowledge Representation · UserStudy · Graph Completion1 IntroductionThe growth of the knowledge graph (KG) user base has triggered the emergenceof new representational requirements. While RDF is the traditional and standardmodel for KG representation, alternative models such as property graphs [25], theWikidata model [34], and RDF-star [12] have also become recently popular. Thec© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14265, pp. 271–289, 2023.https://doi.org/10.1007/978-3-031-47240-4 15http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47240-4_15&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-6606-9735http://orcid.org/0000-0002-1735-0686http://orcid.org/0000-0001-6921-1744http://orcid.org/0000-0002-9260-0753https://doi.org/10.1007/978-3-031-47240-4_15272 A. Iglesias-Molina et al.promise of these alternative and complementary representation models is thatthey can provide more flexibility to address certain use cases, such as statementannotation, for which RDF-based representations are not straightforward [17].While the plurality of knowledge representation (KR) models provides the meansto address a wider range of possibilities in consumer scenarios, there is currentlyno consensus nor sufficient empirical evidence on which representations are mostsuitable for different KG consumer tasks [16].Previous studies comparing knowledge representations have focused primar-ily on query performance [2,6,14,26,28] and graph interoperability [3,4]. Forthis scenario, the representations need to ensure efficiency to minimize perfor-",
        "file_name": "978-3-031-47240-4_15.pdf",
        "file_path": "PDFs\\978-3-031-47240-4_15.pdf"
    },
    {
        "title": "The RML Ontology: A Community-Driven Modular Redesign After a Decade of Experience in Mapping Heterogeneous Data to RDF",
        "implementation_urls": [
            {
                "url": "https://github.com/kg-construct/rml-core",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/SDM-TIB/SDM-RDFizer-Star",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/SDM-TIB/Dragoman",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/RMLio/rmlmapper-java",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/Orange-OpenSource/SMASSIF-RML",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/OP-TED/ePO",
                "url_type": "git",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://github.com/kg-construct/",
                "url_type": "git",
                "frequency": 1,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7919856",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7919852",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7919850",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7919848",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7919845",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7918478",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7907172",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7898764",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.7749949",
                "url_type": "zenodo",
                "frequency": 2,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "http://doi.org/10.5281/zenodo.7918478",
                "url_type": "zenodo",
                "frequency": 1,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.1007/978-3-031-47243-5_9",
        "arxiv": null,
        "abstract": "Abstract. The Relational to RDF Mapping Language (R2RML)became a W3C Recommendation a decade ago. Despite its wide adop-tion, its potential applicability beyond relational databases was swiftlyexplored. As a result, several extensions and new mapping languages wereproposed to tackle the limitations that surfaced as R2RML was appliedin real-world use cases. Over the years, one of these languages, the RDFMapping Language (RML), has gathered a large community of contribu-tors, users, and compliant tools. So far, there has been no well-defined setof features for the mapping language, nor was there a consensus-markingontology. Consequently, it has become challenging for non-experts tofully comprehend and utilize the full range of the language’s capabilities.After three years of work, the W3C Community Group on KnowledgeGraph Construction proposes a new specification for RML. This paperpresents the new modular RML ontology and the accompanying SHACLshapes that complement the specification. We discuss the motivationsand challenges that emerged when extending R2RML, the methodologywe followed to design the new ontology while ensuring its backward com-patibility with R2RML, and the novel features which increase its expres-siveness. The new ontology consolidates the potential of RML, empowerspractitioners to define mapping rules for constructing RDF graphs thatwere previously unattainable, and allows developers to implement sys-tems in adherence with [R2]RML.c© The Author(s) 2023T. R. Payne et al. (Eds.): ISWC 2023, LNCS 14266, pp. 152–175, 2023.https://doi.org/10.1007/978-3-031-47243-5_9http://crossmark.crossref.org/dialog/?doi=10.1007/978-3-031-47243-5_9&domain=pdfhttp://orcid.org/0000-0001-5375-8024http://orcid.org/0000-0002-7195-9935http://orcid.org/0000-0002-3029-6469http://orcid.org/0000-0003-0248-0987http://orcid.org/0000-0003-4734-3847http://orcid.org/0000-0003-1702-8707http://orcid.org/0000-0001-9064-0463http://orcid.org/0000-0003-3236-2789http://orcid.org/0000-0003-2138-7972https://doi.org/10.1007/978-3-031-47243-5_9The RML Ontology: A Community-Driven Modular Redesign 153Resource type: Ontology/License: CC BY 4.0 InternationalDOI: 10.5281/zenodo.7918478/URL: http://w3id.org/rml/portal/Keywords: Declarative Language · R2RML · RML · KnowledgeGraph1 IntroductionIn 2012, the Relational to RDF Mapping Language (R2RML) [37] was releasedas a W3C Recommendation. The R2RML ontology [8] provides a vocabularyto describe how an RDF graph should be generated from data in a relationaldatabase (RDB). Although R2RML gained wide adoption, its potential applica-bility beyond RDBs quickly appeared as a salient need [49,63,76,87].Targeting the generation of RDF from heterogeneous data sources other thanRDBs, several extensions [49,76,87] preserving R2RML’s core structure wereproposed. As R2RML and the growing number of extensions were applied in a",
        "file_name": "978-3-031-47243-5_9.pdf",
        "file_path": "PDFs\\978-3-031-47243-5_9.pdf"
    },
    {
        "title": "Enhancing drug repurposing on graphs by integrating drug molecular structure as feature",
        "implementation_urls": [
            {
                "url": "https://github.com/pckroon/pysmiles",
                "url_type": "git",
                "frequency": 1,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.1109/CBMS58004.2023.00043",
        "arxiv": null,
        "abstract": "Abstract— Drug repurposing has become increasingly important, particularly in light of the COVID-19 pandemic. This process involves identifying new therapeutic uses for existing drugs, which can significantly reduce the cost, risk, and time associated with developing new drugs, de novo development. A previous conducted study proved that Deep Learning can be used to streamline this process by identifying drug repurposing hypotheses. The study presented a model called REDIRECTION, which utilized the rich biomedical information available in graph form and combined it with Geometric Deep Learning to find new indications for existing drugs. The reported metrics for this model were 0.87 for AUROC and 0.83 for AUPRC. In this current study, the importance of node features in GNNs is explored. Specifically, the study used GNNs to embed two-dimensional drug molecular structures and obtain corresponding features. These features were incorporated into the drug repurposing graph, along with some other enhancements, resulting in an improved model called DMSR. Performance score for the reported metrics values raised by 0.0448 in AUROC and 0.0919 in AUPRC. Based on these findings, we believe that the method used for embedding drug molecular structures is interesting and captures valuable information about drugs. Its incorporation in the graph for drug repurposing can significantly benefit the process, leading to improved performance evaluation metrics. Keywords—Drug repurposing; Graph deep learning; Drug molecular structure; Graph Neural Networks; Graph Autoencoder; DISNET knowledge base  I. INTRODUCTION  In our prior work, Ayuso et al. (2022) [1], it was proved that incorporating diverse biomedical data into a multi-layered graph is a helpful method for gaining a deeper understanding of diseases and their associated components and characteristics. Using Graph Neural Networks (GNNs) and DISNET biomedical graph [2] a model called REDIRECTION was presented. DISNET is a biomedical integrated knowledge base containing information regarding diseases and their associations to symptoms and drugs, among others. REDIRECTION is based on graph deep learning [3], [4] and link  prediction. Its aim is to generate drug repurposing hypotheses. Drug repurposing or repositioning involves identifying new therapeutic uses for drugs that have already been approved. The model performed well, scoring 0.87 for the area under the receiving operating characteristics curve (AUROC) and 0.83 for the area under the precision-recall curve (AUPR) for a selected subset of already-tested repurposing hypotheses, RepoDB test. GNNs benefit enormously from the addition of node features, due to the foundational role these features play in the message passing framework upon which GNNs are built [5], [6]. This is an intuitive outcome, as the incorporation of relevant ",
        "file_name": "2023.05.03.539227.full.pdf",
        "file_path": "PDFs\\2023.05.03.539227.full.pdf"
    },
    {
        "title": "Event Extraction and Semantic Representation from Spanish Workers Statute Using Large Language Models",
        "implementation_urls": [
            {
                "url": "https://doi.org/10.5281/zenodo.8147616",
                "url_type": "zenodo",
                "frequency": 1,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            },
            {
                "url": "https://doi.org/10.5281/zenodo.8143596",
                "url_type": "zenodo",
                "frequency": 1,
                "extraction_method": [
                    "regex"
                ],
                "source_paragraphs": []
            }
        ],
        "doi": "10.3233/FAIA230983",
        "arxiv": null,
        "abstract": "Abstract. This work uses Large Language Models to process an important piece ofSpanish legislation: the Workers’ Statute. The proposed method extracts the rele-vant events in its articles using a GPT-3.5 model and represents the entities involvedin the events and the relationships between them as RDF triples. The experimentscarried out to select a high-performance strategy include both zero- and few-shotlearning tests. Finally, this work proposes a strategy to uplift the extracted legalrelations into a legal knowledge graph.Keywords. Spanish Workers’ Statute, Large Language Models, Knowledge Graph,Legal Domain, Event Extraction1. IntroductionThe legal domain is a complex and dynamic field that involves interpreting and applyinglaws and regulations. Legal data (court cases, legislations, contracts, etc.) is becominga valuable source to push forward intelligent legal tools [1]. This work proposes an ap-proach using event extraction and semantic graph modeling to bring these systems closerto the public. The event extraction task is being tackled in the state-of-the-art using deeplearning models. However, it presents numerous challenges, especially for Spanish texts,including ambiguity, polysemy, and domain-specific terminology [2].The recent development of Large Language Models (LLMs) [3, 4] has proven tobe an excellent approach to mitigate these problems and an important tool to deal withlimited data [5, 6] through natural language instructions, called prompts.This research aims to improve the performance of the event extraction task withinthe legal domain and to link the information into a semantic graph representation. Thedata to be used will be the Spanish Workers’ Statute, given its importance for legislatorsand the general public, and the availability of an annotated corpus of 133 sentences fromthe Statute gathered by Revenko and Martı́n-Chozas [7]. The low amount of tagged datawill be tackled using the GPT-3.5 model, as it has been proven the high performanceon Natural Language Processing (NLP) tasks like event extraction [3, 5]. Finally, theextracted events will be represented in a knowledge graph.Legal Knowledge and Information SystemsG. Sileno et al. (Eds.)© 2023 The Authors.This article is published online with Open Access by IOS Press and distributed under the termsof the Creative Commons Attribution Non-Commercial License 4.0 (CC BY-NC 4.0).doi:10.3233/FAIA2309833292. Related WorkThis work defines an event as a textual region likely to compact relevant legal informationencapsulated by the articles on the law. The most common event structure is formed by anevent mention, an event trigger, an event argument, and an argument role. The argumentrole is the relationship between an argument and the event it participates in. The basicargument roles are subject, object, and complement. To classify the legal relations, manyworks [7, 8] use the Hohfeldian classes Right, Duty, No-Right, and Privilege [9].State-of-the-art event extraction relies nowadays on deep learning models like graphneural networks (GAN) [10] and attention mechanisms [11]. However, these models relyon huge amounts of labeled data to improve the model’s performance and are mainly usedfor English corpora. This research uses only 133 annotated sentences from the SpanishWorkers’ Statute [7], which are insufficient to achieve high-performance models.To tackle this issue, common approaches use data augmentation techniques [5],transfer learning [7], and active learning [12]. In recent years, language models (LM)have also been used for this and other NLP tasks [6, 3]. In 2021, the work [13] presented",
        "file_name": "FAIA230983",
        "file_path": "PDFs\\FAIA230983.pdf"
    }
]