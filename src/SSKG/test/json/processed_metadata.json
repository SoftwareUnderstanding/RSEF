{
    "0": {
        "abstract": "Abstract—This paper conceives a novel sparse code multipleaccess (SCMA) codebook design which is motivated by thestrong need for providing ultra-low decoding complexity andgood error performance in downlink Internet-of-things (IoT)networks, in which a massive number of low-end and low-costIoT communication devices are served. By focusing on the typicalRician fading channels, we analyze the pair-wise probability ofsuperimposed SCMA codewords and then deduce the designmetrics for multi-dimensional constellation construction andsparse codebook optimization. For significant reduction of thedecoding complexity, we advocate the key idea of projectingthe multi-dimensional constellation elements to a few overlappedcomplex numbers in each dimension, called low projection (LP).An emerging modulation scheme, called golden angle modulation(GAM), is considered for multi-stage LP optimization, wherethe resultant multi-dimensional constellation is called LP-GAM.Our analysis and simulation results show the superiority of theproposed LP codebooks (LPCBs) including one-shot decodingconvergence and excellent error rate performance. In particular,the proposed LPCBs lead to decoding complexity reduction by atleast 97% compared to that of the conventional codebooks, whilstowning large minimum Euclidean distance. Some examples ofthe proposed LPCBs are available at https://github.com/ethanlq/SCMA-codebook.Index Terms—Sparse code multiple access (SCMA), goldenangle modulation (GAM), codebook design, Internet-of-things(IoT), low complexity detection, Rician channels.I. INTRODUCTIONTHE Internet-of-things (IoT) represents a revolutionaryparadigm shift from the legacy human-centric networks(e.g., in 3G and 4G) to massive machine-type communica-tions, where the latter is a major use case in the 5G-and-beyond mobile networks [1]. Under this big picture, however,it is challenging to support the concurrent communicationsof massive IoT devices. Due to the limited time-frequencyresources, traditional orthogonal multiple access (OMA) maybe infeasible. A disruptive technique for addressing such achallenge is called non-orthogonal multiple access (NOMA)which permits several times of IoT devices larger than that inOMA systems communicating simultaneously [2].Qu Luo, Gaojie Chen, Pei Xiao and Yi Ma are with 5G & 6G InnovationCentre, Institute for Communication Systems (ICS), University of Surrey, UK,email:{q.u.luo, gaojie.chen, p.xiao, m.yi}@surrey.ac.uk.Zilong Liu is with the School of Computer Science and Electronic Engi-neering, University of Essex, UK, email: zilong.liu@essex.ac.uk.Amine Maaref is with the Canada Research Center, Huawei TechnologiesCompany Ltd., Ottawa, Canada, email: amine.maaref@huawei.com.Existing NOMA techniques can be mainly categorizedinto two classes: power-domain NOMA [3] and code-domainNOMA (CD-NOMA) [4]. The former works by superimposingmultiple users with distinctive power levels over the identical",
        "arxiv": null,
        "doi": 0,
        "file_name": "",
        "file_path": "2208.03118v2.pdf",
        "title": "Visualizing Data using t-SNE",
        "urls": [
            [
                "https://github.com/ethanlq/SCMA-codebook",
                4
            ]
        ]
    },
    "1": {
        "abstract": "could be abstracted to other cryptocurrencies or outside of a cryptocurrency setting.• Ether: high risk asset whose USD market prices 𝑝𝐸𝑡 are exogenous• DStablecoin: a ‘stable’ asset collateralized in Ether whose USD price 𝑝𝐷𝑡 is endogenousNotably, a large DStablecoin system may have endogenous amplification effects on Ether price,similarly to how CDOs affected underlying assets in the 2008 financial crisis. We discuss this furtherin Section 7 but leave formal modeling of this to future work.There are several barriers for trading between crypto and fiat, which motivate our choice of assets.Most crypto-fiat pairs are through Bitcoin or Ether, which act as a gateway to other cryptoassets.Trading to fiat can involve moving assets between a number of exchanges and can take considerabletime to confirm on the blockchain. Trading to a stablecoin is comparatively simple. Trading to fiatcan also trigger more clear tax incidence. Additionally, some countries have imposed strict capitalcontrols on trading between fiat and crypto.Model outline. At 𝑡 = 0, the agents have endowments and prior beliefs. In each period 𝑡 :(1) New Ether price is revealed(2) Ether expectations are updated(3) Stablecoin holder decides portfolio weights(4) Speculator, seeing demand, decides leverage(5) DStablecoin market is cleared2.1 Stablecoin holderThe stablecoin holder starts with an initial endowment and decides portfolio weights to attain thedesired stability. The following table defines the agent’s state variables.Variable Definition𝑛𝑡 Ether held at time 𝑡�̄�𝑡 DStablecoin held at time 𝑡wt Portfolio weights chosen at time 𝑡The stablecoin holder weights its portfolio by wt. We denote the components as𝑤𝐸𝑡 and𝑤𝐷𝑡 forEther and DStablecoin weights respectively. The stablecoin holder’s portfolio value at time 𝑡 isA𝑡 = 𝑛𝑡𝑝𝐸𝑡 + �̄�𝑡𝑝𝐷𝑡 = 𝑛𝑡−1𝑝𝐸𝑡 + �̄�𝑡−1𝑝𝐷𝑡 .Given weights, 𝑛𝑡 and �̄�𝑡 will be determined based on the stablecoin clearing price 𝑝𝐷𝑡 .Ariah Klages-Mundt and Andreea Minca 7The basic results in Section 3 hold generally for any wt ≥ 0 (i.e., there is no shorting). In thiscase, wt could be chosen, e.g., from Sharpe ratio optimization, mean-variance optimization, orKelly criterion (among others). In Sections 4 & 5, in order to focus on the effects of speculatordecisions, we simplify the stablecoin holder as exogenous with unit price-elastic demand. In thiscase, DStablecoin demand is constant in dollar terms.2.2 SpeculatorThe speculator starts with an endowment of Ether and initial beliefs about Ether’s returns andvariance and decides leverage to maximize expected returns subject to protocol and self-imposedconstraints. The following tables define variables and parameters for the speculator.Variable Definition",
        "arxiv": null,
        "doi": 1,
        "file_name": "",
        "file_path": "1906.02152v3.pdf",
        "title": "(In)Stability for the Blockchain: Deleveraging Spirals and Stablecoin Attacks",
        "urls": [
            [
                "https://github.com/runtimeverification/verified-smart-contracts",
                3
            ],
            [
                "https://github.com/makerdao/awesome-makerdao",
                2
            ],
            [
                "https://github.com/aklamun/Stablecoin_Deleveraging",
                2
            ]
        ]
    },
    "10.1007/978-3-030-01240-3_10": {
        "abstract": "Abstract. Template-matching methods for visual tracking have gainedpopularity recently due to their comparable performance and fast speed.However, they lack effective ways to adapt to changes in the target ob-ject’s appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adaptthe template to the target’s appearance variations during tracking. AnLSTM is used as a memory controller, where the input is the searchfeature map and the outputs are the control signals for the reading andwriting process of the memory block. As the location of the target is atfirst unknown in the search feature map, an attention mechanism is ap-plied to concentrate the LSTM input on the potential target. To preventaggressive model adaptivity, we apply gated residual template learningto control the amount of retrieved memory that is used to combine withthe initial template. Unlike tracking-by-detection methods where the ob-ject’s information is maintained by the weight parameters of neural net-works, which requires expensive online fine-tuning to be adaptable, ourtracker runs completely feed-forward and adapts to the target’s appear-ance changes by updating the external memory. Moreover, unlike othertracking methods where the model capacity is fixed after offline training –the capacity of our tracker can be easily enlarged as the memory require-ments of a task increase, which is favorable for memorizing long-term ob-ject information. Extensive experiments on OTB and VOT demonstratesthat our tracker MemTrack performs favorably against state-of-the-arttracking methods while retaining real-time speed of 50 fps. 1Keywords: Addressable Memory, Gated Residual Template Learning1 IntroductionAlong with the success of convolution neural networks in object recognitionand detection, an increasing number of trackers [4, 13, 22, 26, 31] have adopteddeep learning models for visual object tracking. Among them are two dominanttracking strategies. One is the tracking-by-detection scheme that online trainsan object appearance classifier [22, 26] to distinguish the target from the back-ground. The model is first learned using the initial frame, and then fine-tunedusing the training samples generated in the subsequent frames based on the1 Code is available at https://github.com/skyoung/MemTrackarXiv:1803.07268v2  [cs.CV]  2",
        "arxiv": "1803.07268",
        "doi": "10.1007/978-3-030-01240-3_10",
        "file_name": "",
        "file_path": "1803.07268v2.pdf",
        "title": "Learning Dynamic Memory Networks for Object Tracking",
        "urls": [
            [
                "https://github.com/skyoung/MemTrack",
                2
            ]
        ]
    },
    "10.1007/978-3-030-16350-1_1": {
        "abstract": "Abstract. We will discuss the RowHammer problem in DRAM, whichis a prime (and likely the first) example of how a circuit-level failuremechanism in Dynamic Random Access Memory (DRAM) can causea practical and widespread system security vulnerability. RowHammeris the phenomenon that repeatedly accessing a row in a modern DRAMchip predictably causes errors in physically-adjacent rows. It is caused bya hardware failure mechanism called read disturb errors. Building on ourinitial fundamental work that appeared at ISCA 2014, Google ProjectZero demonstrated that this hardware phenomenon can be exploited byuser-level programs to gain kernel privileges. Many other recent worksdemonstrated other attacks exploiting RowHammer, including remotetakeover of a server vulnerable to RowHammer. We will analyze theroot causes of the problem and examine solution directions. We will alsodiscuss what other problems may be lurking in DRAM and other typesof memories, e.g., NAND flash and Phase Change Memory, which canpotentially threaten the foundations of reliable and secure systems, asthe memory technologies scale to higher densities.1 SummaryAs memory scales down to smaller technology nodes, new failure mechanismsemerge that threaten its correct operation [79, 80]. If such failures are not antici-pated and corrected, they can not only degrade system reliability and availabilitybut also, even more importantly, open up new security vulnerabilities: a mali-cious attacker can exploit the exposed failure mechanism to take over an entiresystem. As such, new failure mechanisms in memory can become practical andsignificant threats to system security.In this keynote talk, based on our ISCA 2014 paper [55], we introduce theRowHammer problem in DRAM, which is a prime (and likely the first) exampleof a real circuit-level failure mechanism that causes a practical and widespreadsystem security vulnerability. RowHammer, as it is now popularly referred to,is the phenomenon that repeatedly accessing a row in a modern DRAM chipcauses bit flips in physically-adjacent rows at consistently predictable bit loca-tions. It is caused by a hardware failure mechanism called DRAM disturbanceerrors, which is a manifestation of circuit-level cell-to-cell interference in a scaledmemory technology. Specifically, when a DRAM row is opened (i.e., activated)and closed (i.e., precharged) repeatedly (i.e., hammered), enough times withinhttp://arxiv.org/abs/1903.11056v12 Onur Mutlua DRAM refresh interval, one or more bits in physically-adjacent DRAM rowscan be flipped to the wrong value. Using an FPGA-based DRAM testing infras-tructure [70, 42], we tested 129 DRAM modules manufactured by three majormanufacturers in seven recent years (2008–2014) and found that 110 of themexhibited RowHammer errors, the earliest of which dates back to 2010. OurISCA 2014 paper [55] provides a detailed and rigorous analysis of various char-acteristics of RowHammer, including its data pattern dependence, repeatabilityof errors, relationship with leaky cells, and various circuit-level causes of thephenomenon.We demonstrate that a very simple user-level program [55, 3] can reliablyand consistently induce RowHammer errors in commodity AMD and Intel sys-tems using vulnerable DRAM modules. We released the source code of thisprogram [3], which Google Project Zero later enhanced [4]. Using our user-level",
        "arxiv": null,
        "doi": "10.1007/978-3-030-16350-1_1",
        "file_name": "",
        "file_path": "1903.11056v1.pdf",
        "title": "RowHammer and Beyond",
        "urls": [
            [
                "https://github.com/tadfisher/x210-bios",
                1
            ],
            [
                "https://github.com/google/rowhammer-test",
                1
            ],
            [
                "https://github.com/CMU-SAFARI/rowhammer",
                1
            ]
        ]
    },
    "10.1007/978-3-319-26123-2_24": {
        "abstract": "AbstractSolving logistic regression with L1-regularization in distributed settings is an im-portant problem. This problem arises when training dataset is very large and can-not fit the memory of a single machine. We present d-GLMNET, a new algorithmsolving logistic regression with L1-regularization in the distributed settings. Weempirically show that it is superior over distributed online learning via truncatedgradient.1 IntroductionLogistic regression with L1-regularization is the method of choice for solving classification and classprobability estimation problems in text mining, biometrics and clickstream data analysis. Despite thefact that logistic regression can build only linear separating surfaces, the performance (i.e., testingaccuracy) of it, with proper regularization, has shown to be close to that of nonlinear classifiers suchas kernel methods. At the same time training and testing of linear classifiers is much faster. It makesthe logistic regression a good choice for large-scale problems. A desirable trait of model is sparsity,which is conveniently achieved with L1 or elastic net regularizer.A broad survey [15] suggests that coordinate descent methods are the best choice for L1-regularizedlogistic regression on the large scale. Widely used algorithms that fall into this family are: BBR [6],GLMNET [5], newGLMNET [16]. Software implementations of these methods start with loadingthe full training dataset into RAM.Completely different approach is online learning [2, 8, 10, 11]. This kind of algorithms do notrequire to load training dataset into RAM and can access it sequentially (i.e. reading from disk).Balakrishnan and Madigan [2], Langford et al. [8] report that online learning performs well whencompared to batch counterparts (BBR and LASSO).Nowadays we see the growing number of problems where both the number of examples and thenumber of features are very large. Many problems grow beyond the capabilities of a single computerand need to be handled by distributed systems. Approaches to distributed training of classifiersnaturally fall into two groups by the way they split data across computing nodes: by examples [1]or by features [12]. We believe that algorithms that split data by features can achieve better sparsitywhile retaining similar or better performance and competitive training speed with those that split byexamples. Our experiments so far confirm that belief.Parallel block-coordinate descent is a natural algorithmic framework if we choose to split by fea-tures. The challenge here is how to combine steps from coordinate blocks, or computing nodes, andhow to organize communication. When features are independent, parallel updates can be combinedstraightforwardly, otherwise they may come into conflict and not yield enough improvement to ob-jective; this has been clearly illustrated by Bradley et al. [3]. Bradley et al. [3] proposed Shotgunalgorithm based on randomized coordinate descent. They studied how many variables can be up-1arXiv:1411.6520v1  [stat.M",
        "arxiv": "1411.6520",
        "doi": "10.1007/978-3-319-26123-2_24",
        "file_name": "",
        "file_path": "1411.6520v1.pdf",
        "title": "Distributed Coordinate Descent for L1-regularized Logistic Regression",
        "urls": [
            [
                "https://github.com/IlyaTrofimov/dlr",
                1
            ]
        ]
    },
    "10.1007/bf02391668": {
        "abstract": "ABSTRACTAims. In this paper, we aim to characterise the surface magnetic fields of a sample of eight T Tauri stars from high-resolution near-infrared spectroscopy. Some stars in our sample are known to be magnetic from previous spectroscopic or spectropolarimetric studies.Our goals are firstly to apply Zeeman broadening modelling to T Tauri stars with high-resolution data, secondly to expand the sampleof stars with measured surface magnetic field strengths, thirdly to investigate possible rotational or long-term magnetic variability bycomparing spectral time series of given targets, and fourthly to compare the magnetic field modulus 〈B〉 tracing small-scale magneticfields to those of large-scale magnetic fields derived by Stokes V Zeeman Doppler Imaging (ZDI) studies.Methods. We modelled the Zeeman broadening of magnetically sensitive spectral lines in the near-infrared K-band from high-resolution spectra by using magnetic spectrum synthesis based on realistic model atmospheres and by using different descriptionsof the surface magnetic field. We developped a Bayesian framework that selects the complexity of the magnetic field prescriptionbased on the information contained in the data.Results. We obtain individual magnetic field measurements for each star in our sample using four different models. We find that theBayesian Model 4 performs best in the range of magnetic fields measured on the sample (from 1.5 kG to 4.4 kG). We do not detecta strong rotational variation of 〈B〉 with a mean peak-to-peak variation of 0.3 kG. Our confidence intervals are of the same order ofmagnitude, which suggests that the Zeeman broadening is produced by a small-scale magnetic field homogeneously distributed overstellar surfaces. A comparison of our results with mean large-scale magnetic field measurements from Stokes V ZDI show differentfractions of mean field strength being recovered, from 25–42% for relatively simple poloidal axisymmetric field topologies to 2–11%for more complex fields.Key words. stars: pre-main sequence – stars: magnetic field – line: profiles1. IntroductionWith an ever improving understanding of stellar magnetism,we now realise that magnetic fields have a paramount impactthroughout the entire life of a star. Particularly on the pre main-sequence (PMS), where stars are relatively cool and magneticfields seem ubiquitous, magnetism has a significant impact onstars themselves, their formation, accretion properties, rotationrate, flares, and wind characteristics among others. It also in-fluences potential orbiting exoplanets, especially their chemicalevolution and habitability.The origin of stellar magnetic fields and their evolutionalong the PMS, from stars that evolve from a fully-convectiveregime to a partially radiative and then eventually to a fullyradiative regime, is not fully understood. While cool PMS TTauri stars exhibit ubiquitous magnetic fields, it seems that moststars, that evolve into PMS Herbig Ae/Be stars and eventuallyto A/B type main-sequence stars, lose their fields at some point(Alecian et al. 2013; Sikora et al. 2019). The 5–10% of stars re-maining magnetic on the main sequence display simple, fossilfields that are not maintained by an active dynamo. Studying theevolution of magnetic fields in the T Tauri star regime in moredetail can allow us to understand when and how fast this transi-tion occurs, and perhaps identify different populations with dis-tinct magnetic properties. This could improve our understandingof the mechanisms and timescales at play regarding the evolu-tion and survival of the magnetic fields in the later evolutionarystages (Alecian et al. 2019).There are two direct methods widely used to measure stel-lar magnetic fields that both rely on the Zeeman effect. Thefirst is the measurement of Zeeman broadening, or splitting ofspectral lines in intensity spectra, and the second is the anal-",
        "arxiv": null,
        "doi": "10.1007/bf02391668",
        "file_name": "",
        "file_path": "1909.04965v1.pdf",
        "title": "On the identification of Ar x and Ar xiv in the solar corona and the origin of the unidentified coronal lines",
        "urls": [
            [
                "https://github.com/pkgw/mcmc-reporting",
                2
            ],
            [
                "https://github.com/astro-alexis/magnotron-tts",
                2
            ]
        ]
    },
    "10.1016/j.techfore.2022.121478": {
        "abstract": null,
        "arxiv": "2107.03200",
        "doi": "10.1016/j.techfore.2022.121478",
        "file_name": "",
        "file_path": "2107.03200v2.pdf",
        "title": "The Geography of Open Source Software: Evidence from GitHub",
        "urls": [
            [
                "https://github.com/n1tecki/Geography-of-Open-Source-Software",
                2
            ],
            [
                "https://github.com/johanneswachs/OSS_Geography_Data",
                2
            ],
            [
                "https://github.com/Hipo/university-domains-list",
                2
            ],
            [
                "https://github.com/johanneswachs/OSS_",
                1
            ]
        ]
    },
    "10.1051/0004-6361/202039603": {
        "abstract": "ABSTRACTGalaxy clustering is a standard cosmological probe that is commonly analysed through two-point statistics. In observations, the es-timation of the two-point correlation function crucially relies on counting pairs in a random catalogue. The latter contains a largenumber of randomly distributed points, which accounts for the survey window function. Random pair counts can also be advanta-geously used for modelling the window function in the observed power spectrum. Since pair counting scales as O(N2), where N isthe number of points, the computational time to measure random pair counts can be very expensive for large surveys. In this work,we present an alternative approach for estimating those counts that does not rely on the use of a random catalogue. We derived ananalytical expression for the anisotropic random-random pair counts that accounts for the galaxy radial distance distribution, surveygeometry, and possible galaxy weights. We show that a prerequisite is the estimation of the two-point correlation function of theangular selection function, which can be obtained efficiently using pixelated angular maps. Considering the cases of the VIPERS andSDSS-BOSS redshift surveys, we find that the analytical calculation is in excellent agreement with the pair counts obtained fromrandom catalogues. The main advantage of this approach is that the primary calculation only takes a few minutes on a single CPUand it does not depend on the number of random points. Furthermore, it allows for an accuracy on the monopole equivalent to whatwe would otherwise obtain when using a random catalogue with about 1500 times more points than in the data at hand. We alsodescribe and test an approximate expression for data-random pair counts that is less accurate than for random-random counts, but stillprovides subpercent accuracy on the monopole. The presented formalism should be very useful in accounting for the window functionin next-generation surveys, which will necessitate accurate two-point window function estimates over huge observed cosmologicalvolumes.Key words. Cosmology: miscellaneous – large-scale structure of Universe – Methods: numerical – Methods: statistical1. IntroductionThe spatial distribution of galaxies has a long history of provid-ing cosmological parameter constraints (e.g. Strauss et al. 1992;Vogeley et al. 1992; Maddox et al. 1996; Peacock et al. 2001;Cole et al. 2005; Tegmark et al. 2006; Percival et al. 2010; Blakeet al. 2012; de la Torre et al. 2013; Alam et al. 2017; eBOSSCollaboration et al. 2020, and references therein). This arisesfrom the fact that the statistical properties of galaxies, partic-ularly spatial ones, can be predicted by cosmological models.When analysing galaxy clustering, we usually compress the in-formation by using summary statistics, the most natural one be-ing the two-point correlation function or its Fourier counterpartin the power spectrum. This is due to the nearly Gaussian na-ture of primordial matter perturbations, which are almost fullydescribed by their two-point statistics. Although gravitationalevolution leads to non-Gaussianity, and in turn, non-vanishinghigher-order n-point statistics, two-point statistics continues tobe very informative.Despite the cosmological principle that implies that the cor-relation function is isotropic, meaning that it is only a function ofthe norm of the separation vector, because of the way the line-of-sight distance is measured in redshift surveys and the presenceof peculiar velocities, the observed correlation function becomesanisotropic. These velocities are induced on large scales by thecoherent convergence of matter towards overdensities as part ofthe general process of structure growth. This anisotropy makesobserved galaxy n-point statistics sensitive to the strength ofgravity acting on the large-scale structure (Kaiser 1987; Guzzoet al. 2008).Formally, the two-point correlation function is the excessprobability of finding a pair of objects at a given distance, with",
        "arxiv": "2010.02793",
        "doi": "10.1051/0004-6361/202039603",
        "file_name": "",
        "file_path": "2010.02793v2.pdf",
        "title": "Fast analytical calculation of the random pair counts for realistic survey geometry",
        "urls": [
            [
                "http://github.com/mianbreton/RR_code",
                2
            ]
        ]
    },
    "10.1063/5.0015950": {
        "abstract": "Abstract  The ability to understand and engineer molecular structures relies on having accurate descriptions of the energy as a function of atomic coordinates. Here we outline a new paradigm for deriving energy functions of hyperdimensional molecular systems, which involves generating data for low-dimensional systems in virtual reality (VR) to then efficiently train atomic neural networks (ANNs). This generates high-quality data for specific areas of interest within the hyperdimensional space that characterizes a molecule’s potential energy surface (PES). We demonstrate the utility of this approach by gathering data within VR to train ANNs on chemical reactions involving fewer than 8 heavy atoms. This strategy enables us to predict the energies of much higher-dimensional systems, e.g. containing nearly 100 atoms. Training on datasets containing only 15k geometries, this approach generates mean absolute errors around 2 kcal mol-1. This represents one of the first times that an ANN-PES for a large reactive radical has been generated using such a small dataset. Our results suggest VR enables the intelligent curation of high-quality data, which accelerates the learning process.              mailto:*glowacki@bristol.ac.uk1. Introduction  In the recent past, computations were mostly limited by the available processing power.  With the machine learning revolution, the issues related to generating and curating data have become equally as important as the algorithms used to process and learn the data1.  The molecular sciences have seen a surge in popularity of machine learning methods for a variety of applications, from designing new drug molecules2, 3 to planning synthetic chemistry strategies4. Multiple research groups have been applying machine learning to the prediction of molecular energies and forces5-8 , with the goal of accelerating molecular dynamics (MD) simulations. For small systems, ab initio calculations can be used to evaluate accurate energies and forces at each step of an MD simulation. However, this becomes too computationally expensive for larger systems and more approximate methods, such as force fields, are generally used. While much faster, these incur a trade-off in accuracy.  An alternative to force fields is to use accurate data, e.g. from electronic structure calculations, to fit potential energy surfaces (PES). Evaluating fitted PES should be faster than performing electronic structure calculations but should provide similar accuracy to the underlying data. A variety of machine learning techniques have been used for either fitting or interpolating PES, for example permutationally invariant fitting9, cubic splines10, modified ",
        "arxiv": "2007.02824",
        "doi": "10.1063/5.0015950",
        "file_name": "",
        "file_path": "2007.02824v1.pdf",
        "title": "Training atomic neural networks using fragment-based data generated in virtual reality",
        "urls": [
            [
                "https://github.com/SilviaAmAm/squalane_paper_si",
                4
            ]
        ]
    },
    "10.1093/gigascience/giab055": {
        "abstract": "AbstractMachine learning brings the hope of finding new biomarkersextracted from cohorts with rich biomedical measurements.A good biomarker is one that gives reliable detection ofthe corresponding condition. However, biomarkers are of-ten extracted from a cohort that differs from the targetpopulation. Such a mismatch, known as a dataset shift,can undermine the application of the biomarker to newindividuals. Dataset shifts are frequent in biomedical re-search, e.g. because of recruitment biases. When a datasetshift occurs, standard machine-learning techniques do notsuffice to extract and validate biomarkers. This articleprovides an overview of when and how dataset shifts breaksmachine-learning extracted biomarkers, as well as detectionand correction strategies.1 Introduction: dataset shiftbreaks learned biomarkersBiomarkers are measurements that provide informationabout a medical condition or physiological state [Strimbuand Tavel, 2010]. For example, the presence of an anti-body may indicate an infection; a complex combination offeatures extracted from a medical image can help assessthe evolution of a tumor. Biomarkers are important fordiagnosis, prognosis, and treatment or risk assessments.Complex biomedical measures may carry precious medi-cal information, as with histopathological images or genomesequencing of biopsy samples in oncology. Identifying quan-titative biomarkers from these requires sophisticated sta-tistical analysis. With large datasets becoming accessi-ble, supervised machine learning provides new promisesby optimizing the information extracted to relate to a spe-cific output variable of interest, such as a cancer diagnosis[Andreu-Perez et al., 2015, Faust et al., 2018, Deo, 2015].These methods, cornerstones of artificial intelligence, arestarting to appear in clinical practice: a machine-learningbased radiological tool for breast-cancer diagnosis has re-cently been approved by the FDA1.Can such predictive biomarkers, extracted through com-plex data processing, be safely used in clinical practice,beyond the initial research settings? One risk is the poten-tial mismatch, or dataset shift, between the distribution ofthe individuals used to estimate this statistical link andthat of the target population that should benefit from thebiomarker. In this case, the extracted associations may notapply to the target population [Kakarmath et al., 2020].Computer aided diagnostic of thoracic diseases from X-rayimages has indeed been shown to be unreliable for individ-uals of a given sex if built from a cohort over-representingthe other sex [Larrazabal et al., 2020]. More generally,machine-learning systems may fail on data from different",
        "arxiv": "2107.09947",
        "doi": "10.1093/gigascience/giab055",
        "file_name": "",
        "file_path": "2107.09947v1.pdf",
        "title": "Preventing dataset shift from breaking machine-learning biomarkers",
        "urls": [
            [
                "https://github.com/neurodatascience/dataset_shift_biomarkers",
                2
            ]
        ]
    },
    "10.1109/cig.2019.8848075": {
        "abstract": "Abstract—In this paper, we propose Rogue-Gym, a simple andclassic style roguelike game built for evaluating generalization inreinforcement learning (RL). Combined with the recent progressof deep neural networks, RL has successfully trained human-level agents without human knowledge in many games such asthose for Atari 2600. However, it has been pointed out that agentstrained with RL methods often overfit the training environment,and they work poorly in slightly different environments. To inves-tigate this problem, some research environments with proceduralcontent generation have been proposed. Following these studies,we propose the use of roguelikes as a benchmark for evaluatingthe generalization ability of RL agents. In our Rogue-Gym,agents need to explore dungeons that are structured differentlyeach time they start a new game. Thanks to the very diversestructures of the dungeons, we believe that the generalizationbenchmark of Rogue-Gym is sufficiently fair. In our experiments,we evaluate a standard reinforcement learning method, PPO,with and without enhancements for generalization. The resultsshow that some enhancements believed to be effective fail tomitigate the overfitting in Rogue-Gym, although others slightlyimprove the generalization ability.Index Terms—roguelike games, reinforcement learning, gener-alization, domain adaptation, neural networksI. INTRODUCTIONReinforcement learning (RL) is a key method for trainingAI agents without human knowledge. Recent advances in deepreinforcement learning have created human-level agents inmany games, such as those for Atari 2600 [1] and the gameDOOM [2], using only pixels as inputs. This method could beapplied to many domains, from robotics to the game industry.However, it is still difficult to generalize learned policiesbetween tasks even for current state of the art RL algorithms.Recent studies (e.g., by Zhang et al. [3] and by Cobbe etal. [4]) have shown that agents trained by RL methods oftenoverfit the training environment and perform poorly in a testenvironment, when the test environment is not exactly the sameas the training environment. This is an important problembecause test environments are often differ somewhat fromtraining environments in many applications of reinforcementlearning. For example, in real world applications includingself-driving cars [5], agents are often trained via simulatorsA part of this work was supported by JSPS KAKENHI Grant Number18K19832 and by JST, PRESTO.or designated areas but need to perform safely in real worldsituations that are similar to but different from their trainingenvironments. For agents to act appropriately in unknownsituations, they need to properly generalize their policies thatthey learned from the training environment. Generalization isalso important in transfer learning, where the goal is to transfera policy learned in a training environment to another similar",
        "arxiv": "1904.08129",
        "doi": "10.1109/cig.2019.8848075",
        "file_name": "",
        "file_path": "1904.08129v2.pdf",
        "title": "Rogue-Gym: A New Challenge for Generalization in Reinforcement Learning",
        "urls": [
            [
                "https://github.com/kngwyu/rogue-gym-agents-cog19",
                2
            ],
            [
                "https://github.com/kngwyu/rogue-gym",
                2
            ]
        ]
    },
    "10.1109/cvpr52688.2022.00822": {
        "abstract": "AbstractObject encoding and identification are vital for robotictasks such as autonomous exploration, semantic scene un-derstanding, and re-localization. Previous approaches haveattempted to either track objects or generate descriptorsfor object identification. However, such systems are lim-ited to a “fixed” partial object representation from a singleviewpoint. In a robot exploration setup, there is a require-ment for a temporally “evolving” global object represen-tation built as the robot observes the object from multipleviewpoints. Furthermore, given the vast distribution of un-known novel objects in the real world, the object identifi-cation process must be class-agnostic. In this context, wepropose a novel temporal 3D object encoding approach,dubbed AirObject, to obtain global keypoint graph-basedembeddings of objects. Specifically, the global 3D objectembeddings are generated using a temporal convolutionalnetwork across structural information of multiple framesobtained from a graph attention-based encoding method.We demonstrate that AirObject achieves the state-of-the-artperformance for video object identification and is robust tosevere occlusion, perceptual aliasing, viewpoint shift, de-formation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors. To thebest of our knowledge, AirObject is one of the first tempo-ral object encoding methods. Source code is available athttps://github.com/Nik-V9/AirObject.1. IntroductionObject encoding and identification are crucial for robotictasks such as autonomous exploration, semantic scene un-derstanding, and loop closure in simultaneous localizationand mapping (SLAM). For example, object-based seman-tic SLAM and identification of revisited objects require ro-bust and efficient object encodings [41, 45, 46]. Prior ap-proaches proposed in the literature have attempted to trackobject detections [47], use keypoint features [10], and gen-erate graph-based embeddings for object matching [50].However, such systems are limited to a “fixed” object rep-✓✓✓Figure 1. Temporally evolving topological graph representationsof objects within a video sequence. We propose a method, AirOb-ject, to match these temporally evolving representations and al-leviate problems caused by perceptually-aliased occluded singleframe representations.resentation from a single viewpoint and are not robust tosevere occlusion, viewpoint shift, perceptual aliasing, orscale transform. These single frame representations tend tolead to false correspondences amongst perceptually-aliased",
        "arxiv": "2111.15150",
        "doi": "10.1109/cvpr52688.2022.00822",
        "file_name": "",
        "file_path": "2111.15150v2.pdf",
        "title": "AirObject: A Temporally Evolving Graph Embedding for Object Identification",
        "urls": [
            [
                "https://github.com/Nik-V9/AirObject",
                2
            ]
        ]
    },
    "10.1109/cvprw.2018.00132": {
        "abstract": "AbstractThis work identifies and addresses two important techni-cal challenges in single-image super-resolution: (1) how toupsample an image without magnifying noise and (2) how topreserve large scale structure when upsampling. We sum-marize the techniques we developed for our second placeentry in Track 1 (Bicubic Downsampling), seventh place en-try in Track 2 (Realistic Adverse Conditions), and seventhplace entry in Track 3 (Realistic difficult) in the 2018 NTIRESuper-Resolution Challenge. Furthermore, we present newneural network architectures that specifically address thetwo challenges listed above: denoising and preservation oflarge-scale structure.1. IntroductionSuper-resolution (SR) is a classic problem in image pro-cessing where the goal is to generate a high resolution im-age from one or more low resolution images. Applica-tions of super-resolution are wide-ranging. For instance,SR is important for allowing modern high-definition dis-plays to function properly when showing video recorded atlower resolutions. SR also has many applications in med-ical imaging, such as reducing noise in images stemmingfrom uncontrollable patient motions [11]. This work fo-cuses on single image super-resolution, which is useful forphotographic enhancement, license plate recognition, satel-lite imaging, and other remote sensing applications such asrecognition of a military target [16].Deep learning techniques can learn a mapping directlyfrom low resolution to high resolution images, where allfeature construction is automated. This makes some typesof complex preprocessing much easier than previous ap-proaches, for example, we no longer need to explicitlychoose a dictionary of low-level features (e.g., edge detec-tors) to convolve with the image. The fact that trainingdeep neural networks has become much easier within the∗All authors contributed equally. Thanks to other members of DukeData Science Teampast few years has led to more reliable automated training.On the other hand, the fact that these deep learning meth-ods use recursive mathematical formulas that are now muchmore complicated than before makes it more difficult to de-termine how to best troubleshoot them to achieve higher-quality performance.In this work we discuss several insights into the prob-lem of single-image super-resolution – many of which haveled to higher quality performance beyond entries from lastyear’s NTIRE single-image SR competition. These insightsconcern the amplification of noise when upsampling andthe preservation of large scale structure in enhanced im-ages. We introduce neural network architectures for both",
        "arxiv": "1805.03383",
        "doi": "10.1109/cvprw.2018.00132",
        "file_name": "",
        "file_path": "1805.03383v2.pdf",
        "title": "New Techniques for Preserving Global Structure and Denoising with Low Information Loss in Single-Image Super-Resolution",
        "urls": [
            [
                "https://github.com/websterbei/EDSR_tensorflow",
                2
            ],
            [
                "https://github.com/nikhilvravi/DukeSR",
                2
            ]
        ]
    },
    "10.1109/iccv48922.2021.00150": {
        "abstract": "AbstractUnderstanding videos to localize moments with naturallanguage often requires large expensive annotated video re-gions paired with language queries. To eliminate the an-notation costs, we make a first attempt to train a natu-ral language video localization model in zero-shot man-ner. Inspired by unsupervised image captioning setup, wemerely require random text corpora, unlabeled video collec-tions, and an off-the-shelf object detector to train a model.With the unpaired data, we propose to generate pseudo-supervision of candidate temporal regions and correspond-ing query sentences, and develop a simple NLVL modelto train with the pseudo-supervision. Our empirical vali-dations show that the proposed pseudo-supervised methodoutperforms several baseline approaches and a number ofmethods using stronger supervision on Charades-STA andActivityNet-Captions.1. IntroductionOn increasing demands of understanding videos tosearch with natural language queries, natural languagevideo localization (NLVL) has been actively investigated inrecent literature [19,35,39,43,44,57]. The task targets to lo-calize a temporal moment in a video by a natural languagequery. In recent years, significant performance improve-ments on benchmark datasets has been made, facilitated bythe advances on deep learning methods [19, 39, 43, 56] andmassively annotated data [2, 19, 26, 37, 58].As illustrated in Fig. 1-(a), the annotations consist of atemporal region in a video (start time, end time) and a cor-responding query sentence. However, obtaining such pairedannotation is laborious and expensive. To alleviate the an-notation cost, a number of recent works addressed weakly-supervised setup of NLVL [12, 21, 33] which aims to lo-calize a moment without the temporal alignment of givenquery sentence. Although it eliminates the annotation cost∗: equal contribution. †: corresponding author. § now at U. of Minnesota,Twin Cities. Code: https://github.com/gistvision/PSVL(a) Fully-supervised NLVL (b) Weakly-supervised NLVL(c) Unsupervised Image Captioning (d) Zero-shot NLVLText CorporaImage collection Video collectionQueryVideo + Temporal region {ts, te} VideoObject detector Text Corpora Object detectorthe person pours some waterinto the glassQuerythe person pours some water",
        "arxiv": "2110.00428",
        "doi": "10.1109/iccv48922.2021.00150",
        "file_name": "",
        "file_path": "2110.00428v1.pdf",
        "title": "Zero-shot Natural Language Video Localization",
        "urls": [
            [
                "https://github.com/gistvision/PSVL",
                2
            ]
        ]
    },
    "10.1109/iccv48922.2021.00276": {
        "abstract": "AbstractFeature pyramids have been proven powerful in imageunderstanding tasks that require multi-scale features.State-of-the-art methods for multi-scale feature learningfocus on performing feature interactions across space andscales using neural networks with a fixed topology. In thispaper, we propose graph feature pyramid networks that arecapable of adapting their topological structures to varyingintrinsic image structures, and supporting simultaneousfeature interactions across all scales. We first define animage specific superpixel hierarchy for each input image torepresent its intrinsic image structures. The graph featurepyramid network inherits its structure from this superpixelhierarchy. Contextual and hierarchical layers are designedto achieve feature interactions within the same scaleand across different scales. To make these layers morepowerful, we introduce two types of local channel attentionfor graph neural networks by generalizing global channelattention for convolutional neural networks. The proposedgraph feature pyramid network can enhance the multiscalefeatures from a convolutional feature pyramid network.We evaluate our graph feature pyramid network in theobject detection task by integrating it into the Faster R-CNN algorithm. The modified algorithm outperforms notonly previous state-of-the-art feature pyramid based meth-ods with a clear margin but also other popular detec-tion methods on both MS-COCO 2017 validation and testdatasets. Codes are available at https://github.com/GangmingZhao/GraphFPN-Graph-Feature-Pyramid-Network-for-Object-Detection.† This work is done when Gangming Zhao is a visiting studentat Fudan University. *Corresponding authors: wfge@fudan.edu.cn andyizhouy@acm.org1. IntroductionDeep convolutional neural networks exploit local con-nectivity and weights sharing, and have led to a series ofbreakthroughs in computer vision tasks, including imagerecognition [24, 48, 13, 49], object detection [10, 43, 34, 41,5, 31, 47], and semantic segmentation [33, 56, 29, 18, 54,50]. Since objects in an image may have varying scales, itis much desired to obtain multiscale feature maps that havefused high-level and low-level features with sufficient spa-tial resolution at every distinct scale. This motivated fea-ture pyramid networks (FPN [30]) and its improved ver-sions, such as path aggregation network (PANet [33]) andfeature pyramid transformer (FPT [54]), and other mteh-ods [22, 19, 9, 52, 12].Every image has multiscale intrinsic structures, includ-ing the grouping of pixels into object parts, the furthergrouping of parts into objects as well as the spatial layout ofobjects in the image space. Such multiscale intrinsic struc-",
        "arxiv": "2108.00580",
        "doi": "10.1109/iccv48922.2021.00276",
        "file_name": "",
        "file_path": "2108.00580v3.pdf",
        "title": "GraphFPN: Graph Feature Pyramid Network for Object Detection",
        "urls": [
            [
                "https://github.com/GangmingZhao/GraphFPN-Graph-Feature-Pyramid-Network-for-Object-Detection",
                2
            ]
        ]
    },
    "10.1109/iccv48922.2021.00338": {
        "abstract": "AbstractLearning maps between data samples is fundamental. Ap-plications range from representation learning, image trans-lation and generative modeling, to the estimation of spatialdeformations. Such maps relate feature vectors, or map be-tween feature spaces. Well-behaved maps should be regular,which can be imposed explicitly or may emanate from thedata itself. We explore what induces regularity for spatialtransformations, e.g., when computing image registrations.Classical optimization-based models compute maps betweenpairs of samples and rely on an appropriate regularizerfor well-posedness. Recent deep learning approaches haveattempted to avoid using such regularizers altogether byrelying on the sample population instead. We explore if it ispossible to obtain spatial regularity using an inverse consis-tency loss only and elucidate what explains map regularityin such a context. We find that deep networks combinedwith an inverse consistency loss and randomized off-gridinterpolation yield well behaved, approximately diffeomor-phic, spatial transformations. Despite the simplicity of thisapproach, our experiments present compelling evidence, onboth synthetic and real data, that regular maps can be ob-tained without carefully tuned explicit regularizers, whileachieving competitive registration performance.1. MotivationLearning maps between feature vectors or spaces is animportant task. Feature vector maps are used to improverepresentation learning [7], or to learn correspondences innatural language processing [4]. Maps between spaces areimportant for generative models when using normalizingflows [24] (to map between a simple and a complex probabil-ity distribution), or to determine spatial correspondences be-tween images, e.g., for optical flow [16] to determine motionfrom videos [12], depth estimation from stereo images [25],or medical image registration [40, 41].Regular maps are typically desired; e.g., diffeomorphicarXiv:2105.04459v3  [cs.CV",
        "arxiv": "2105.04459",
        "doi": "10.1109/iccv48922.2021.00338",
        "file_name": "",
        "file_path": "2105.04459v3.pdf",
        "title": "ICON: Learning Regular Maps Through Inverse Consistency",
        "urls": [
            [
                "https://github.com/uncbiag/ICON",
                1
            ]
        ]
    },
    "10.1109/icse43902.2021.00072": {
        "abstract": "Abstract—This paper presents a coverage-guided grammar-based fuzzing technique for automatically synthesizing a corpusof concise test inputs. We walk-through a case study of acompiler designed for education and the corresponding problemof generating meaningful test cases to provide to students.The prior state-of-the-art solution is a combination of fuzzingand test-case reduction techniques such as variants of delta-debugging. Our key insight is that instead of attempting tominimize convoluted fuzzer-generated test inputs, we can insteadgrow concise test inputs by construction using a form of iterativedeepening. We call this approach bonsai fuzzing. Experimentalresults show that bonsai fuzzing can generate test corpora havinginputs that are 16–45% smaller in size on average as comparedto a fuzz-then-reduce approach, while achieving approximatelythe same code coverage and fault-detection capability.Index Terms—test-case generation, grammar-based testing,fuzz testing, small scope hypothesis, test-case reductionI. INTRODUCTIONThis paper describes a new technique for automaticallygenerating a concise corpus of test inputs having a well-defined syntax and non-trivial semantics (e.g. for a compiler).This project originated when the authors were faced withthe task of generating a test corpus for use in an undergraduatecompilers course. The course project targets the ChocoPy pro-gramming language [1]. ChocoPy is a statically typed subsetof Python, designed specifically for education. In a ChocoPy-based course, students are expected to build a compiler in Javathat statically checks and then translates ChocoPy programsto RISC-V assembly. Student projects can be autograded bycomparing their compilers’ output at various stages—parser,type checker, and code generator—with the correspondingoutput produced by a reference implementation. When startingtheir project, students are provided with a suite of ChocoPy testprograms and the autograder, which together serve as a partialexecutable specification. This workflow simulates test-drivendevelopment, while also enabling students to continuously getfeedback about their progress. For instructors, writing testcases to validate every language feature is a tedious task; wewanted to automatically synthesize such a test corpus. Thispaper describes the technique we developed for this purpose.In particular, we focus on the problem of automatically gener-ating test cases that exercise the typechecker, since generatingwell-typed programs is known to be a difficult problem [2]–[5].This task presents two conflicting challenges: (1) the gen-erated test suite must be comprehensive in covering variousF2,2,2F2,2,1 F2,1,2 F1,2,2F2,1,1 F1,2,1 F1,1,2F1,1,1",
        "arxiv": "2103.04388",
        "doi": "10.1109/icse43902.2021.00072",
        "file_name": "",
        "file_path": "2103.04388v1.pdf",
        "title": "Growing a Test Corpus with Bonsai Fuzzing",
        "urls": [
            [
                "https://github.com/vasumv/bonsai-fuzzing",
                2
            ],
            [
                "https://github.com/renatahodovan/picireny",
                2
            ],
            [
                "https://github.com/renatahodovan/picire",
                2
            ],
            [
                "https://github.com/google/closure-compiler",
                2
            ],
            [
                "https://github.com/google/AFL",
                2
            ]
        ]
    },
    "10.1109/ijcnn52387.2021.9533521": {
        "abstract": "Abstract—In this paper, we tackle the transductive semi-supervised learning problem that aims to obtain label predictionsfor the given unlabeled data points according to Vapnik’sprinciple. Our proposed approach is based on optimal trans-port, a mathematical theory that has been successfully used toaddress various machine learning problems, and is starting toattract renewed interest in semi-supervised learning community.The proposed approach, Optimal Transport Propagation (OTP),performs in an incremental process, label propagation throughthe edges of a complete bipartite edge-weighted graph, whoseaffinity matrix is constructed from the optimal transport planbetween empirical measures defined on labeled and unlabeleddata. OTP ensures a high degree of predictions certitude bycontrolling the propagation process using a certainty score basedon Shannon’s entropy. We also provide a convergence analysisof our algorithm. Experiments task show the superiority of theproposed approach over the state-of-the-art. We make our codepublicly available. 1Index Terms—Optimal Transport, Semi-supervised Learning,Label PropagationI. INTRODUCTIONDeep learning models have achieved state-of-the-artperformance on a broad spectrum of learning tasks, andare becoming increasingly popular in various applicationdomains [18], such as image classification and speechrecognition, where a large amount of labeled data is available.However, for many tasks, it is often prohibitively expensiveto collect a large high quality labeled dataset due to lackof time, resources, or other factors, while unlabeled data ischeap and abundant. Medicine is the best illustration of thisscenario, where measurement require expensive machineryand labels are the result of a labor and expensive expert-assisted time-consuming analysis.In conjunction with transfer learning (TL), semi-supervisedlearning (SSL) constitute an attractive approach towardsaddressing the lack of massive labeled datasets. It seeks tolargely alleviate the need for labeled samples by providing ameans to jointly leverage unlabeled instances. Graph-basedsemi-supervised approaches are one of the most widely usedclasses of semi-supervised learning methods, due to theirperformance and to more and more real graph datasets. Theproblem is to predict the label of all the unlabeled vertices inthe graph based only on a small subset of labeled vertices.1Code is available at: https://github.com/MouradElHamri/OTPA popular graph-based semi-supervised learning methodis to use label propagation, this latter has shown goodperformances in different machine learning applications overthe past few years, such as social network analysis [4] [35],natural language processing [2], and image segmentation [7].Most existing label propagation algorithms essentially estimate",
        "arxiv": "2110.01446",
        "doi": "10.1109/ijcnn52387.2021.9533521",
        "file_name": "",
        "file_path": "2110.01446v1.pdf",
        "title": "Label Propagation Through Optimal Transport",
        "urls": [
            [
                "https://github.com/MouradElHamri/OTP",
                2
            ]
        ]
    },
    "10.1109/isbi52829.2022.9761409": {
        "abstract": "ABSTRACTLow-rank tensor models have been applied in accelerating dy-namic magnetic resonance imaging (dMRI). Recently, a newtensor nuclear norm based on t-SVD has been proposed andapplied to tensor completion. Inspired by the different proper-ties of the tensor nuclear norm (TNN) and the Casorati matrixnuclear norm (MNN), we introduce a combined TNN and Ca-sorati MNN regularizations framework to reconstruct dMRI,which we term as TMNN. The proposed method simultane-ously exploits the spatial structure and the temporal correla-tion of the dynamic MR data. The optimization problem canbe efficiently solved by the alternating direction method ofmultipliers (ADMM). In order to further improve the com-putational efficiency, we develop a fast algorithm under theCartesian sampling scenario. Numerical experiments basedon cardiac cine MRI and perfusion MRI data demonstrate theperformance improvement over the traditional Casorati nu-clear norm regularization method.Index Terms— Dynamic MRI reconstruction, tensor nu-clear norm, t-SVD1. INTRODUCTIONDynamic magnetic resonance imaging (dMRI) is one of themost important non-invasive imaging modalities, which hasfound a wide range of applications in clinical practice. How-ever, due to the physical limitations, it is usually challengingto obtain dynamic MR images with high spatiotemporal res-olution within clinically acceptable scan time. By exploitingthe low-rank structure of the spatiotemporal Casorati matrixformulated by extracting and unfolding each time frame as acolumn, the low-rank matrix recovery methods [1] have beensuccessfully applied to reconstruct dynamic MR images fromhighly undersampled k-space data to accelerate dMRI.Low-rank tensor priors have been introduced as power-ful alternatives due to their improvement in recovered imagequality [2, 3, 4]. Compared with matrix, tensor is a morenatural representation for multi-frame dynamic MR data.Recently, a new tensor decomposition called tensor singularThis work is supported by the National Natural Science Foundationof China under Grant 61871159 and Natural Science Foundation of Hei-longjiang YQ2021F005.value decomposition (t-SVD) [5] has been proposed. Com-pared with the traditional tensor decompositions (e.g., CP,TUCKER), t-SVD can be easily computed by solving thematrix SVDs in the Fourier domain. Based on t-SVD, Lu etal. proposed a new tensor nuclear norm (TNN) [6], which isthe convex envelope of the tensor average rank. In addition,unlike certain traditional tensor constraints, utilizing the TNNconstraint avoids explicit selection of tensor ranks. Someworks have adopted this framework to reconstruct dMRI.Banco et al. [7] have applied t-SVD in dMRI reconstruc-",
        "arxiv": "2206.00831",
        "doi": "10.1109/isbi52829.2022.9761409",
        "file_name": "",
        "file_path": "2206.00831v1.pdf",
        "title": "Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations",
        "urls": null
    },
    "10.1109/iwis56333.2022.9920762": {
        "abstract": "Abstract—A novel clustering technique based on the projectiononto convex set (POCS) method, called POCS-based clusteringalgorithm, is proposed in this paper. The proposed POCS-basedclustering algorithm exploits a parallel projection method ofPOCS to find appropriate cluster prototypes in the featurespace. The algorithm considers each data point as a convex setand projects the cluster prototypes parallelly to the memberdata points. The projections are convexly combined to mini-mize the objective function for data clustering purpose. Theperformance of the proposed POCS-based clustering algorithmis verified through experiments on various synthetic datasets.The experimental results show that the proposed POCS-basedclustering algorithm is competitive and efficient in terms ofclustering error and execution speed when compared withother conventional clustering methods including Fuzzy C-Means(FCM) and K-Means clustering algorithms. Code is available at:https://github.com/tranleanh/pocs-based-clusteringIndex Terms—POCS, clustering, unsupervised learning, ma-chine learning, K-MeansI. INTRODUCTIONProjection onto convex set (POCS) is a powerful tool forsignal synthesis and image restoration which was originallyintroduced by Bregman in the mid-1960s [1]. The POCSmethod has been widely used to find a common point ofconvex sets in several signal processing problems. The maintarget of the POCS approach is to find a vector that residesin the intersection of convex sets. Bregman has shown thatsuccessive projections between two or more convex sets withnon-empty intersection converge to a point that exists in theintersection of the convex sets. In the case of disjoint closedconvex sets, the sequential projection does not converge to asingle point, instead it converges to greedy limit cycles whichare dependent on the order of the projections [1]. This propertyof POCS, however, can be applied to clustering problems.Clustering is an unsupervised data analysis technique thatcategories similar data points while separating them fromthe different ones [2]. Most clustering algorithms try to find*Corresponding Authorhomogeneous subgroups that have similar characteristics bythe type of metric employed. The K-Means clustering algo-rithm, which has been one of the most popular methods forgeneral clustering purposes [9], uses the Euclidean distance tomeasure the similarity [2]. The K-Means clustering algorithmalternates between assigning cluster membership for each datapoint to the nearest cluster center and computing the centerof each cluster as the prototype of its member data points.The objective of the K-Means clustering algorithm is to finda set of prototypes that minimize the cost function. The K-Means clustering algorithm terminates its training procedurewhen there is no further change in the assignment of instances",
        "arxiv": "2208.08888",
        "doi": "10.1109/iwis56333.2022.9920762",
        "file_name": "",
        "file_path": "2208.08888v2.pdf",
        "title": "POCS-based Clustering Algorithm",
        "urls": [
            [
                "https://github.com/tranleanh/pocs-based-clustering",
                2
            ]
        ]
    },
    "10.1109/pacificvis48177.2020.3756": {
        "abstract": "ABSTRACTForce-directed algorithms are widely used to generate aesthetically-pleasing layouts of graphs or networks arisen in many scientificdisciplines. To visualize large-scale graphs, several parallel al-gorithms have been discussed in the literature. However, exist-ing parallel algorithms do not utilize memory hierarchy efficientlyand often offer limited parallelism. This paper addresses theselimitations with BatchLayout, an algorithm that groups verticesinto minibatches and processes them in parallel. BatchLayoutalso employs cache blocking techniques to utilize memory hier-archy efficiently. More parallelism and improved memory ac-cesses coupled with force approximating techniques, better initial-ization, and optimized learning rate make BatchLayout significantlyfaster than other state-of-the-art algorithms such as ForceAtlas2 andOpenOrd. The visualization quality of layouts from BatchLayoutis comparable or better than similar visualization tools. All of oursource code, links to datasets, results and log files are available athttps://github.com/khaled-rahman/BatchLayout.Index Terms: Human-centered computing—Visualization—Visualization techniques—Graph drawings; Human-centeredcomputing—Visualization—Visualization systems and tools—Visualization toolkits1 INTRODUCTIONNetworks or graphs are common representations of scientific, socialand business data. In a graph, a set of vertices represents entities(e.g., persons, brain neurons, atoms) and a set of edges indicatesrelationships among entities (friendship, neuron synapses, chemicalbonds). A key aspect of data-driven graph analytics is to visuallystudy large-scale networks, such as biological and social networks,with millions or even billions of vertices and edges. In networkvisualization, the first step is to generate a layout in a 2D or 3Dcoordinate system which can be fed into visualization tools such asGephi [4] and Cytoscape [29]. Therefore, the quality and computa-tional complexity of network visualization are often dominated bythe graph layout algorithms.Force-directed layout algorithms are among the most popularlyused techniques to generate the layout of a graph. Following thephilosophy of the spring energy model, these algorithms calculate at-tractive and repulsive forces among vertices in a graph and iterativelyminimize an energy function. Classical force-directed algorithms,such as the Fruchterman and Reingold (FR) algorithm [9], requireO(n2) time per iteration where n is the number of vertices in a graph.By approximating the repulsive force between non-adjacent nodes,we can get a faster O(n logn) algorithm. In this paper, we used theBarnes-Hut approximation [3] based on the quad-tree data structure.*e-mail: morahma@iu.edu, Department of Computer Science†e-mail: msujon@iu.edu, Department of Intelligent Systems Engineering‡Corresponding author.§e-mail: azad@iu.edu, Department of Intelligent Systems EngineeringEven though the layout quality (measured by stress, neighborhood",
        "arxiv": "2002.08233",
        "doi": "10.1109/pacificvis48177.2020.3756",
        "file_name": "",
        "file_path": "2002.08233v1.pdf",
        "title": "BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory",
        "urls": [
            [
                "https://github.com/khaled-rahman/BatchLayout",
                2
            ],
            [
                "https://github.com/SciTechStrategies/OpenOrd",
                1
            ]
        ]
    },
    "10.1109/tcss.2021.3059286": {
        "abstract": "ABSTRACTInterest surrounding cryptocurrencies, digital or virtual currenciesthat are used as a medium for financial transactions, has growntremendously in recent years. The anonymity surrounding thesecurrencies makes investors particularly susceptible to fraud—suchas “pump and dump” scams—where the goal is to artificially inflatethe perceived worth of a currency, luring victims into investingbefore the scammers can sell their holdings. Because of the speedand relative anonymity offered by social platforms such as Twitterand Telegram, social media has become a preferred platform forscammers who wish to spread false hype about the cryptocurrencythey are trying to pump. In this work we propose and evaluatea computational approach that can automatically identify pumpand dump scams as they unfold by combining information acrosssocial media platforms. We also develop a multi-modal approachfor predicting whether a particular pump attempt will succeed ornot. Finally, we analyze the prevalence of bots in cryptocurrencyrelated tweets, and observe a significant significant presence of botsduring the pump attempts.KEYWORDScryptocurrency, pump and dump, social media data mining, anom-aly detection1 INTRODUCTIONThe inception of blockchain technology [14] gave birth to the pop-ular cryptocurrency Bitcoin (symbol BTC). Since then, thousandsof cryptocurrencies have emerged, and their hype has caused mas-sive price swings on the trading markets. In December 2017, BTCquadrupled in market value in just over a month, then within a fewdays started a gradual decline until it reached half of its peak value.These price changes allowed some investors to realize huge profits,contributing to the allure of cryptocurrencies. Even though mostinvestments are made in relatively established cryptocurrencies,including Bitcoin (BTC) and Ethereum (ETH), there are thousands ofother smaller cryptocurrencies. These currencies are prime targetsfor manipulation by scammers, as evidenced by the proliferation ofpump and dump schemes.Pump and dump schemes are those in which a security priceinflates due to deliberately deceptive activities. Those fraudulentschemes originated in the early days of the stock market and arenow growing rapidly in the cryptocurrency market. The fact that07:0009:0011:0013:0015:00",
        "arxiv": "1902.03110",
        "doi": "10.1109/tcss.2021.3059286",
        "file_name": "",
        "file_path": "1902.03110v2.pdf",
        "title": "Identifying and Analyzing Cryptocurrency Manipulations in Social Media",
        "urls": [
            [
                "https://github.com/Mehrnoom/Cryptocurrency-Pump-Dump",
                1
            ]
        ]
    },
    "10.1109/tpami.1979.4766956": {
        "abstract": "Abstract—This paper addresses the problem of photometric stereo, in both calibrated and uncalibrated scenarios, for non-Lambertiansurfaces based on deep learning. We first introduce a fully convolutional deep network for calibrated photometric stereo, which we callPS-FCN. Unlike traditional approaches that adopt simplified reflectance models to make the problem tractable, our method directlylearns the mapping from reflectance observations to surface normal, and is able to handle surfaces with general and unknown isotropicreflectance. At test time, PS-FCN takes an arbitrary number of images and their associated light directions as input and predicts asurface normal map of the scene in a fast feed-forward pass. To deal with the uncalibrated scenario where light directions areunknown, we introduce a new convolutional network, named LCNet, to estimate light directions from input images. The estimated lightdirections and the input images are then fed to PS-FCN to determine the surface normals. Our method does not require a pre-definedset of light directions and can handle multiple images in an order-agnostic manner. Thorough evaluation of our approach on bothsynthetic and real datasets shows that it outperforms state-of-the-art methods in both calibrated and uncalibrated scenarios.Index Terms—photometric stereo, non-Lambertian, uncalibrated, convolutional neural network.F1 INTRODUCTIONPHOTOMETRIC stereo aims at recovering the surface nor-mals of a static scene from a set of images capturedunder different light directions with a fixed camera [1], [2].Based on the availability of calibrated lighting conditions,photometric stereo can be categorized into calibrated and un-calibrated photometric stereo settings. Early calibrated pho-tometric stereo methods assumed a simplified reflectancemodel, such as the ideal Lambertian model [1], [2] or an-alytical reflectance models [3], [4], [5]. However, most ofthe real-world objects are non-Lambertian, and a specificanalytical model is only valid for a small set of materials.A bidirectional reflectance distribution function (BRDF) isa general form for describing the reflectance property of asurface, but it is difficult to directly use a non-parametricform of BRDFs for photometric stereo.Recently, with the great success of deep learning invarious computer vision tasks, deep learning based methodshave been introduced to calibrated photometric stereo tohandle surfaces with general and unknown isotropic re-flectance [6], [7], [8]. Instead of explicitly modeling complexsurface reflectances, they directly learn the mapping fromreflectance observations to surface normals given knownlight directions. However, the method in [6] depends ona pre-defined set of light directions during training andtesting. The methods in [6], [7] estimate the surface normals• G. Chen and K-Y. K. Wong are with The University of Hong Kong, HongKong, China.E-mail: {gychen,kykwong}@cs.hku.hk• K. Han is with University of Oxford, Oxford, United Kingdom.E-mail: khan@robots.ox.ac.uk• B. Shi is with Peking University, Beijing and Peng Cheng Laboratory,Shenzhen, China.E-mail: shiboxin@pku.edu.cn• Y. Matsushita is with Osaka University, Osaka, Japan.E-mail: yasumat@ist.osaka-u.ac.jpin a pixel-wise manner, making them not possible to accountfor the local context information of a surface point (e.g., sur-face smoothness prior). Taniai and Maehara [8] introduced",
        "arxiv": null,
        "doi": "10.1109/tpami.1979.4766956",
        "file_name": "",
        "file_path": "2007.13145v1.pdf",
        "title": "1979 Index - IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-1",
        "urls": null
    },
    "10.1109/tsp.2022.3229948": {
        "abstract": "Abstract—We consider resource management problems inmulti-user wireless networks, which can be cast as optimizinga network-wide utility function, subject to constraints on thelong-term average performance of users across the network.We propose a state-augmented algorithm for solving the afore-mentioned radio resource management (RRM) problems, where,alongside the instantaneous network state, the RRM policy takesas input the set of dual variables corresponding to the constraints,which evolve depending on how much the constraints are violatedduring execution. We theoretically show that the proposed state-augmented algorithm leads to feasible and near-optimal RRMdecisions. Moreover, focusing on the problem of wireless powercontrol using graph neural network (GNN) parameterizations,we demonstrate the superiority of the proposed RRM algorithmover baseline methods across a suite of numerical experiments.Index Terms—Radio resource management, wireless networks,graph neural networks, Lagrangian duality, state augmentation,wireless power control.I. INTRODUCTIONWith the proliferation of 5G network implementationsacross the globe and research already underway for future6G wireless networks, novel wireless services and capabili-ties are expected to emerge that require carefully-optimizedmanagement of wireless resources. Aside from traditionalapproaches for addressing such radio resource management(RRM) problems [2]–[6], learning-based methods have re-cently gained significant traction and demonstrated superiorperformance over prior approaches [7]–[12]. Such methods areenvisioned to play a key role in current and future wirelessnetworks with the ubiquitous availability of computationalresources both at the end-user devices and within the networkinfrastructure [13]–[16].As a general formulation of the RRM problem, similarly toprior work [7], [10], [17], [18], we consider a network utilitymaximization problem, subject to multiple constraints, whereboth the utility and the constraints are defined based on thelong-term average performance of users across the network.A common method for solving such problems is to move tothe Lagrangian dual domain, where a single objective, i.e., theN. NaderiAlizadeh and A. Ribeiro are with the Department of Elec-trical and Systems Engineering, University of Pennsylvania, Philadelphia,PA 19104, USA (e-mails: {nnaderi, aribeiro}@seas.upenn.edu). M. Eisenis with Intel Labs, Intel Corporation, Hillsboro, OR 97124, USA (e-mail:mark.eisen@intel.com).This work was supported in part by ARL DCIST CRA under GrantW911NF-17-2-0181, the AI Institute for Learning-Enabled Optimization atScale (TILOS) (NSF CCF-2112665), and the NSF-Simons Research Collab-oration on the Mathematical and Scientific Foundations of Deep Learning(MoDL) (NSF DMS-2031985).This paper has been presented in part at the 2022 Asilomar Conference on",
        "arxiv": "2207.02242",
        "doi": "10.1109/tsp.2022.3229948",
        "file_name": "",
        "file_path": "2207.02242v2.pdf",
        "title": "State-Augmented Learnable Algorithms for Resource Management in Wireless Networks",
        "urls": [
            [
                "https://github.com/navid-naderi/StateAugmented_RRM_GNN",
                2
            ]
        ]
    },
    "10.1145/3318170.3318183": {
        "abstract": "ABSTRACTOver the past few years machine learning has seen a renewedexplosion of interest, following a number of studies showing theeffectiveness of neural networks in a range of tasks which hadpreviously been considered incredibly hard. Neural networks’ ef-fectiveness in the fields of image recognition and natural languageprocessing stems primarily from the vast amounts of data availableto companies and researchers, coupled with the huge amounts ofcompute power available in modern accelerators such as GPUs,FPGAs and ASICs. There are a number of approaches available todevelopers for utilizing GPGPU technologies such as SYCL, OpenCLand CUDA, however many applications require the same low levelmathematical routines. Libraries dedicated to accelerating thesecommon routines allow developers to easily make full use of theavailable hardware without requiring low level knowledge of thehardware themselves, however such libraries are often provided byhardware manufacturers for specific hardware such as cuDNN [9]for Nvidia hardware or MIOpen [5] for AMD hardware.SYCL-DNN is a new open-source library dedicated to provid-ing accelerated routines for neural network operations which arehardware and vendor agnostic. Built on top of the SYCL open stan-dard and written entirely in standard C++, SYCL-DNN allows auser to easily accelerate neural network code for a wide range ofhardware using a modern C++ interface. The library is tested onAMD’s OpenCL for GPU, Intel’s OpenCL for CPU and GPU, ARM’sOpenCL for Mali GPUs as well as ComputeAorta’s OpenCL forR-Car CV engine and host CPU. In this talk we will present per-formance figures for SYCL-DNN on this range of hardware, anddiscuss how high performance was achieved on such a varied setof accelerators with such different hardware features.CCS CONCEPTS• Computing methodologies → Neural networks; Massivelyparallel algorithms; Parallel programming languages; Computervision problems.KEYWORDSSYCL, OpenCL, neural networks, GPGPU, machine learning∗Authors listed alphabeticallyPermission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).IWOCL’19, May 13–15, 2019, Boston, MA, USA© 2019 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-6230-6/19/05.https://doi.org/10.1145/3318170.3318183ACM Reference Format:Rod Burns, John Lawson, Duncan McBain and Daniel Soutar. 2019. Acceler-ated Neural Networks on OpenCL Devices Using SYCL-DNN. In Interna-tional Workshop on OpenCL (IWOCL’19), May 13–15, 2019, Boston, MA, USA.",
        "arxiv": "1904.04174",
        "doi": "10.1145/3318170.3318183",
        "file_name": "",
        "file_path": "1904.04174v1.pdf",
        "title": "Accelerated Neural Networks on OpenCL Devices Using SYCL-DNN",
        "urls": [
            [
                "https://github.com/triSYCL/triSYCL",
                2
            ],
            [
                "https://github.com/intel/mkl-dnn",
                2
            ],
            [
                "https://github.com/illuhad/hipSYCL",
                2
            ],
            [
                "https://github.com/ROCmSoftwarePlatform/MIOpen",
                2
            ],
            [
                "https://github.com/CodeplaySoftware/SYCL-DNN",
                2
            ],
            [
                "https://github.com/ARM-software/ComputeLibrary",
                2
            ]
        ]
    },
    "10.1145/3442381.3449934": {
        "abstract": "ABSTRACTElectric Vehicle (EV) has become a preferable choice in the moderntransportation system due to its environmental and energy sus-tainability. However, in many large cities, EV drivers often fail tofind the proper spots for charging, because of the limited charg-ing infrastructures and the spatiotemporally unbalanced charg-ing demands. Indeed, the recent emergence of deep reinforcementlearning provides great potential to improve the charging expe-rience from various aspects over a long-term horizon. In this pa-per, we propose a framework, named Multi-Agent Spatio-TemporalReinforcement Learning (Master), for intelligently recommendingpublic accessible charging stations by jointly considering variouslong-term spatiotemporal factors. Specifically, by regarding eachcharging station as an individual agent, we formulate this prob-lem as a multi-objective multi-agent reinforcement learning task.We first develop a multi-agent actor-critic framework with thecentralized attentive critic to coordinate the recommendation be-tween geo-distributed agents. Moreover, to quantify the influenceof future potential charging competition, we introduce a delayedaccess strategy to exploit the knowledge of future charging compe-tition during training. After that, to effectively optimize multiplelearning objectives, we extend the centralized attentive critic tomulti-critics and develop a dynamic gradient re-weighting strategyto adaptively guide the optimization direction. Finally, extensiveexperiments on two real-world datasets demonstrate that Masterachieves the best comprehensive performance compared with ninebaseline approaches.KEYWORDSCharging station recommendation,multi-agent reinforcement learn-ing, multi-objective optimizationACM Reference Format:Weijia Zhang1†, Hao Liu2∗, Fan Wang3, Tong Xu1, Haoran Xin1, DejingDou2, Hui Xiong4∗. 2021. Intelligent Electric Vehicle Charging Recommen-dation Based on Multi-Agent Reinforcement Learning. In Proceedings of the∗ Corresponding author.† The research was done when the first author was an intern in Baidu Research underthe supervision of the second author.This paper is published under the Creative Commons Attribution 4.0 International(CC-BY 4.0) license. Authors reserve their rights to disseminate the work on theirpersonal and corporate Web sites with the appropriate attribution.WWW ’21, April 19–23, 2021, Ljubljana, Slovenia© 2021 IW3C2 (International World Wide Web Conference Committee), publishedunder Creative Commons CC-BY 4.0 License.ACM ISBN 978-1-4503-8312-7/21/04.https://doi.org/10.1145/3442381.3449934Web Conference 2021 (WWW ’21), April 19–23, 2021, Ljubljana, Slovenia.ACM,New York, NY, USA, 12 pages. https://doi.org/10.1145/3442381.34499341 INTRODUCTIONDue to the low-carbon emission and energy efficiency, Electricvehicles (EVs) are emerging as a favorable choice in the modern",
        "arxiv": "2102.07359",
        "doi": "10.1145/3442381.3449934",
        "file_name": "",
        "file_path": "2102.07359v1.pdf",
        "title": "Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning",
        "urls": [
            [
                "https://github.com/Vvrep/MASTER-electric_vehicle_charging_recommendation",
                1
            ]
        ]
    },
    "10.1145/3485447.3511945": {
        "abstract": "ABSTRACTEntity alignment, aiming to identify equivalent entities across dif-ferent knowledge graphs (KGs), is a fundamental problem for con-structing Web-scale KGs. Over the course of its development, thelabel supervision has been considered necessary for accurate align-ments. Inspired by the recent progress of self-supervised learning,we explore the extent to which we can get rid of supervision forentity alignment. Commonly, the label information (positive entitypairs) is used to supervise the process of pulling the aligned enti-ties in each positive pair closer. However, our theoretical analysissuggests that the learning of entity alignment can actually benefitmore from pushing unlabeled negative pairs far away from eachother than pulling labeled positive pairs close. By leveraging thisdiscovery, we develop the self-supervised learning objective forentity alignment. We present SelfKG with efficient strategies tooptimize this objective for aligning entities without label supervi-sion. Extensive experiments on benchmark datasets demonstratethat SelfKG without supervision can match or achieve comparableresults with state-of-the-art supervised baselines. The performanceof SelfKG suggests that self-supervised learning offers great poten-tial for entity alignment in KGs. The code and data are available athttps://github.com/THUDM/SelfKG.CCS CONCEPTS• Computing methodologies→ Neural networks; • Informa-tion systems→ Information integration.KEYWORDSKnowledge Graphs, Self-Supervised Learning, Entity AlignmentACM Reference Format:Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov,Yuxiao Dong, Jie Tang. 2022. SelfKG: Self-Supervised Entity Alignment inKnowledge Graphs. In Proceedings of the ACM Web Conference 2022 (WWW’22), April 25–29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA,11 pages. https://doi.org/10.1145/3485447.3511945Xiao and Haoyun contributed equally to this work.Jie Tang is the corresponding author.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.WWW ’22, April 25–29, 2022, Virtual Event, Lyon, France.© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9096-5/22/04. . . $15.00https://doi.org/10.1145/3485447.3511945DWY100K(dbp_wd) DWY100K(dbp_yg) DBP15K(fr_en)80859095",
        "arxiv": "2203.01044",
        "doi": "10.1145/3485447.3511945",
        "file_name": "",
        "file_path": "2203.01044v1.pdf",
        "title": "SelfKG: Self-Supervised Entity Alignment in Knowledge Graphs",
        "urls": [
            [
                "https://github.com/THUDM/SelfKG",
                4
            ],
            [
                "https://github.com/nju-websoft/JAPE",
                2
            ],
            [
                "https://github.com/facebookresearch/faiss",
                2
            ],
            [
                "https://github.com/syxu828/Crosslingula-KG-Matching",
                1
            ],
            [
                "https://github.com/nju-websoft/BootEA",
                1
            ]
        ]
    },
    "10.1145/3488560.3498439": {
        "abstract": "AbstractWe study the practical consequences of dataset sampling strategieson the ranking performance of recommendation algorithms. Rec-ommender systems are generally trained and evaluated on samplesof larger datasets. Samples are often taken in a naïve or ad-hoc fash-ion: e.g. by sampling a dataset randomly or by selecting users oritems with many interactions. As we demonstrate, commonly-useddata sampling schemes can have significant consequences on algo-rithm performance. Following this observation, this paper makesthree main contributions: (1) characterizing the effect of samplingon algorithm performance, in terms of algorithm and dataset char-acteristics (e.g. sparsity characteristics, sequential dynamics, etc.);(2) designing SVP-CF, which is a data-specific sampling strategy,that aims to preserve the relative performance of models after sam-pling, and is especially suited to long-tailed interaction data; and (3)developing an oracle, Data-genie, which can suggest the samplingscheme that is most likely to preserve model performance for agiven dataset. The main benefit of Data-genie is that it will allowrecommender system practitioners to quickly prototype and com-pare various approaches, while remaining confident that algorithmperformance will be preserved, once the algorithm is retrained anddeployed on the complete data. Detailed experiments show thatusing Data-genie, we can discard upto 5× more data than anysampling strategy with the same level of performance.CCS Concepts• Information systems→ Recommender systems; • Comput-ing methodologies → Feature selection.KeywordsSampling; Coreset Mining; Benchmarking; Large-scale LearningACM Reference Format:Noveen Sachdeva, Carole-Jean Wu, and Julian McAuley. 2022. On Sam-pling Collaborative Filtering Datasets. In Proceedings of the Fifteenth ACMInternational Conference on Web Search and Data Mining (WSDM ’22), Feb-ruary 21–25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 9 pages.https://doi.org/10.1145/3488560.34984391 IntroductionRepresentative sampling of collaborative filtering (CF) data is acrucial problem from numerous stand-points and can be performedPermission to make digital or hard copies of part or all of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for third-party components of this work must be honored.For all other uses, contact the owner/author(s).WSDM ’22, February 21–25, 2022, Tempe, AZ, USA© 2022 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-9132-0/22/02.https://doi.org/10.1145/3488560.3498439at various levels: (1) mining hard-negatives while training com-plex algorithms over massive datasets [5, 31]; (2) down-samplingthe item-space to estimate expensive ranking metrics [3, 22]; and",
        "arxiv": "2201.04768",
        "doi": "10.1145/3488560.3498439",
        "file_name": "",
        "file_path": "2201.04768v1.pdf",
        "title": "On Sampling Collaborative Filtering Datasets",
        "urls": [
            [
                "https://github.com/noveens/sampling_cf",
                2
            ]
        ]
    },
    "10.1145/3503161.3547950": {
        "abstract": "ABSTRACTBlind watermarking provides powerful evidence for copyright pro-tection, image authentication, and tampering identification. How-ever, it remains a challenge to design a watermarking model withhigh imperceptibility and robustness against strong noise attacks.To resolve this issue, we present a framework Combining theInvertible and Non-invertible (CIN) mechanisms. The CIN is com-posed of the invertible part to achieve high imperceptibility andthe non-invertible part to strengthen the robustness against strongnoise attacks. For the invertible part, we develop a diffusion andextraction module (DEM) and a fusion and split module (FSM)to embed and extract watermarks symmetrically in an invertibleway. For the non-invertible part, we introduce a non-invertibleattention-based module (NIAM) and the noise-specific selectionmodule (NSM) to solve the asymmetric extraction under a strongnoise attack. Extensive experiments demonstrate that our frame-work outperforms the current state-of-the-art methods of imper-ceptibility and robustness significantly. Our framework can achievean average of 99.99% accuracy and 67.66 𝑑𝐵 𝑃𝑆𝑁𝑅 under noise-freeconditions, while 96.64% and 39.28 𝑑𝐵 combined strong noise at-tacks. The code will be available in https://github.com/rmpku/CIN.CCS CONCEPTS• Security and privacy→ Digital rights management.KEYWORDSRobust blind watermarking; Invertible network∗Corresponding authors.CINCIN*Without NoiseJPEG Noise99.93 %48.31 dBMBRS99.99 %67.66 dB95.52 %33.50 dB77.49 %36.22 dB97.40 %",
        "arxiv": "2212.12678",
        "doi": "10.1145/3503161.3547950",
        "file_name": "",
        "file_path": "2212.12678v1.pdf",
        "title": "Towards Blind Watermarking: Combining Invertible and Non-invertible Mechanisms",
        "urls": [
            [
                "https://github.com/rmpku/CIN",
                1
            ],
            [
                "https://github.com/RM1110/CIN",
                1
            ]
        ]
    },
    "10.1145/3503161.3548035": {
        "abstract": "ABSTRACTVideo Question Answering (VideoQA) is the task of answering thenatural language questions about a video. Producing an answer re-quires understanding the interplay across visual scenes in video andlinguistic semantics in question. However, most leading VideoQAmodels work as black boxes, which make the visual-linguistic align-ment behind the answering process obscure. Such black-box naturecalls for visual explainability that reveals “What part of the videoshould the model look at to answer the question?”. Only a fewworks present the visual explanations in a post-hoc fashion, whichemulates the target model’s answering process via an additionalmethod. Nonetheless, the emulation struggles to faithfully exhibitthe visual-linguistic alignment during answering.Instead of post-hoc explainability, we focus on intrinsic inter-pretability to make the answering process transparent. At its core isgrounding the question-critical cues as the causal scene to yield an-swers, while rolling out the question-irrelevant information as theenvironment scene. Taking a causal look at VideoQA, we devise aself-interpretable framework, Equivariant and Invariant Groundingfor Interpretable VideoQA (EIGV). Specifically, the equivariantgrounding encourages the answering to be sensitive to the semanticchanges in the causal scene and question; in contrast, the invariantgrounding enforces the answering to be insensitive to the changesin the environment scene. By imposing them on the answeringprocess, EIGV is able to distinguish the causal scene from the envi-ronment information, and explicitly present the visual-linguisticalignment. Extensive experiments on three benchmark datasetsjustify the superiority of EIGV in terms of accuracy and visual in-terpretability over the leading baselines. Our code is available athttps://github.com/yl3800/EIGV.CCS CONCEPTS• Information systems → Question answering; Multimediaand multimodal retrieval.KEYWORDSVideo Question Answering, Invariant Learning, Equivariant Learn-ing, Interpretability∗ Corresponding author. This research is supported by the Sea-NExT Joint Lab, andthe CCCD Key Lab of Ministry of Culture and Tourism, USTC..This work is licensed under a Creative Commons AttributionInternational 4.0 License.MM ’22, October 10–14, 2022, Lisboa, Portugal© 2022 Copyright held by the owner/author(s).ACM ISBN 978-1-4503-9203-7/22/10.https://doi.org/10.1145/3503161.3548035ACM Reference Format:Yicong Li, Xiang Wang, Junbin Xiao, and Tat-Seng Chua. 2022. Equivariantand Invariant Grounding for Video Question Answering. In Proceedingsof the 30th ACM International Conference on Multimedia (MM ’22), Oct.10–14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3503161.3548035",
        "arxiv": "2207.12783",
        "doi": "10.1145/3503161.3548035",
        "file_name": "",
        "file_path": "2207.12783v1.pdf",
        "title": "Equivariant and Invariant Grounding for Video Question Answering",
        "urls": [
            [
                "https://github.com/yl3800/EIGV",
                2
            ]
        ]
    },
    "10.1145/3508352.3549415": {
        "abstract": "ABSTRACTThe dramatic increase of data breaches in modern computing plat-forms has emphasized that access control is not sufficient to protectsensitive user data. Recent advances in cryptography allow end-to-end processing of encrypted data without the need for decryptionusing Fully Homomorphic Encryption (FHE). Such computationhowever, is still orders of magnitude slower than direct (unen-crypted) computation. Depending on the underlying cryptographicscheme, FHE schemes can work natively either at bit-level usingBoolean circuits, or over integers using modular arithmetic. Op-erations on integers are limited to addition/subtraction and mul-tiplication. On the other hand, bit-level arithmetic is much morecomprehensive allowing more operations, such as comparison anddivision. While modular arithmetic can emulate bit-level compu-tation, there is a significant cost in performance. In this work, wepropose a novel method, dubbed bridging, that blends faster andrestricted modular computation with slower and comprehensivebit-level computation, making them both usable within the sameapplication and with the same cryptographic scheme instantiation.We introduce and open source C++ types representing the twodistinct arithmetic modes, offering the possibility to convert fromone to the other. Experimental results show that bridging modularand bit-level arithmetic computation can lead to 1-2 orders of mag-nitude performance improvement for tested synthetic benchmarks,as well as one real-world FHE application: a genotype imputationcase study.CCS CONCEPTS• Security and privacy → Domain-specific security and pri-vacy architectures.KEYWORDSfully homomorphic encryption, privacy-preserving computationACM Reference Format:Eduardo Chielle, Oleg Mazonka, Homer Gamil, and Michail Maniatakos.2022. Accelerating Fully Homomorphic Encryption by BridgingModular andBit-Level Arithmetic. In IEEE/ACM International Conference on Computer-Aided Design (ICCAD ’22), October 30-November 3, 2022, San Diego, CA, USA.ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3508352.3549415Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.ICCAD ’22, October 30-November 3, 2022, San Diego, CA, USA© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 978-1-4503-9217-4/22/10. . . $15.00https://doi.org/10.1145/3508352.35494151 INTRODUCTIONWith the ever increasing rates of data generation, digital informa-",
        "arxiv": "2204.12201",
        "doi": "10.1145/3508352.3549415",
        "file_name": "",
        "file_path": "2204.12201v3.pdf",
        "title": "Accelerating Fully Homomorphic Encryption by Bridging Modular and Bit-Level Arithmetic",
        "urls": [
            [
                "https://github.com/nucypher/nufhe",
                2
            ],
            [
                "https://github.com/momalab/e3",
                2
            ],
            [
                "https://github.com/Microsoft/SEAL",
                2
            ]
        ]
    },
    "10.1145/3555552": {
        "abstract": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requiresprior specific permission and/or a fee. Request permissions from permissions@acm.org.CSCW ’22, November 03–05, 2022, Woodstock, NY© 2022 Association for Computing Machinery.ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00https://doi.org/xx.xxxx/xxxxxxx.xxxxxxx1arXiv:2208.13094v1  [cs.HC]  27 Aug 2022https://doi.org/xx.xxxx/xxxxxxx.xxxxxxxhttps://doi.org/xx.xxxx/xxxxxxx.xxxxxxxCSCW ’22, November 03–05, 2022, Woodstock, NY Joon Sung Park et al.for community self-moderation [19, 71]. Researchers have likewise sought to help reduce theprevalence of these behaviors, for example by creating tools for governance [34, 97] and automaticdetection of anti-social behaviors [14, 93].Given these harms, and the effort to combat them, it is critical to understand how much ofthe content in online communities remains anti-social. A large empirical foundation in socialpsychology demonstrates that if these behaviors are visible and widespread, it will encourageothers to engage in anti-social behaviors as well [25, 26]. Benchmarking progress, or regression,requires an honest accounting of the situation. Platforms themselves have begun measuring theprevalence of specific behaviors such as hate speech and harassment [29, 79, 84].Despite this need, empirically measuring the prevalence of anti-social behavior remains difficult.AI tools remain too error-prone to be fully relied upon [8, 44], and manual labeling via randomsampling is labor-intensive to perform at scale. In addition, defining which behaviors cross theline remains contentious, with different online communities applying different definitions [52] andplatforms each establishing different standards for content and enforcement [49, 52]. Therefore, itis no surprise that empirical studies that measure the prevalence of anti-social behavior are fewand far between, with platform-published performance metrics often tucked under the platforms’“transparency” or compliance reports using vaguely defined categories [29].In this paper, we develop a method to measure the proportion of these behaviors in online",
        "arxiv": "2208.13094",
        "doi": "10.1145/3555552",
        "file_name": "",
        "file_path": "2208.13094v1.pdf",
        "title": "Measuring the Prevalence of Anti-Social Behavior in Online Communities",
        "urls": [
            [
                "https://github.com/StanfordHCI/ContentModAudit_CodeRelease",
                2
            ]
        ]
    },
    "10.1159/000453569": {
        "abstract": "Abstract—Recent studies in Learning to Rank have shown the possibility to effectively distill a neural network from an ensemble ofregression trees. This result leads neural networks to become a natural competitor of tree-based ensembles on the ranking task.Nevertheless, ensembles of regression trees outperform neural models both in terms of efficiency and effectiveness, particularly whenscoring on CPU. In this paper, we propose an approach for speeding up neural scoring time by applying a combination of Distillation,Pruning and Fast Matrix multiplication. We employ knowledge distillation to learn shallow neural networks from an ensemble ofregression trees. Then, we exploit an efficiency-oriented pruning technique that performs a sparsification of the mostcomputationally-intensive layers of the neural network that is then scored with optimized sparse matrix multiplication. Moreover, bystudying both dense and sparse high performance matrix multiplication, we develop a scoring time prediction model which helps indevising neural network architectures that match the desired efficiency requirements. Comprehensive experiments on two publiclearning-to-rank datasets show that neural networks produced with our novel approach are competitive at any point of theeffectiveness-efficiency trade-off when compared with tree-based ensembles, providing up to 4x scoring time speed-up withoutaffecting the ranking quality.Index Terms—Web search, learning-to-rank, neural networks, efficiency, distillation, pruning, matrix multiplication.F©2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or futuremedia, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale orredistribution to servers or lists, or reuse of any copyrighted component of this work in other works.1 INTRODUCTIONTHE estimation of relevance is a task of paramount im-portance in Web search. In fact, search engines providethe users with a list of relevant results answering a infor-mation need formulated as a textual query. In the last years,Learning to Rank (LtR) techniques have been successfullyapplied to solve this task. LtR is the field of machine learn-ing devoted to the development of supervised techniquesaddressing the ranking problem. LtR techniques have beenproficiently used in Web search, a scenario characterizedby tight latency bounds for query processing [11]. For thisreason, the investigation of new LtR techniques targets botheffectiveness and efficiency to provide accurate solutionsthat can be used in modern query processors. State-of-the-art approaches in learning to rank are ensembles of re-gression trees. Specifically, LambdaMART [9] is an effectivestate-of-the-art LtR algorithm that builds ensembles of re-gression trees by optimizing a loss function that depends ona listwise information retrieval metric, e.g., NDCG [30]. Thecounterpart of the retrieval accuracy guaranteed by tree-based models is the computational effort needed to traversehundreds or even thousands of trees. This computationaleffort hinders the application of this kind of models onlow-latency query processors. Furthermore, each tree in anensemble work by testing a sequence of boolean conditionson the input. The natural translation of this structure in if-then-else code conflicts with modern CPU architectures thatheavily rely on branch prediction and caching. A recent line• Cosimo Rulli and Rossano Venturini are with the University of Pisa, Italy.E-mail: cosimo.rulli@phd.unipi.it, rossano.venturini@di.unipi.it.• Franco Maria Nardini, Cosimo Rulli, Salvatore Trani and Rossano Ven-turini are with the ISTI–CNR, Pisa, Italy. E-mail: {cosimo.rulli, f.nardini,s.trani, rossano.venturini}@isti.cnr.it.of research investigates techniques for efficient traversal of",
        "arxiv": null,
        "doi": "10.1159/000453569",
        "file_name": "",
        "file_path": "2202.10728v1.pdf",
        "title": "PD-1 and PD-L1 Immune Checkpoint Blockade to Treat Breast Cancer",
        "urls": [
            [
                "https://github.com/oneapi-src/oneDNN",
                2
            ],
            [
                "https://github.com/albanie/convnet-burden",
                2
            ],
            [
                "https://github.com/hpclab/efficient_nn_for_ltr",
                1
            ],
            [
                "https://github.com/hpclab/efficient",
                1
            ]
        ]
    },
    "10.1609/aaai.v33i01.33013396": {
        "abstract": "AbstractStatistical characteristics of deep network representations,such as sparsity and correlation, are known to be relevant tothe performance and interpretability of deep learning. When astatistical characteristic is desired, often an adequate regular-izer can be designed and applied during the training phase.Typically, such a regularizer aims to manipulate a statisti-cal characteristic over all classes together. For classificationtasks, however, it might be advantageous to enforce the de-sired characteristic per class such that different classes canbe better distinguished. Motivated by the idea, we design twoclass-wise regularizers that explicitly utilize class informa-tion: class-wise Covariance Regularizer (cw-CR) and class-wise Variance Regularizer (cw-VR). cw-CR targets to reducethe covariance of representations calculated from the sameclass samples for encouraging feature independence. cw-VRis similar, but variance instead of covariance is targeted toimprove feature compactness. For the sake of completeness,their counterparts without using class information, Covari-ance Regularizer (CR) and Variance Regularizer (VR), areconsidered together. The four regularizers are conceptuallysimple and computationally very efficient, and the visualiza-tion shows that the regularizers indeed perform distinct rep-resentation shaping. In terms of classification performance,significant improvements over the baseline and L1/L2 weightregularization methods were found for 21 out of 22 tasks overpopular benchmark datasets. In particular, cw-VR achievedthe best performance for 13 tasks including ResNet-32/110.IntroductionFor deep learning, a variety of regularization techniqueshave been developed by focusing on the weight parame-ters. A classic example is the use of L2 (Hoerl and Kennard1970) and L1 (Tibshirani 1996) weight regularizers. Theyhave been popular because they are easy to use, computa-tionally light, and often result in performance enhancements.Another example is the parameter sharing technique that en-forces the same weight values as in the Convolutional NeuralNetworks (CNNs). Regularization techniques that focus onthe representation (the activations of the units in a deep net-work), however, have been less popular even though the per-formance of deep learning is known to depend on the learnedrepresentation heavily.∗Authors contributed equally.Figure 1: A single unit’s activation histogram (upper threeplots) and two randomly chosen units’ activation scatterplots (lower three plots) for MNIST. For a 6-layer Multi-layer Perceptron (MLP), the fifth layer’s representation vec-tors calculated using 10,000 test samples were used to gen-erate the plots. For the baseline model, a substantial overlapamong different classes can be observed at the time of ini-",
        "arxiv": "1809.09307",
        "doi": "10.1609/aaai.v33i01.33013396",
        "file_name": "",
        "file_path": "1809.09307v2.pdf",
        "title": "Utilizing Class Information for Deep Network Representation Shaping",
        "urls": [
            [
                "https://github.com/snu-adsl/class_",
                1
            ]
        ]
    },
    "10.18653/v1/2020.acl-main.55": {
        "abstract": "AbstractExisting automatic evaluation metrics foropen-domain dialogue response generationsystems correlate poorly with human evalua-tion. We focus on evaluating response gener-ation systems via response selection. To eval-uate systems properly via response selection,we propose the method to construct responseselection test sets with well-chosen false can-didates. Specifically, we propose to constructtest sets filtering out some types of false candi-dates: (i) those unrelated to the ground-truthresponse and (ii) those acceptable as appro-priate responses. Through experiments, wedemonstrate that evaluating systems via re-sponse selection with the test sets developedby our method correlates more strongly withhuman evaluation, compared with widely usedautomatic evaluation metrics such as BLEU.1 IntroductionAutomatic evaluation for open-domain dialoguegeneration systems has a potential for driving theirresearch and development because of its high re-producibility and low cost. However, existing auto-matic evaluation metrics, such as BLEU (Papineniet al., 2002), correlate poorly with human evalua-tion (Liu et al., 2016). This poor correlation arisesfrom a nature of dialogue, that is, there are manyacceptable responses to an input context, known asthe one-to-many problem (Zhao et al., 2017).To tackle this problematic issue, we focus onevaluating response generation systems via re-sponse selection. In this task, systems select anappropriate response for a given context from aset of response candidates. Each candidate has thelabel that indicates whether the candidate is appro-priate for the given context. Traditionally, responseselection has been used to evaluate retrieval-baseddialogue systems (Lowe et al., 2015; Wu et al.,2017). Here, we consider applying response selec-tion to driving the research for dialogue generationRepositoryContext:  Do you have a car?Ground-Truth: Yes, I have a car.QueryNo, I have a car.I don’t know.I have a cold.No, I have a car. I don’t know.",
        "arxiv": "2004.14302",
        "doi": "10.18653/v1/2020.acl-main.55",
        "file_name": "",
        "file_path": "2004.14302v1.pdf",
        "title": "Evaluating Dialogue Generation Systems via Response Selection",
        "urls": [
            [
                "https://github.com/cl-tohoku/eval-via-selection",
                2
            ]
        ]
    },
    "10.18653/v1/2020.emnlp-main.478": {
        "abstract": "AbstractThere is little to no data available to build nat-ural language processing models for most en-dangered languages. However, textual datain these languages often exists in formats thatare not machine-readable, such as paper booksand scanned images. In this work, we addressthe task of extracting text from these resources.We create a benchmark dataset of transcrip-tions for scanned books in three critically en-dangered languages and present a systematicanalysis of how general-purpose OCR toolsare not robust to the data-scarce setting of en-dangered languages. We develop an OCR post-correction method tailored to ease training inthis data-scarce setting, reducing the recogni-tion error rate by 34% on average across thethree languages.11 IntroductionNatural language processing (NLP) systems existfor a small fraction of the world’s over 6,000 liv-ing languages, the primary reason being the lackof resources required to train and evaluate models.Technological advances are concentrated on lan-guages that have readily available data, and mostother languages are left behind (Joshi et al., 2020).This is particularly notable in the case of endan-gered languages, i.e., languages that are in dangerof becoming extinct due to dwindling numbers ofnative speakers and the younger generations shift-ing to using other languages. For most endangeredlanguages, finding any data at all is challenging.In many cases, natural language text in theselanguages does exist. However, it is locked awayin formats that are not machine-readable — pa-per books, scanned images, and unstructured webpages. These include books from local publishing†: Work done at Carnegie Mellon University.1Code and data are available at https://shrutirij.github.io/ocr-el/.(a) Ainu (left) – Japanese (right)(b) Griko (top) – Italian (bottom)(c) Yakkha (top) – Nepali (middle) – English (bottom)(d) Handwritten Shangaji – typed English glossesText 31: cashew nuts Amina Sharaama explains how to make a sauce of green cashew nuts. Recorded on the 26th of April 2007. notebooks: p. 1230  31.1 ",
        "arxiv": "2011.05402",
        "doi": "10.18653/v1/2020.emnlp-main.478",
        "file_name": "",
        "file_path": "2011.05402v1.pdf",
        "title": "OCR Post Correction for Endangered Language Texts",
        "urls": null
    },
    "10.18653/v1/2020.textgraphs-1.7": {
        "abstract": "AbstractWe present a novel method for injecting temporality into entailment graphs to address the prob-lem of spurious entailments, which may arise from similar but temporally distinct events involv-ing the same pair of entities. We focus on the sports domain in which the same pairs of teams playon different occasions, with different outcomes. We present an unsupervised model that aims tolearn entailments such as win/lose→ play, while avoiding the pitfall of learning non-entailmentssuch as win 6→ lose. We evaluate our model on a manually constructed dataset, showing that in-corporating time intervals and applying a temporal window around them, are effective strategies.1 IntroductionRecognising textual entailment and paraphrases is core to many downstream NLP applications such asquestion answering and semantic parsing. In the case of open-domain question answering over unstruc-tured data, the answer to a question may not be explicitly stated in the text, but may be recovered viaparaphrases and/or entailment rules.Entailment graphs (Berant et al., 2011; Berant et al., 2015; Hosseini et al., 2018), in which nodesrepresent predicates and edges are entailment relations, have been proposed as a means to answer suchquestions. They can be mined using unsupervised methods applied over large collections of text, bykeeping track of which entity pairs occur with which predicates. One common error made by thesegraphs, however, is that they assert spurious associations between similar but temporally distinct eventsthat occur with the same entity pairs. For example, both the predicates beat and lost against will applyto sports team entity pairs such as (Arsenal, Man United). This is likely to mislead the current methodsinto incorrectly assigning an entailment relation between these two predicates.In this paper we extend the framework of Hosseini et al. (2018) to incorporate the temporal locationof events, with the aim of mitigating these spurious entailments. Temporal information can be used todisentangle these groups of highly correlated predicates, because although they will share entity pairs,they will never occur at the same time. For example, in Figure 1 Arsenal and Man United played eachother three times in 2019, with three different outcomes: win (beat), lose (lost against), tie (tied with).In previous methods, the context in which the predicates occur appears to be the same, because theyonly consider entity pairs as context. Therefore they mistakenly take the examples in Figure 1 as evidenceof entailments or paraphrases between the three outcome predicates (win, lose, and tie), depending onthe distributions found in the data. Our method enriches this context to include time interval information,thereby filtering out combinations that are not temporally near each other. Thus we hope to avoid learningthat beat→ lost against, while still learning that beat→ play.As an initial test domain, we focus on the sports news genre, using extracted relations that involvetwo sports teams. We evaluate on a manually constructed dataset of 1,312 entailment pairs based onparaphrases of the predicates in the graph on the right hand side of Figure 1. Our goal is to recoverthe structure of this graph in an unsupervised way, separating each of the highly correlated outcomepredicates while predicting that they all entail play.This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/.*The first two authors contributed equally to this workarXiv:2109.09412v1 ",
        "arxiv": "2109.09412",
        "doi": "10.18653/v1/2020.textgraphs-1.7",
        "file_name": "",
        "file_path": "2109.09412v1.pdf",
        "title": "Incorporating Temporal Information in Entailment Graph Mining",
        "urls": [
            [
                "https://gitlab.com/lianeg/temporal-entailment-sports-dataset",
                1
            ]
        ]
    },
    "10.18653/v1/2021.eacl-main.60": {
        "abstract": "AbstractWe introduce a top-down approach to dis-course parsing that is conceptually simplerthan its predecessors (Kobayashi et al., 2020;Zhang et al., 2020). By framing the task as asequence labelling problem where the goal isto iteratively segment a document into individ-ual discourse units, we are able to eliminatethe decoder and reduce the search space forsplitting points. We explore both traditionalrecurrent models and modern pre-trained trans-former models for the task, and additionally in-troduce a novel dynamic oracle for top-downparsing. Based on the Full metric, our pro-posed LSTM model sets a new state-of-the-artfor RST parsing.11 IntroductionDiscourse analysis involves the modelling of thestructure of text in a document. It provides a sys-tematic way to understand how texts are segmentedhierarchically into discourse units, and the relation-ships between them. Unlike syntax parsing whichmodels the relationship of words in a sentence, dis-course parsing operates at the document-level, andaims to explain the flow of writing. Studies havefound that discourse parsing is beneficial for down-stream NLP tasks including document-level senti-ment analysis (Bhatia et al., 2015) and abstractivesummarization (Koto et al., 2019).Rhetorical Structure Theory (RST; Mann andThompson (1988)) is one of the most widely useddiscourse theories in NLP (Hernault et al., 2010;Feng and Hirst, 2014; Ji and Eisenstein, 2014; Liet al., 2016; Yu et al., 2018). RST organizes textspans into a tree, where the leaves represent thebasic unit of discourse, known as elementary dis-course units (EDUs). EDUs are typically clauses1Code and trained models: https://github.com/fajri91/NeuralRST-TopDownEDU-1EDU-4EDU-2 EDU-3elabEDU-1:\tRoy\tE.\tParrott,\tthe\tcompany's\tpresident\tand\tchief\t\t\t\t\t\t\t\t\t\t\t\t\t\t\toperating\tofficer\tsince\tSept.\t1,\twas\tnamed\tto\tits\tboard.EDU-2:\tThe\tappointment\tincreased\tthe\tnumber\tof\tdirectors\tto\t10,EDU-3:\tthree\tof\twhom\tare\tcompany\temployees.EDU-4:\tSimpson\tis\tan\tauto\tparts\tmaker.elabelab",
        "arxiv": "2102.02080",
        "doi": "10.18653/v1/2021.eacl-main.60",
        "file_name": "",
        "file_path": "2102.02080v2.pdf",
        "title": "Top-down Discourse Parsing via Sequence Labelling",
        "urls": [
            [
                "https://github.com/yunan4nlp/NNDisParser",
                2
            ],
            [
                "https://github.com/fajri91/NeuralRST-TopDown",
                2
            ]
        ]
    },
    "10.18653/v1/2021.emnlp-main.185": {
        "abstract": "AbstractLearning sentence embeddings from dialogueshas drawn increasing attention due to its lowannotation cost and high domain adaptabil-ity. Conventional approaches employ thesiamese-network for this task, which obtainsthe sentence embeddings through modelingthe context-response semantic relevance by ap-plying a feed-forward network on top of thesentence encoders. However, as the seman-tic textual similarity is commonly measuredthrough the element-wise distance metrics (e.g.cosine and L2 distance), such architectureyields a large gap between training and evaluat-ing. In this paper, we propose DialogueCSE, adialogue-based contrastive learning approachto tackle this issue. DialogueCSE first in-troduces a novel matching-guided embedding(MGE) mechanism, which generates a context-aware embedding for each candidate responseembedding (i.e. the context-free embedding)according to the guidance of the multi-turncontext-response matching matrices. Then itpairs each context-aware embedding with itscorresponding context-free embedding and fi-nally minimizes the contrastive loss acrossall pairs. We evaluate our model on threemulti-turn dialogue datasets: the MicrosoftDialogue Corpus, the Jing Dong DialogueCorpus, and the E-commerce Dialogue Cor-pus. Evaluation results show that our ap-proach significantly outperforms the baselinesacross all three datasets in terms of MAP andSpearman’s correlation measures, demonstrat-ing its effectiveness. Further quantitative ex-periments show that our approach achievesbetter performance when leveraging more di-alogue context and remains robust when lesstraining data is provided.1 IntroductionSentence embeddings are used with success fora variety of NLP applications (Cer et al., 2018)and many prior methods have been proposed withdifferent learning schemes. Kiros et al. (2015); Lo-geswaran and Lee (2018); Hill et al. (2016) trainsentence encoders in a self-supervised manner withweb pages and books. Conneau et al. (2017); Ceret al. (2018); Reimers and Gurevych (2019) pro-pose to learn sentence embeddings on the super-vised datasets such as SNLI (Bowman et al., 2015)",
        "arxiv": "2109.12599",
        "doi": "10.18653/v1/2021.emnlp-main.185",
        "file_name": "",
        "file_path": "2109.12599v1.pdf",
        "title": "DialogueCSE: Dialogue-based Contrastive Learning of Sentence Embeddings",
        "urls": [
            [
                "https://github.com/wangruicn/DialogueCSE",
                1
            ]
        ]
    },
    "10.18653/v1/2021.findings-emnlp.116": {
        "abstract": "AbstractDeep reinforcement learning provides apromising approach for text-based games instudying natural language communicationbetween humans and artificial agents. How-ever, the generalization still remains a bigchallenge as the agents depend critically onthe complexity and variety of training tasks.In this paper, we address this problem byintroducing a hierarchical framework builtupon the knowledge graph-based RL agent.In the high level, a meta-policy is executedto decompose the whole game into a set ofsubtasks specified by textual goals, and selectone of them based on the KG. Then a sub-policy in the low level is executed to conductgoal-conditioned reinforcement learning. Wecarry out experiments on games with variousdifficulty levels and show that the proposedmethod enjoys favorable generalizability.1 IntroductionText-based games are simulated systems wherethe agent takes textual observations as the input,and interacts with the environment via text com-mands (Hausknecht et al., 2020). They are suitabletest-beds to study natural language understanding,commonsense reasoning and language-informeddecision making (Luketina et al., 2019). Reinforce-ment Learning (RL) based agents (Narasimhanet al., 2015; Zahavy et al., 2018) have been de-veloped to handle challenges such as language-based representation learning and combinatorialaction space. Among them, KG-based agents (Am-manabrolu and Hausknecht, 2020) yield promis-ing performance with the aid of Knowledge Graph(KG), which serves as a belief state to provide struc-tural information.To design intelligent RL-based agents for text-based games, it is necessary to build agents thatautomatically learn to solve different games. How-ever, generalization remains as one of the key chal-lenges of RL − the agent tends to overfit the train-ing environment and fails to generalize to new en-vironments (Cobbe et al., 2019). In the domainof text-based games, the TextWorld (Côté et al.,2018) makes it feasible to study generalizability bycreating non-overlapping game sets with customiz-able domain gaps (e.g., themes, vocabulary sets,difficulty levels and layouts). Most previous worksstudy generalizability either upon games with the",
        "arxiv": "2109.09968",
        "doi": "10.18653/v1/2021.findings-emnlp.116",
        "file_name": "",
        "file_path": "2109.09968v1.pdf",
        "title": "Generalization in Text-based Games via Hierarchical Reinforcement Learning",
        "urls": [
            [
                "https://github.com/xingdi-eric-yuan/GATA-public",
                2
            ],
            [
                "https://github.com/YunqiuXu/H-KGA",
                2
            ]
        ]
    },
    "10.18653/v1/2021.findings-emnlp.60": {
        "abstract": "AbstractData-driven subword segmentation has be-come the default strategy for open-vocabularymachine translation and other NLP tasks,but may not be sufficiently generic for op-timal learning of non-concatenative morphol-ogy. We design a test suite to evaluate seg-mentation strategies on different types of mor-phological phenomena in a controlled, semi-synthetic setting. In our experiments, wecompare how well machine translation mod-els trained on subword- and character-levelcan translate these morphological phenomena.We find that learning to analyse and generatemorphologically complex surface representa-tions is still challenging, especially for non-concatenative morphological phenomena likereduplication or vowel harmony and for rareword stems. Based on our results, we recom-mend that novel text representation strategiesbe tested on a range of typologically diverselanguages to minimise the risk of adopting astrategy that inadvertently disadvantages cer-tain languages.11 IntroductionData-driven subword-level segmentation of text(Sennrich et al., 2016; Kudo, 2018) is a well-knownand widely used text representation strategy inthe natural language processing (NLP) commu-nity. While subword segmentation largely solvesthe open vocabulary problem, previous researchhas shown that models often break down in out-of-domain contexts (El Boukkouri et al., 2020),when encountering spelling errors (Belinkov andBisk, 2018; Pruthi et al., 2019), when translatingmorphologically-rich languages (Ataman and Fed-erico, 2018) and in multilingual scenarios (Chunget al., 2020; Wang et al., 2021). The reason forthis is that even slight deviations from the text seen1Test suite and code available at https://github.com/ZurichNLP/segtestwhen learning a segmentation model can result inentirely different segmentations and often aggres-sively over-segmented text.Given the rich morphological diversity acrossnatural languages, it is especially interesting to in-vestigate the suitability of subword segmentation torepresent different morphological phenomena. Forexample, reduplication is a non-concatenative mor-phological phenomenon2 that is common across",
        "arxiv": "2109.01100",
        "doi": "10.18653/v1/2021.findings-emnlp.60",
        "file_name": "",
        "file_path": "2109.01100v1.pdf",
        "title": "How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?",
        "urls": [
            [
                "https://github.com/ZurichNLP/segtest",
                2
            ]
        ]
    },
    "10.18653/v1/2021.naacl-main.458": {
        "abstract": "AbstractNon-autoregressive Transformer is a promis-ing text generation model. However, cur-rent non-autoregressive models still fall be-hind their autoregressive counterparts in trans-lation quality. We attribute this accuracy gapto the lack of dependency modeling among de-coder inputs. In this paper, we propose CNAT,which learns implicitly categorical codes as la-tent variables into the non-autoregressive de-coding. The interaction among these categor-ical codes remedies the missing dependenciesand improves the model capacity. Experimentresults show that our model achieves compara-ble or better performance in machine transla-tion tasks, compared with several strong base-lines.1 IntroductionNon-autoregressive Transformer (NAT, Gu et al.,2018; Wang et al., 2019; Lee et al., 2018;Ghazvininejad et al., 2019) is a promising text gen-eration model for machine translation. It introducesthe conditional independent assumption among thetarget language outputs and simultaneously gener-ates the whole sentence, bringing in a remarkableefficiency improvement (more than 10× speed-up)versus the autoregressive model. However, the NATmodels still lay behind the autoregressive models interms of BLEU (Papineni et al., 2002) for machinetranslation. We attribute the low-quality of NATmodels to the lack of dependencies modeling forthe target outputs, making it harder to model thegeneration of the target side translation.A promising way is to model the dependenciesof the target language by the latent variables. A lineof research works (Kaiser et al., 2018; Roy et al.,2018; Shu et al., 2019; Ma et al., 2019) introducelatent variable modeling to the non-autoregressiveTransformer and improves translation quality. Thelatent variables could be regarded as the spring-board to bridge the modeling gap, introducingmore informative decoder inputs than the previ-ously copied inputs. More specifically, the latent-variable based model first predicts a latent vari-able sequence conditioned on the source represen-tation, where each variable represents a chunk ofwords. The model then simultaneously could gen-erate all the target tokens conditioning on the latentsequence and the source representation since thetarget dependencies have been modeled into the",
        "arxiv": "2103.11405",
        "doi": "10.18653/v1/2021.naacl-main.458",
        "file_name": "",
        "file_path": "2103.11405v1.pdf",
        "title": "Non-Autoregressive Translation by Learning Target Categorical Codes",
        "urls": [
            [
                "https://github.com/pytorch/fairseq",
                2
            ]
        ]
    },
    "10.18653/v1/2022.naacl-main.131": {
        "abstract": "AbstractParody is a figurative device used for mimick-ing entities for comedic or critical purposes.Parody is intentionally humorous and ofteninvolves sarcasm. This paper explores jointlymodelling these figurative tropes with the goalof improving performance of political parodydetection in tweets. To this end, we present amulti-encoder model that combines three par-allel encoders to enrich parody-specific repre-sentations with humor and sarcasm informa-tion. Experiments on a publicly available dataset of political parody tweets demonstrate thatour approach outperforms previous state-of-the-art methods.11 IntroductionParody is a figurative device which imitates enti-ties such as politicians and celebrities by copyingtheir particular style or a situation where the en-tity was involved (Rose, 1993). It is an intrinsicpart of social media as a relatively new comedicform (Vis, 2013). A very popular type of parodyis political parody, which is used to express politi-cal opposition and civic engagement (Davis et al.,2018).One of the hallmarks of parody expression is thedeployment of other figurative devices, such as hu-mor and sarcasm, as emphasized on studies of par-ody in linguistics (Haiman et al., 1998; Highfield,2016). For example, in Table 1 the text expressessarcasm about Myspace2 being a ‘winning tech-nology’, while mocking the fact that three morepopular social media sites were unavailable. Thisexample also highlights the similarities betweenparody and real tweets, which may pose issues tomisinformation classification systems (Mu and Ale-tras, 2020).1Code is available here https://github.com/iamoscar1/Multi_Encoder_Model_for_Political_Parody_Prediction2https://myspace.comTwitterHandle @Queen_UKParodytweetBoris Johnson on the phone. Verysmug that #myspace hasn’t gonedown. Says he’s always backedwinning technologies #whatsappdown#instagramdown #FacebookIsDown",
        "arxiv": "2205.03313",
        "doi": "10.18653/v1/2022.naacl-main.131",
        "file_name": "",
        "file_path": "2205.03313v1.pdf",
        "title": "Combining Humor and Sarcasm for Improving Political Parody Detection",
        "urls": [
            [
                "https://github.com/iamoscar1/Multi_Encoder_Model_for_Political_Parody_Prediction",
                3
            ]
        ]
    },
    "10.18653/v1/2022.naacl-main.207": {
        "abstract": "AbstractLong-context question answering (QA) tasksrequire reasoning over a long document ormultiple documents. Addressing these tasksoften benefits from identifying a set of evi-dence spans (e.g., sentences), which providesupporting evidence for answering the ques-tion. In this work, we propose a novel methodfor equipping long-context QA models with anadditional sequence-level objective for betteridentification of the supporting evidence. Weachieve this via an additional contrastive super-vision signal in finetuning, where the model isencouraged to explicitly discriminate support-ing evidence sentences from negative ones bymaximizing question-evidence similarity. Theproposed additional loss exhibits consistentimprovements on three different strong long-context transformer models, across two chal-lenging question answering benchmarks – Hot-potQA and QAsper.11 IntroductionAnswering questions that require reasoning overa long sequence, such over long documents ormultiple documents, is a challenging task (Panget al., 2021). Research in this domain mostly in-cludes tasks that involve multiple text segments,over benchmarks like HotpotQA (Yang et al., 2018)and QAsper (Dasigi et al., 2021). HotpotQA is amulti-hop QA benchmark over multiple paragraphsfrom Wikipedia, while QAsper involves readingcomprehension from long academic papers, whererelevant information on a question could be spreadacross the paper.Given the task complexity (Choi et al., 2017),benchmarks often provide an additional set of evi-dence spans, such as sentences or paragraphs, fora given question answer pair. This breaks downthe long-context QA task, adding a preliminary∗ Work partly done as an intern at AI2.1Our code is available at https://github.com/aviclu/long-context-qa-contrast.evidence span detection, which is crucial for suc-cessfully finding the correct answer, and also poten-tially helps in model interpretability. In this work,we propose a method for improving long-contextQA via leveraging such evidence spans, by maxi-mizing their similarity with the question.Since identifying the evidences provides rele-vant information for answering the question, prior",
        "arxiv": "2112.08777",
        "doi": "10.18653/v1/2022.naacl-main.207",
        "file_name": "",
        "file_path": "2112.08777v2.pdf",
        "title": "Long Context Question Answering via Supervised Contrastive Learning",
        "urls": [
            [
                "https://github.com/aviclu/long-context-qa-contrast",
                2
            ],
            [
                "https://github.com/aviclu/CDLM",
                2
            ]
        ]
    },
    "10.18653/v1/w19-5368": {
        "abstract": "AbstractWhile neural machine translation (NMT)achieves remarkable performance on clean, in-domain text, performance is known to degradedrastically when facing text which is full oftypos, grammatical errors and other varietiesof noise. In this work, we propose a multi-task learning algorithm for transformer-basedMT systems that is more resilient to this noise.We describe our submission to the WMT 2019Robustness shared task (Li et al., 2019) basedon this method. Our model achieves a BLEUscore of 32.8 on the shared task French to En-glish dataset, which is 7.1 BLEU points higherthan the baseline vanilla transformer trainedwith clean text1.1 IntroductionReal world data, especially in the realm of socialmedia, often contains noise such as mis-spellings,grammar errors, or lexical variations. Even thoughhumans do not have much difficulty in recognizingand translating noisy or ungrammatical sentences,neural machine translation (NMT; Bahdanau et al.(2015); Vaswani et al. (2017)) systems are knownto degrade drastically when confronted with noisydata (Belinkov and Bisk, 2017; Khayrallah andKoehn, 2018; Anastasopoulos et al., 2019). Thus,there is increasing need to build robust NMT sys-tems that are resilient to naturally occurring noise.In this work, we attempt to enhance the ro-bustness of the NMT system through multi-tasklearning. Our model is a transformer-based model(Vaswani et al., 2017) augmented with two de-coders, with each decoder bound to different learn-ing objectives. It has a cascade architecture(Niehues et al., 2016; Anastasopoulos and Chiang,2018) where the first decoder reads in the outputof the encoder and the second decoder reads in the1The code is available at https://github.com/shuyanzhou/multitask_transformeroutput of both encoder and the first decoder. Theobjective of the first decoder, namely the denois-ing decoder, is to recover from the noisy sentenceand generate the corresponding clean sentence.Given both the noisy and clean sentence, the ob-jective of the second decoder, namely the transla-tion decoder, is to correctly translate the sentenceto the target language. This framework shouldbe beneficial in two ways: 1) Since the model istrained with noisy text, it should inherently bet-",
        "arxiv": null,
        "doi": "10.18653/v1/w19-5368",
        "file_name": "",
        "file_path": "W19-5368.pdf",
        "title": "Improving Robustness of Neural Machine Translation with Multi-task Learning",
        "urls": [
            [
                "https://github.com/pytorch/fairseq",
                3
            ],
            [
                "https://github.com/shuyanzhou/multitask_transformer",
                2
            ]
        ]
    },
    "10.2140/agt.2022.22.3577": {
        "abstract": "AbstractWe present a general method to compute a presentation for any cusped arithmetic hyperbolic latticeΓ, applying a classical result of Macbeath to a suitable Γ-invariant horoball cover of the correspondingsymmetric space. As applications we compute presentations for the Picard modular groups PU(2, 1,Od) ford = 1, 3, 7 and the quaternion hyperbolic lattice PU(2, 1,H) with entries in the Hurwitz integer ring H. Theimplementation of the method for these groups is computer-assisted.1 IntroductionDiscrete subgroups and lattices in semisimple Lie groups form a rich and well-studied class of finitely gen-erated groups acting on non-positively curved metric spaces. The case of real rank one, where the associatedsymmetric space is negatively curved, is of special interest. There are essentially two main families of con-structions of such lattices, arithmetic on one hand and geometric on the other. Arithmetic lattices are roughlyspeaking obtained by taking matrices with entries lying in the integer ring of some number field; the generaldefinition is more complicated and we will not give it here, as the arithmetic lattices that we consider in this pa-per are of this simplest type. By Margulis’ celebrated superrigidity and arithmeticity theorems, all (irreducible)lattices in G are of this arithmetic type when G is a semisimple Lie group of real rank at least 2.The other family involves geometric constructions such as polyhedra, reflections or other types of involutionsor other finite-order isometries. A prototype of this type of construction is given by Coxeter groups in theconstant curvature geometries En, Sn and Hn, which are generated by reflections across hyperplanes. Thesegroups are classical and were classified by Coxeter in the spaces En and Sn, whereas their hyperbolic counterparts(studied by Vinberg and others) are still not completely understood. However by construction these groupscome equipped with data including a presentation (as an abtract Coxeter group) and a fundamental domain fortheir action on the symmetric space.Arithmetic lattices are given by a global description and their global structure is in some sense well under-stood by work of Siegel, Borel, Tits, Prasad and others. However concrete information such as a presentationand a fundamental domain are not readily accessible from the arithmetic construction. One can obtain geo-metric information such as volume by Prasad’s celebrated volume formula ([Pr]) but computing the constantsappearing in this formula usually involves some non-trivial work (see for example [Be] and [Sto]).Very few presentations of arithmetic lattices, and of lattices in general, are known. Presentations canprovide useful geometric and algebraic information about groups, such as explicit index of torsion-free subgroups(effective Selberg lemma, as used for example in [Sto]), cohomology of the group Γ or quotient space X/Γ, seefor instance [Y] (for the Picard modular groups with d = 1, 3) and of course representations of Γ, for instanceif one is interested in deformations of Γ into a larger Lie group.Presentations for SL(n,Z) with n > 3 were given by Steinberg ([Ste], following Magnus); the case of SL(2,Z)is classical and possibly dates to Gauss; see also Siegel [Si]. In rank one, Swan gave in [Sw] presentations forthe Bianchi groups PGL(2,Od) (where Od denotes the ring of integers of Q[i√d] for d a positive square-freeinteger), following Bianchi’s original construction in [Bi]. These act as isometries of (real) hyperbolic 3-space,as they are lattices in PGL(2,C) ' Isom+(H3R).Presentations for the related Picard modular groups PU(2, 1,Od) were found only recently in the simplestcases of d = 3 ([FP]) and d = 1 ([FFP]). One of the reasons for this is that the associated symmetric space,∗Second author partially supported by National Science Foundation Grant DMS-1708463.1arXiv:1709.06",
        "arxiv": "1709.06691",
        "doi": "10.2140/agt.2022.22.3577",
        "file_name": "",
        "file_path": "1709.06691v5.pdf",
        "title": "Presentations for cusped arithmetic hyperbolic lattices",
        "urls": [
            [
                "https://github.com/alice-mark/LatticePresentations",
                2
            ]
        ]
    },
    "10.2478/pralin-2018-0002": {
        "abstract": "AbstractThis article describes our experiments in neural machine translation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequencemodel (Vaswani et al., 2017).We examine some of the critical parameters that affect the final translation quality, memoryusage, training stability and training time, concluding each experiment with a set of recom-mendations for fellow researchers. In addition to confirming the general mantra “more dataand larger models”, we address scaling to multiple GPUs and provide practical tips for im-proved training regarding batch size, learning rate, warmup steps, maximum sentence lengthand checkpoint averaging. We hope that our observations will allow others to get better resultsgiven their particular hardware and data constraints.1. IntroductionIt has been already clearly established that neural machine translation (NMT) isthe new state of the art in machine translation, see e.g. the most recent evaluationcampaigns (Bojar et al., 2017a; Cettolo et al., 2017). Many fundamental changes of theunderlying neural network architecture are nevertheless still frequent and it is verydifficult to predict which of the architectures has the best combination of propertiesto win in the long term, considering all relevant criteria like translation quality, modelsize, stability and speed of training, interpretability but also practical availability ofgood implementations. A considerable part of amodel’s success in translation qualityconsists in the training data, the model’s sensitivity to noise in the data but also on awide range of hyper-parameters that affect the training. Having the right setting ofthem turns out to be often a critical component for the success.1arXiv:1804.00247v2  [cs.CL]  2 May 2018In this article, we experimentwith a relatively newNMTmodel, calledTransformer(Vaswani et al., 2017) as implemented in the Tensor2Tensor1 (abbreviated T2T) toolkit,version 1.2.9. Themodel and the toolkit have been released shortly after the evaluationcampaign atWMT20172 and its behavior on large-data news translation is not yet fullyexplored. We want to empirically explore some of the important hyper-parameters.Hopefully, our observations will be useful also for other researchers considering this",
        "arxiv": "1804.00247",
        "doi": "10.2478/pralin-2018-0002",
        "file_name": "",
        "file_path": "1804.00247v2.pdf",
        "title": "Training Tips for the Transformer Model",
        "urls": [
            [
                "https://github.com/tensorflow/tensor2tensor",
                10
            ],
            [
                "https://github.com/awslabs/sockeye",
                2
            ]
        ]
    },
    "10.48550/arxiv.2210.08826": {
        "abstract": "AbstractMany state-of-the-art noisy-label learning methods relyon learning mechanisms that estimate the samples’ cleanlabels during training and discard their original noisy la-bels. However, this approach prevents the learning of therelationship between images, noisy labels and clean la-bels, which has been shown to be useful when dealingwith instance-dependent label noise problems. Further-more, methods that do aim to learn this relationship re-quire cleanly annotated subsets of data, as well as dis-tillation or multi-faceted models for training. In this pa-per, we propose a new training algorithm that relies on asimple model to learn the relationship between clean andnoisy labels without the need for a cleanly labelled subset ofdata. Our algorithm follows a 3-stage process, namely: 1)self-supervised pre-training followed by an early-stoppingtraining of the classifier to confidently predict clean la-bels for a subset of the training set; 2) use the clean setfrom stage (1) to bootstrap the relationship between images,noisy labels and clean labels, which we exploit for effec-tive relabelling of the remaining training set using semi-supervised learning; and 3) supervised training of the clas-sifier with all relabelled samples from stage (2). By learningthis relationship, we achieve state-of-the-art performancein asymmetric and instance-dependent label noise prob-lems1. Code is available at https://github.com/btsmart/bootstrapping-label-noise.1. IntroductionSupervised deep learning has had great success gen-erating effective classification models from sets of la-belled training data [24, 26]. Modern deep learning mod-els require large-scale datasets to achieve state-of-the-art (SOTA) results [38, 39]. However, real-world large-1Supported by Australian Research Council through grantsDP180103232 and FT190100525.scale datasets, such as those collected from search en-gines or available from hospitals and clinics, tend to havea non-negligible amount of instance-dependent label noise(IDN) [32, 53]. Existing methods often attempt to addressinstance-independent label noise (IIN), such as symmetricor asymmetric noise [15, 58, 65]. Handling the IDN presentin large-scale real-world datasets has become one of themain research problems in the field.When naively trained with noisy-labelled data, deeplearning models generalise poorly because they can easilyoverfit the incorrectly labelled samples [62]. Many meth-ods have been developed for handling label noise, withSOTA approaches relying on sample relabelling mecha-nisms. These strategies are based on techniques to estimatethe relationship between images and clean labels, and after",
        "arxiv": "2210.08826",
        "doi": "10.48550/arxiv.2210.08826",
        "file_name": "",
        "file_path": "2210.08826v1.pdf",
        "title": "Bootstrapping the Relationship Between Images and Their Clean and Noisy\n  Labels",
        "urls": [
            [
                "https://github.com/btsmart/bootstrapping-label-noise",
                2
            ]
        ]
    },
    "10.48550/arxiv.2211.08278": {
        "abstract": "Abstract—In perception tasks of automated vehicles (AVs)data-driven have often outperformed conventional approaches.This motivated us to develop a data-driven methodology tocompute occupancy grid maps (OGMs) from lidar measurements.Our approach extends previous work such that the estimatedenvironment representation now contains an additional layer forcells occupied by dynamic objects. Earlier solutions could onlydistinguish between free and occupied cells. The informationwhether an obstacle could move plays an important role forplanning the behavior of an AV. We present two approaches togenerating training data. One approach extends our previouswork on using synthetic training data so that OGMs withthe three aforementioned cell states are generated. The otherapproach uses manual annotations from the nuScenes [1] datasetto create training data. We compare the performance of bothmodels in a quantitative analysis on unseen data from the real-world dataset. Next, we analyze the ability of both approachesto cope with a domain shift, i.e. when presented with lidarmeasurements from a different sensor on a different vehicle.We propose using information gained from evaluation on real-world data to further close the reality gap and create bettersynthetic data that can be used to train occupancy grid mappingmodels for arbitrary sensor configurations. Code is available athttps://github.com/ika-rwth-aachen/DEviLOG.Index Terms—AD, perception, simulation, deep learningI. INTRODUCTIONAn automated vehicle can determine the drivable space inthe static environment by localizing itself on a high-definitionmap. These HD maps describe the exact road geometry andtraffic rules applying to each lane.However, these maps can be outdated so that it becomesnecessary to substitute them with measurement-based repre-*This research is accomplished within the project ”UNICARagil” (FKZ16EMO0284K). We acknowledge the financial support for the project by theFederal Ministry of Education and Research of Germany (BMBF).(a) Trained on Synthetic Data (b) Trained on nuScenesFig. 1: The left OGM was predicted by a model trainedon synthetic data and the right OGM was predicted by amodel trained on labels generated from annotations of thenuScenes [1] dataset. Green indicates belief mass for free,red for statically occupied and blue for dynamically occupiedcells. The lidar point cloud as input data is superimposedand colored by the class annotations, i.e. vehicles are orange,drivable surface is yellow and occupied space is grey.sentations to determine the actual drivable space. Grid-basedenvironment representations are particularly suitable for thisas they discretize a defined area around the vehicle into cellsthat can be aligned with the HD map. Distances measurede.g. by a lidar sensor can be used to assign an occupancystate to each cell in an OGM. Such OGMs are also used in",
        "arxiv": "2211.08278",
        "doi": "10.48550/arxiv.2211.08278",
        "file_name": "",
        "file_path": "2211.08278v1.pdf",
        "title": "Data-Driven Occupancy Grid Mapping using Synthetic and Real-World Data",
        "urls": [
            [
                "https://github.com/ika-rwth-aachen/DEviLOG",
                2
            ]
        ]
    }
}